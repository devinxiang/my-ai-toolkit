
================================================================================
filename: API-Reference/api-build.mdx
================================================================================

---
title: Build endpoints
slug: /api-build
---

:::info
The `/build` endpoints are used by Langflow's frontend visual editor code.
These endpoints are part of the internal Langflow codebase.

Don't use these endpoints to run flows in applications that use your Langflow flows.
To run flows in your apps, see [Flow trigger endpoints](/api-flows-run).
:::

The `/build` endpoints support Langflow's frontend code for building flows in the Langflow visual editor.
You can use these endpoints to build vertices and flows, as well as execute flows with streaming event responses.
You might need to use or understand these endpoints when contributing to the Langflow codebase.

## Build flow and stream events

This endpoint builds and executes a flow, returning a job ID that can be used to stream execution events.

1. Send a POST request to the `/build/$FLOW_ID/flow` endpoint:

    ```shell
    curl -X POST \
      "$LANGFLOW_URL/api/v1/build/$FLOW_ID/flow" \
      -H "accept: application/json" \
      -H "Content-Type: application/json" \
      -H "x-api-key: $LANGFLOW_API_KEY" \
      -d '{
        "inputs": {
          "input_value": "Tell me a story"
        }
      }'
    ```

    <details>
    <summary>Result</summary>

    ```json
    {
      "job_id": "123e4567-e89b-12d3-a456-426614174000"
    }
    ```

    </details>

2. After receiving a job ID from the build endpoint, use the `/build/$JOB_ID/events` endpoint to stream the execution results:

    ```shell
    curl -X GET \
      "$LANGFLOW_URL/api/v1/build/123e4567-e89b-12d3-a456-426614174000/events" \
      -H "accept: application/json" \
      -H "x-api-key: $LANGFLOW_API_KEY"
    ```

    <details>
    <summary>Result</summary>

    ```json
    {"event": "vertices_sorted", "data": {"ids": ["ChatInput-XtBLx"], "to_run": ["Prompt-x74Ze", "ChatOutput-ylMzN", "ChatInput-XtBLx", "OpenAIModel-d1wOZ"]}}

    {"event": "add_message", "data": {"timestamp": "2025-03-03T17:42:23", "sender": "User", "sender_name": "User", "session_id": "d2bbd92b-187e-4c84-b2d4-5df365704201", "text": "Tell me a story", "files": [], "error": false, "edit": false, "properties": {"text_color": "", "background_color": "", "edited": false, "source": {"id": null, "display_name": null, "source": null}, "icon": "", "allow_markdown": false, "positive_feedback": null, "state": "complete", "targets": []}, "category": "message", "content_blocks": [], "id": "28879bd8-6a68-4dd5-b658-74d643a4dd92", "flow_id": "d2bbd92b-187e-4c84-b2d4-5df365704201"}}

    // ... Additional events as the flow executes ...

    {"event": "end", "data": {}}
    ```

    </details>

The `/build/$FLOW_ID/events` endpoint has a `stream` query parameter that defaults to `true`.
To disable streaming and get all events at once, set `?stream=false`.

```shell
curl -X GET \
  "$LANGFLOW_URL/api/v1/build/123e4567-e89b-12d3-a456-426614174000/events?stream=false" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

## Build headers

| Header | Info | Example |
|--------|------|---------|
| Content-Type | Required. Specifies the JSON format. | "application/json" |
| accept | Optional. Specifies the response format. | "application/json" |
| x-api-key | Optional. Required only if authentication is enabled. | "sk-..." |

## Build parameters

| Parameter | Type | Description |
|-----------|------|-------------|
| inputs | object | Optional. Input values for flow components. |
| data | object | Optional. Flow data to override stored configuration. |
| files | array[string] | Optional. List of file paths to use. |
| start_component_id | string | Optional. ID of the component where the execution should start. Component `id` values can be found in [Langflow JSON files](/concepts-flows-import#langflow-json-file-contents) |
| stop_component_id | string | Optional. ID of the component where the execution should stop. Component `id` values can be found in [Langflow JSON files](/concepts-flows-import#langflow-json-file-contents).|
| log_builds | Boolean | Whether to record build logs. Default: Enabled (`true`). |

### Set start and stop points

The `/build` endpoint accepts optional values for `start_component_id` and `stop_component_id` to control where the flow run starts and stops.
Setting `stop_component_id` for a component triggers the same behavior as clicking **Run component** on that component in the visual editor: The specified component and all dependent components leading up to that component will run.

The following example stops flow execution at an **OpenAI** component:

```shell
curl -X POST \
  "$LANGFLOW_URL/api/v1/build/$FLOW_ID/flow" \
  -H "accept: application/json" \
  -H "Content-Type: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -d '{"stop_component_id": "OpenAIModel-Uksag"}'
```

### Override flow parameters

The `/build` endpoint also accepts inputs for `data` directly, instead of using the values stored in the Langflow database.
This is useful for running flows without having to pass custom values through the visual editor.

```shell
curl -X POST \
  "$LANGFLOW_URL/api/v1/build/$FLOW_ID/flow" \
  -H "accept: application/json" \
  -H "Content-Type: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -d '{
    "data": {
      "nodes": [],
      "edges": []
    },
    "inputs": {
      "input_value": "Your custom input here",
      "session": "session_id"
    }
  }'
```

<details>
<summary>Result</summary>

```json
{ "job_id": "0bcc7f23-40b4-4bfa-9b8a-a44181fd1175" }
```

</details>

## See also

- [Get Vertex builds](/api-monitor#get-vertex-builds)
- [Delete Vertex builds](/api-monitor#delete-vertex-builds)
- [Session ID](/session-id)


================================================================================
filename: API-Reference/api-files.mdx
================================================================================

---
title: Files endpoints
slug: /api-files
---

Use the `/files` endpoints to move files between your local machine and Langflow.

## Differences between `/v1/files` and `/v2/files`

There are two versions of the `/files` endpoints.

`/v2/files` offers the following improvements over `/v1/files`:

- `/v2` files are organized by `user_id` instead of `flow_id`.
  This means files are owned by users, and they aren't attached to specific flows.
  You can upload a file to Langflow one time, and use it with multiple flows.
- `/v2` files are tracked in the Langflow database.
- `/v2` supports bulk upload and delete.
- `/v2` responses contain more descriptive metadata.

However, `/v2/files` doesn't support image files.
To send image files to your flows through the API, use [Upload image files (v1)](#upload-image-files-v1).

## Files/V1 endpoints

Use the `/files` endpoints to move files between your local machine and Langflow.

### Upload file (v1)

Upload a file to the `v1/files/upload/$FLOW_ID` endpoint:
Replace **FILE_NAME** with the uploaded file name.

```bash
curl -X POST \
  "$LANGFLOW_URL/api/v1/files/upload/$FLOW_ID" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -F "file=@FILE_NAME.txt"
```

Replace `FILE_NAME.txt` with the name and extension of the file you want to upload.
Not all file types are supported.

<details>
<summary>Result</summary>

```json
{
  "flowId": "92f9a4c5-cfc8-4656-ae63-1f0881163c28",
  "file_path": "92f9a4c5-cfc8-4656-ae63-1f0881163c28/2024-12-30_15-19-43_your_file.txt"
}
```

</details>

### Upload image files (v1)

Send image files to Langflow to use them in flows.

The default file limit is 1024 MB.
To change this limit, set the `LANGFLOW_MAX_FILE_SIZE_UPLOAD` [environment variable](/environment-variables).

1. Attach the image to a `POST /v1/files/upload/$FLOW_ID` request with `--form` (`-F`) and the file path:

    ```bash
    curl -X POST "$LANGFLOW_URL/api/v1/files/upload/$FLOW_ID" \
      -H "Content-Type: multipart/form-data" \
      -H "x-api-key: $LANGFLOW_API_KEY" \
      -F "file=@PATH/TO/FILE.png"
    ```

    A successful request returns the `file_path` for the image in the Langflow file management system in the format `FLOW_ID/TIMESTAMP_FILENAME.TYPE`.
    For example:

    ```json
    {
      "flowId": "a430cc57-06bb-4c11-be39-d3d4de68d2c4",
      "file_path": "a430cc57-06bb-4c11-be39-d3d4de68d2c4/2024-11-27_14-47-50_image-file.png"
    }
    ```

2. Use the returned `file_path` to send the image file to other components that can accept file input. Where you specify the file path depends on the component type.

    The following example runs the **Basic Prompting** template flow, passing the image file and the query `describe this image` as input for the **Chat Input** component.
    In this case, the file path is specified in `tweaks`.

    ```bash
    curl -X POST \
        "$LANGFLOW_URL/api/v1/run/a430cc57-06bb-4c11-be39-d3d4de68d2c4?stream=false" \
        -H "Content-Type: application/json" \
        -H "x-api-key: $LANGFLOW_API_KEY" \
        -d '{
        "output_type": "chat",
        "input_type": "chat",
        "tweaks": {
          "ChatInput-b67sL": {
            "files": "a430cc57-06bb-4c11-be39-d3d4de68d2c4/2024-11-27_14-47-50_image-file.png",
            "input_value": "describe this image"
          }
        }
      }'
    ```

    :::tip
    For help with tweaks, use the **Input Schema** in a flow's [**API access** pane](/concepts-publish#api-access).
    Setting tweaks with **Input Schema** also automatically populates the required component IDs.
    :::

### List files (v1)

List all files associated with a specific flow.

```bash
curl -X GET \
  "$LANGFLOW_URL/api/v1/files/list/$FLOW_ID" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

<details>
<summary>Result</summary>

```json
{
  "files": ["2024-12-30_15-19-43_your_file.txt"]
}
```

</details>

### Download file (v1)

Download a specific file from a flow.

```bash
curl -X GET \
  "$LANGFLOW_URL/api/v1/files/download/$FLOW_ID/2024-12-30_15-19-43_your_file.txt" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  --output downloaded_file.txt
```

<details>
<summary>Result</summary>

```text
File contents downloaded to downloaded_file.txt
```

</details>

### Delete file (v1)

Delete a specific file from a flow.

```bash
curl -X DELETE \
  "$LANGFLOW_URL/api/v1/files/delete/$FLOW_ID/2024-12-30_15-19-43_your_file.txt" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

<details>
<summary>Result</summary>

```json
{
  "message": "File 2024-12-30_15-19-43_your_file.txt deleted successfully"
}
```

</details>

## Files/V2 endpoints

Use the `/files` endpoints to move files between your local machine and Langflow.

The `/v2/files` endpoints can be authenticated by an API key or JWT.
To create a Langflow API key and export it as an environment variable, see [Get started with the Langflow API](/api-reference-api-examples).

### Upload file (v2)

Upload a file to your user account. The file can be used across multiple flows.

The file is uploaded in the format `USER_ID/FILE_ID.FILE_EXTENSION`, such as `07e5b864-e367-4f52-b647-a48035ae7e5e/d44dc2e1-9ae9-4cf6-9114-8d34a6126c94.pdf`.

1. To retrieve your current `user_id`, call the `/whoami` endpoint:

    ```bash
    curl -X GET \
      "$LANGFLOW_URL/api/v1/users/whoami" \
      -H "accept: application/json" \
      -H "x-api-key: $LANGFLOW_API_KEY"
    ```

    <details>
    <summary>Result</summary>

    ```
    {"id":"07e5b864-e367-4f52-b647-a48035ae7e5e","username":"langflow","profile_image":null,"store_api_key":null,"is_active":true,"is_superuser":true,"create_at":"2025-05-08T17:59:07.855965","updated_at":"2025-05-28T19:00:42.556460","last_login_at":"2025-05-28T19:00:42.554338","optins":{"github_starred":false,"dialog_dismissed":true,"discord_clicked":false,"mcp_dialog_dismissed":true}}
    ```

    </details>

2. In the POST request to `v2/files`, replace **@FILE_NAME.EXTENSION** with the uploaded file name and its extension.
You must include the ampersand (`@`) in the request to instruct curl to upload the contents of the file, not the string `FILE_NAME.EXTENSION`.

    ```bash
    curl -X POST \
      "$LANGFLOW_URL/api/v2/files" \
      -H "accept: application/json" \
      -H "Content-Type: multipart/form-data" \
      -H "x-api-key: $LANGFLOW_API_KEY" \
      -F "file=@FILE_NAME.EXTENSION"
    ```

    The file is uploaded in the format `USER_ID/FILE_ID.FILE_EXTENSION`, and the API returns metadata about the uploaded file:

    ```json
    {
      "id":"d44dc2e1-9ae9-4cf6-9114-8d34a6126c94",
      "name":"engine_manual",
      "path":"07e5b864-e367-4f52-b647-a48035ae7e5e/d44dc2e1-9ae9-4cf6-9114-8d34a6126c94.pdf",
      "size":851160,
      "provider":null
    }
    ```

### Send files to your flows (v2)

:::info
The `/v2/files` endpoint can't send image files to flows.
To send image files to your flows through the API, see [Upload image files (v1)](#upload-image-files-v1).
:::

This endpoint uploads files to your Langflow server's file management system.
To use an uploaded file in a flow, send the file path to a flow with a [**File** component](/components-data#file).

The default file limit is 1024 MB. To configure this value, change the `LANGFLOW_MAX_FILE_SIZE_UPLOAD` [environment variable](/environment-variables).

1. To send a file to your flow with the API, POST the file to the `/api/v2/files` endpoint.

    Replace **FILE_NAME.EXTENSION** with the name and extension of the file you want to upload.
    This is the same step described in [Upload file (v2)](#upload-file-v2), but since you need the filename to upload to your flow, it is included here.

    ```bash
    curl -X POST \
      "$LANGFLOW_URL/api/v2/files" \
      -H "accept: application/json" \
      -H "Content-Type: multipart/form-data" \
      -H "x-api-key: $LANGFLOW_API_KEY" \
      -F "file=@FILE_NAME.EXTENSION"
    ```

    The file is uploaded in the format `USER_ID/FILE_ID.FILE_EXTENSION`, and the API returns metadata about the uploaded file:

    ```json
    {
      "id":"d44dc2e1-9ae9-4cf6-9114-8d34a6126c94",
      "name":"engine_manual",
      "path":"07e5b864-e367-4f52-b647-a48035ae7e5e/d44dc2e1-9ae9-4cf6-9114-8d34a6126c94.pdf",
      "size":851160,
      "provider": null
    }
    ```

2. To use this file in your flow, add a **File** component to your flow.
This component loads files into flows from your local machine or Langflow file management.

3. Run the flow, passing the `path` to the `File` component in the `tweaks` object:

    ```text
    curl --request POST \
      --url "$LANGFLOW_URL/api/v1/run/$FLOW_ID" \
      --header "Content-Type: application/json" \
      --header "x-api-key: $LANGFLOW_API_KEY" \
      --data '{
      "input_value": "what do you see?",
      "output_type": "chat",
      "input_type": "text",
      "tweaks": {
        "File-1olS3": {
          "path": [
            "07e5b864-e367-4f52-b647-a48035ae7e5e/3a290013-fe1e-4d3d-a454-cacae81288f3.pdf"
          ]
        }
      }
    }'
    ```

    To get the `File` component's ID, call the [Read flow](/api-flows#read-flow) endpoint or inspect the component in the visual editor.

    If the file path is valid, the flow runs successfully.

### List files (v2)

List all files associated with your user account.

```bash
curl -X GET \
  "$LANGFLOW_URL/api/v2/files" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

<details>
<summary>Result</summary>

```json
[
  {
    "id": "c7b22c4c-d5e0-4ec9-af97-5d85b7657a34",
    "name": "your_file",
    "path": "6f17a73e-97d7-4519-a8d9-8e4c0be411bb/c7b22c4c-d5e0-4ec9-af97-5d85b7657a34.txt",
    "size": 1234,
    "provider": null
  }
]
```

</details>

### Download file (v2)

Download a specific file by its ID and file extension.

You must specify the file type you expect in the `--output` value.

```bash
curl -X GET \
  "$LANGFLOW_URL/api/v2/files/c7b22c4c-d5e0-4ec9-af97-5d85b7657a34" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  --output downloaded_file.txt
```

<details>
<summary>Result</summary>

```text
File contents downloaded to downloaded_file.txt
```

</details>

### Edit file name (v2)

Change a file name.

```bash
curl -X PUT \
  "$LANGFLOW_URL/api/v2/files/$FILE_ID?name=new_file_name" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

<details>
<summary>Result</summary>

```json
{
  "id": "76543e40-f388-4cb3-b0ee-a1e870aca3d3",
  "name": "new_file_name",
  "path": "6f17a73e-97d7-4519-a8d9-8e4c0be411bb/76543e40-f388-4cb3-b0ee-a1e870aca3d3.png",
  "size": 2728251,
  "provider": null
}
```

</details>

### Delete file (v2)

Delete a specific file by its ID.

```bash
curl -X DELETE \
  "$LANGFLOW_URL/api/v2/files/$FILE_ID" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

<details>
<summary>Result</summary>

```json
{
  "message": "File deleted successfully"
}
```

</details>

### Delete all files (v2)

Delete all files associated with your user account.

```bash
curl -X DELETE \
  "$LANGFLOW_URL/api/v2/files" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

<details>
<summary>Result</summary>

```json
{
  "message": "All files deleted successfully"
}
```

</details>

## Create upload file (Deprecated)

This endpoint is deprecated. Use the `/files` endpoints instead.

## See also

* [Manage files](/concepts-file-management)


================================================================================
filename: API-Reference/api-flows-run.mdx
================================================================================

---
title: Flow trigger endpoints
slug: /api-flows-run
---

Use the `/run` and `/webhook` endpoints to run flows.

To create, read, update, and delete flows, see [Flow management endpoints](/api-flows).

## Run flow

:::tip
Langflow automatically generates Python, JavaScript, and curl code snippets for the `/v1/run/$FLOW_ID` endpoint for all flows.
For more information, see [Generate API code snippets](/concepts-publish#generate-api-code-snippets).
:::

Execute a specified flow by ID or name.
Flow IDs can be found on the code snippets on the [**API access** pane](/concepts-publish#api-access) or in a flow's URL.

The following example runs the **Basic Prompting** template flow with flow parameters passed in the request body.
This flow requires a chat input string (`input_value`), and uses default values for all other parameters.

```bash
curl -X POST \
  "$LANGFLOW_SERVER_URL/api/v1/run/$FLOW_ID" \
  -H "Content-Type: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -d '{
    "input_value": "Tell me about something interesting!",
    "session_id": "chat-123",
    "input_type": "chat",
    "output_type": "chat",
    "output_component": "",
    "tweaks": null
  }'
```

The response from `/v1/run/$FLOW_ID` includes metadata, inputs, and outputs for the run.

<details>
<summary>Result</summary>

The following example illustrates a response from a Basic Prompting flow:

```json
{
  "session_id": "chat-123",
  "outputs": [{
    "inputs": {
      "input_value": "Tell me about something interesting!"
    },
    "outputs": [{
      "results": {
        "message": {
          "text": "Sure! Have you ever heard of the phenomenon known as \"bioluminescence\"? It's a fascinating natural occurrence where living organisms produce and emit light. This ability is found in various species, including certain types of jellyfish, fireflies, and deep-sea creatures like anglerfish.\n\nBioluminescence occurs through a chemical reaction in which a light-emitting molecule called luciferin reacts with oxygen, catalyzed by an enzyme called luciferase. The result is a beautiful glow that can serve various purposes, such as attracting mates, deterring predators, or luring prey.\n\nOne of the most stunning displays of bioluminescence can be seen in the ocean, where certain plankton emit light when disturbed, creating a mesmerizing blue glow in the water. This phenomenon is often referred to as \"sea sparkle\" and can be seen in coastal areas around the world.\n\nBioluminescence not only captivates our imagination but also has practical applications in science and medicine, including the development of biosensors and imaging techniques. It's a remarkable example of nature's creativity and complexity!",
          "sender": "Machine",
          "sender_name": "AI",
          "session_id": "chat-123",
          "timestamp": "2025-03-03T17:17:37+00:00",
          "flow_id": "d2bbd92b-187e-4c84-b2d4-5df365704201",
          "properties": {
            "source": {
              "id": "OpenAIModel-d1wOZ",
              "display_name": "OpenAI",
              "source": "gpt-4o-mini"
            },
            "icon": "OpenAI"
          },
          "component_id": "ChatOutput-ylMzN"
        }
      }
    }]
  }]
}
```
</details>

If you are parsing the response in an application, you most likely need to extract the relevant content from the response, rather than pass the entire response back to the user.
For an example of a script that extracts data from a Langflow API response, see the [Quickstart](/get-started-quickstart).

### Stream LLM token responses

With `/v1/run/$FLOW_ID`, the flow is executed as a batch with optional LLM token response streaming.

To stream LLM token responses, append the `?stream=true` query parameter to the request:

```bash
curl -X POST \
  "$LANGFLOW_SERVER_URL/api/v1/run/$FLOW_ID?stream=true" \
  -H "accept: application/json" \
  -H "Content-Type: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -d '{
    "message": "Tell me something interesting!",
    "session_id": "chat-123"
  }'
```

LLM chat responses are streamed back as `token` events, culminating in a final `end` event that closes the connection.

<details>
<summary>Result</summary>

The following example is truncated to illustrate a series of `token` events as well as the final `end` event that closes the LLM's token streaming response:

```text
{"event": "add_message", "data": {"timestamp": "2025-03-03T17:20:18", "sender": "User", "sender_name": "User", "session_id": "chat-123", "text": "Tell me about something interesting!", "files": [], "error": false, "edit": false, "properties": {"text_color": "", "background_color": "", "edited": false, "source": {"id": null, "display_name": null, "source": null}, "icon": "", "allow_markdown": false, "positive_feedback": null, "state": "complete", "targets": []}, "category": "message", "content_blocks": [], "id": "0103a21b-ebf7-4c02-9d72-017fb297f812", "flow_id": "d2bbd92b-187e-4c84-b2d4-5df365704201"}}

{"event": "add_message", "data": {"timestamp": "2025-03-03T17:20:18", "sender": "Machine", "sender_name": "AI", "session_id": "chat-123", "text": "", "files": [], "error": false, "edit": false, "properties": {"text_color": "", "background_color": "", "edited": false, "source": {"id": "OpenAIModel-d1wOZ", "display_name": "OpenAI", "source": "gpt-4o-mini"}, "icon": "OpenAI", "allow_markdown": false, "positive_feedback": null, "state": "complete", "targets": []}, "category": "message", "content_blocks": [], "id": "27b66789-e673-4c65-9e81-021752925161", "flow_id": "d2bbd92b-187e-4c84-b2d4-5df365704201"}}

{"event": "token", "data": {"chunk": " Have", "id": "27b66789-e673-4c65-9e81-021752925161", "timestamp": "2025-03-03 17:20:18 UTC"}}

{"event": "token", "data": {"chunk": " you", "id": "27b66789-e673-4c65-9e81-021752925161", "timestamp": "2025-03-03 17:20:18 UTC"}}

{"event": "token", "data": {"chunk": " ever", "id": "27b66789-e673-4c65-9e81-021752925161", "timestamp": "2025-03-03 17:20:18 UTC"}}

{"event": "token", "data": {"chunk": " heard", "id": "27b66789-e673-4c65-9e81-021752925161", "timestamp": "2025-03-03 17:20:18 UTC"}}

{"event": "token", "data": {"chunk": " of", "id": "27b66789-e673-4c65-9e81-021752925161", "timestamp": "2025-03-03 17:20:18 UTC"}}

{"event": "token", "data": {"chunk": " the", "id": "27b66789-e673-4c65-9e81-021752925161", "timestamp": "2025-03-03 17:20:18 UTC"}}

{"event": "token", "data": {"chunk": " phenomenon", "id": "27b66789-e673-4c65-9e81-021752925161", "timestamp": "2025-03-03 17:20:18 UTC"}}

{"event": "end", "data": {"result": {"session_id": "chat-123", "message": "Sure! Have you ever heard of the phenomenon known as \"bioluminescence\"?..."}}}
```

</details>

### Run endpoint headers

| Header | Info | Example |
|--------|------|---------|
| Content-Type | Required. Specifies the JSON format. | "application/json" |
| accept | Optional. Specifies the response format. | "application/json" |
| x-api-key | Optional. Required only if authentication is enabled. | "sk-..." |

### Run endpoint parameters

| Parameter | Type | Info |
|-----------|------|------|
| flow_id | UUID/string | Required. Part of URL: `/run/$FLOW_ID` |
| stream | Boolean | Optional. Query parameter: `/run/$FLOW_ID?stream=true` |
| input_value | string | Optional. JSON body field. Main input text/prompt. Default: `null` |
| input_type | string | Optional. JSON body field. Input type ("chat" or "text"). Default: `"chat"` |
| output_type | string | Optional. JSON body field. Output type ("chat", "any", "debug"). Default: `"chat"` |
| output_component | string | Optional. JSON body field. Target component for output. Default: `""` |
| tweaks | object | Optional. JSON body field. Component adjustments. Default: `null` |
| session_id | string | Optional. JSON body field. Conversation context ID. See [Session ID](/session-id). Default: `null` |

### Request example with all headers and parameters

```bash
curl -X POST \
  "$LANGFLOW_SERVER_URL/api/v1/run/$FLOW_ID?stream=true" \
  -H "Content-Type: application/json" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -d '{
    "input_value": "Tell me a story",
    "input_type": "chat",
    "output_type": "chat",
    "output_component": "chat_output",
    "session_id": "chat-123",
    "tweaks": {
      "component_id": {
        "parameter_name": "value"
      }
    }
  }'
```

## Webhook run flow

Use the `/webhook` endpoint to start a flow by sending an HTTP `POST` request.

:::tip
After you add a [**Webhook** component](/components-data#webhook) to a flow, open the [**API access** pane](/concepts-publish), and then click the **Webhook curl** tab to get an automatically generated `POST /webhook` request for your flow.
For more information, see [Trigger flows with webhooks](/webhook).
:::

```bash
curl -X POST \
  "$LANGFLOW_SERVER_URL/api/v1/webhook/$FLOW_ID" \
  -H "Content-Type: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -d '{"data": "example-data"}'
```

<details>
<summary>Result</summary>

```json
{
  "message": "Task started in the background",
  "status": "in progress"
}
```

</details>

## Deprecated flow trigger endpoints

The following endpoints are deprecated and replaced by the `/run` endpoint:

* `/process`
* `/predict`


================================================================================
filename: API-Reference/api-flows.mdx
================================================================================

---
title: Flow management endpoints
slug: /api-flows
---

Use the `/flows` endpoint to create, read, update, and delete flows.

If you want to use the Langflow API to run a flow, see [Flow trigger endpoints](/api-flows-run).

## Create flow

Creates a new flow.

```bash
curl -X POST \
  "$LANGFLOW_URL/api/v1/flows/" \
  -H "accept: application/json" \
  -H "Content-Type: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -d '{
  "name": "string2",
  "description": "string",
  "icon": "string",
  "icon_bg_color": "#FF0000",
  "gradient": "string",
  "data": {},
  "is_component": false,
  "updated_at": "2024-12-30T15:48:01.519Z",
  "webhook": false,
  "endpoint_name": "string",
  "tags": [
    "string"
  ]
}'
```

<details>
<summary>Result</summary>

```json
{
  "name": "string2",
  "description": "string",
  "icon": "string",
  "icon_bg_color": "#FF0000",
  "gradient": "string",
  "data": {},
  "is_component": false,
  "updated_at": "2025-02-04T21:07:36+00:00",
  "webhook": false,
  "endpoint_name": "string",
  "tags": ["string"],
  "locked": false,
  "id": "e8d81c37-714b-49ae-ba82-e61141f020ee",
  "user_id": "f58396d4-a387-4bb8-b749-f40825c3d9f3",
  "project_id": "1415de42-8f01-4f36-bf34-539f23e47466"
}
```

</details>

## Create flows

Creates multiple new flows, returning an array of flow objects.

```bash
curl -X POST \
  "$LANGFLOW_URL/api/v1/flows/batch/" \
  -H "accept: application/json" \
  -H "Content-Type: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -d '{
  "flows": [
    {
      "name": "string",
      "description": "string",
      "icon": "string",
      "icon_bg_color": "string",
      "gradient": "string",
      "data": {},
      "is_component": false,
      "updated_at": "2024-12-30T18:36:02.737Z",
      "webhook": false,
      "endpoint_name": "string",
      "tags": [
        "string"
      ],
      "locked": false,
      "user_id": "3fa85f64-5717-4562-b3fc-2c963f66afa6",
      "project_id": "3fa85f64-5717-4562-b3fc-2c963f66afa6"
    },
    {
      "name": "string",
      "description": "string",
      "icon": "string",
      "icon_bg_color": "string",
      "gradient": "string",
      "data": {},
      "is_component": false,
      "updated_at": "2024-12-30T18:36:02.737Z",
      "webhook": false,
      "endpoint_name": "string",
      "tags": [
        "string"
      ],
      "locked": false,
      "user_id": "3fa85f64-5717-4562-b3fc-2c963f66afa6",
      "project_id": "3fa85f64-5717-4562-b3fc-2c963f66afa6"
    }
  ]
}'
```

## Read flow

Retrieves a specific flow by its ID.

```bash
curl -X GET \
  "$LANGFLOW_URL/api/v1/flows/$FLOW_ID" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

<details>
<summary>Result</summary>

```json
{
  "name": "Basic Prompting",
  "description": "Perform basic prompting with an OpenAI model.",
  "icon": "Braces",
  "icon_bg_color": null,
  "gradient": "2",
  "data": {
    "nodes": [
     ...
    ]
  }
}
```

</details>

## Read flows

Returns a JSON object containing a list of flows.

Retrieve all flows with pagination:

```bash
curl -X GET \
  "$LANGFLOW_URL/api/v1/flows/?remove_example_flows=false&components_only=false&get_all=true&header_flows=false&page=1&size=50" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

To retrieve flows from a specific project, use the `project_id` query parameter:

```bash
curl -X GET \
  "$LANGFLOW_URL/api/v1/flows/?remove_example_flows=true&components_only=false&get_all=false&project_id=$PROJECT_ID&header_flows=false&page=1&size=1" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

## Read sample flows

Retrieves a list of sample flows:

```bash
curl -X GET \
  "$LANGFLOW_URL/api/v1/flows/basic_examples/" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

## Update flow

Updates an existing flow by its ID.

This example changes the value for `endpoint_name` from a random UUID to `my_new_endpoint_name`.

```bash
curl -X PATCH \
  "$LANGFLOW_URL/api/v1/flows/$FLOW_ID" \
  -H "accept: application/json" \
  -H "Content-Type: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -d '{
  "name": "string",
  "description": "string",
  "data": {},
  "project_id": "3fa85f64-5717-4562-b3fc-2c963f66afa6",
  "endpoint_name": "my_new_endpoint_name",
  "locked": true
}'
```

<details>
<summary>Result</summary>

```json
{
  "name": "string",
  "description": "string",
  "icon": "Braces",
  "icon_bg_color": null,
  "gradient": "2",
  "data": {},
  "is_component": false,
  "updated_at": "2024-12-30T18:30:22+00:00",
  "webhook": false,
  "endpoint_name": "my_new_endpoint_name",
  "tags": null,
  "locked": true,
  "id": "01ce083d-748b-4b8d-97b6-33adbb6a528a",
  "user_id": "f58396d4-a387-4bb8-b749-f40825c3d9f3",
  "project_id": "3fa85f64-5717-4562-b3fc-2c963f66afa6"
}
```

</details>

## Delete flow

Deletes a specific flow by its ID.

```bash
curl -X DELETE \
  "$LANGFLOW_URL/api/v1/flows/$FLOW_ID" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

<details>
<summary>Result</summary>

```json
{
  "message": "Flow deleted successfully"
}
```

</details>

## Export flows

Exports specified flows to a ZIP file.

This endpoint downloads a ZIP file containing [Langflow JSON files](/concepts-flows-import#langflow-json-file-contents) for each flow ID listed in the request body.

```bash
curl -X POST \
  "$LANGFLOW_URL/api/v1/flows/download/" \
  -H "accept: application/json" \
  -H "Content-Type: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -d '[
  "e1e40c77-0541-41a9-88ab-ddb3419398b5",
  "92f9a4c5-cfc8-4656-ae63-1f0881163c28"
]' \
  --output langflow-flows.zip
```

<details>
<summary>Result</summary>

```text
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 76437    0 76353  100    84  4516k   5088 --:--:-- --:--:-- --:--:-- 4665k
```

</details>

## Import flows

Imports flows by uploading a [Langflow-compatible JSON file](/concepts-flows-import#langflow-json-file-contents).

To specify a target project for the flow, include the query parameter `folder_id`.
The target `folder_id` must already exist before uploading a flow. Call the [/api/v1/projects/](/api-projects#read-projects) endpoint for a list of available folders and projects.

This example uploads a local file named `agent-with-astra-db-tool.json` to a folder specified by a `FOLDER_ID` variable:

```bash
curl -X POST \
  "$LANGFLOW_URL/api/v1/flows/upload/?folder_id=$FOLDER_ID" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -F "file=@agent-with-astra-db-tool.json;type=application/json"
```

<details>
<summary>Result</summary>

```json
[
  {
    "name": "agent-with-astra-db-tool",
    "description": "",
    "icon": null,
    "icon_bg_color": null,
    "gradient": null,
    "data": {}
  ...
  }
]
```
</details>


================================================================================
filename: API-Reference/api-logs.mdx
================================================================================

---
title: Logs endpoints
slug: /api-logs
---

Retrieve logs for your Langflow flows and server.

## Enable log retrieval

The `/logs` endpoint requires log retrieval to be enabled in your Langflow instance.

To enable log retrieval, include set the following [environment variables](/environment-variables) in your Langflow `.env` file, and then start Langflow with `uv run langflow run --env-file .env`:

```text
LANGFLOW_ENABLE_LOG_RETRIEVAL=True
LANGFLOW_LOG_RETRIEVER_BUFFER_SIZE=10000. // Must be greater than 0
LANGFLOW_LOG_LEVEL=DEBUG. // Can be DEBUG, ERROR, INFO, WARNING, or CRITICAL
```

## Stream logs

Stream logs in real-time using Server Sent Events (SSE):

```bash
curl -X GET \
  "$LANGFLOW_URL/logs-stream" \
  -H "accept: text/event-stream" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

<details>
<summary>Result</summary>

```text
keepalive

{"1736355791151": "2025-01-08T12:03:11.151218-0500 DEBUG Building Chat Input\n"}

{"1736355791485": "2025-01-08T12:03:11.485380-0500 DEBUG consumed event add_message-153bcd5d-ef4d-4ece-8cc0-47c6b6a9ef92 (time in queue, 0.0000, client 0.0001)\n"}

{"1736355791499": "2025-01-08T12:03:11.499704-0500 DEBUG consumed event end_vertex-3d7125cd-7b8a-44eb-9113-ed5b785e3cf3 (time in queue, 0.0056, client 0.0047)\n"}

{"1736355791502": "2025-01-08T12:03:11.502510-0500 DEBUG consumed event end-40d0b363-5618-4a23-bbae-487cd0b9594d (time in queue, 0.0001, client 0.0004)\n"}

{"1736355791513": "2025-01-08T12:03:11.513097-0500 DEBUG Logged vertex build: 729ff2f8-6b01-48c8-9ad0-3743c2af9e8a\n"}

{"1736355791834": "2025-01-08T12:03:11.834982-0500 DEBUG Telemetry data sent successfully.\n"}

{"1736355791941": "2025-01-08T12:03:11.941840-0500 DEBUG Telemetry data sent successfully.\n"}

keepalive
```

</details>

## Retrieve logs with optional parameters

Retrieve logs with optional query parameters:

- `lines_before`: The number of logs before the timestamp or the last log.
- `lines_after`: The number of logs after the timestamp.
- `timestamp`: The timestamp to start getting logs from.

The default values for all three parameters is `0`.
With default values, the endpoint returns the last 10 lines of logs.

```bash
curl -X GET \
  "$LANGFLOW_URL/logs?lines_before=0&lines_after=0&timestamp=0" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

<details>
<summary>Result</summary>

```text
{
  "1736354770500": "2025-01-08T11:46:10.500363-0500 DEBUG Creating starter project Document Q&A\n",
  "1736354770511": "2025-01-08T11:46:10.511146-0500 DEBUG Creating starter project Image Sentiment Analysis\n",
  "1736354770521": "2025-01-08T11:46:10.521018-0500 DEBUG Creating starter project SEO Keyword Generator\n",
  "1736354770532": "2025-01-08T11:46:10.532677-0500 DEBUG Creating starter project Sequential Tasks Agents\n",
  "1736354770544": "2025-01-08T11:46:10.544010-0500 DEBUG Creating starter project Custom Component Generator\n",
  "1736354770555": "2025-01-08T11:46:10.555513-0500 DEBUG Creating starter project Prompt Chaining\n",
  "1736354770588": "2025-01-08T11:46:10.588105-0500 DEBUG Create service ServiceType.CHAT_SERVICE\n",
  "1736354771021": "2025-01-08T11:46:11.021817-0500 DEBUG Telemetry data sent successfully.\n",
  "1736354775619": "2025-01-08T11:46:15.619545-0500 DEBUG Create service ServiceType.STORE_SERVICE\n",
  "1736354775699": "2025-01-08T11:46:15.699661-0500 DEBUG File 046-rocket.svg retrieved successfully from flow /Users/USER/Library/Caches/langflow/profile_pictures/Space.\n"
}
```

</details>


================================================================================
filename: API-Reference/api-monitor.mdx
================================================================================

---
title: Monitor endpoints
slug: /api-monitor
---

The `/monitor` endpoints are for internal Langflow functionality, primarily related to running flows in the **Playground**, storing chat history, and generating flow logs.

This information is primarily for those who are building custom components or contributing to the Langflow codebase in a way that requires calling or understanding these endpoints.

For typical application development with Langflow, there are more appropriate options for monitoring, debugging, and memory management.
For more information, see the following:

* [Logs](/logging): Langflow log storage locations, customization options, and where to view logs in the visual editor
* [Test flows in the Playground](/concepts-playground): Run flows and inspect message history
* [Memory management options](/memory): Langflow storage locations and options, including the database, cache, and chat history

## Vertex builds

The Vertex build endpoints (`/monitor/builds`) are exclusively for **Playground** functionality.

When you run a flow in the **Playground**, Langflow calls the `/build/$FLOW_ID/flow` endpoint in [chat.py](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/api/v1/chat.py#L143). This call retrieves the flow data, builds a graph, and executes the graph. As each component (or node) is executed, the `build_vertex` function calls `build_and_run`, which may call the individual components' `def_build` method, if it exists. If a component doesn't have a `def_build` function, the build still returns a component.

The `build` function allows components to execute logic at runtime. For example, the [**Recursive Character Text Splitter** component](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/components/langchain_utilities/recursive_character.py) is a child of the `LCTextSplitterComponent` class. When text needs to be processed, the parent class's `build` method is called, which creates a `RecursiveCharacterTextSplitter` object and uses it to split the text according to the defined parameters. The split text is then passed on to the next component. This all occurs when the component is built.

### Get Vertex builds

Retrieve Vertex builds for a specific flow.

```bash
curl -X GET \
  "$LANGFLOW_URL/api/v1/monitor/builds?flow_id=$FLOW_ID" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

<details>
<summary>Result</summary>

```json
{
  "vertex_builds": {
    "ChatInput-NCmix": [
      {
        "data": {
          "results": {
            "message": {
              "text_key": "text",
              "data": {
                "timestamp": "2024-12-23 19:10:57",
                "sender": "User",
                "sender_name": "User",
                "session_id": "01ce083d-748b-4b8d-97b6-33adbb6a528a",
                "text": "Hello",
                "files": [],
                "error": "False",
                "edit": "False",
                "properties": {
                  "text_color": "",
                  "background_color": "",
                  "edited": "False",
                  "source": {
                    "id": "None",
                    "display_name": "None",
                    "source": "None"
                  },
                  "icon": "",
                  "allow_markdown": "False",
                  "positive_feedback": "None",
                  "state": "complete",
                  "targets": []
                },
                "category": "message",
                "content_blocks": [],
                "id": "c95bed34-f906-4aa6-84e4-68553f6db772",
                "flow_id": "01ce083d-748b-4b8d-97b6-33adbb6a528a"
              },
              "default_value": "",
              "text": "Hello",
              "sender": "User",
              "sender_name": "User",
              "files": [],
              "session_id": "01ce083d-748b-4b8d-97b6-33adbb6a528a",
              "timestamp": "2024-12-23 19:10:57+00:00",
              "flow_id": "01ce083d-748b-4b8d-97b6-33adbb6a528a",
              "error": "False",
              "edit": "False",
              "properties": {
                "text_color": "",
                "background_color": "",
                "edited": "False",
                "source": {
                  "id": "None",
                  "display_name": "None",
                  "source": "None"
                },
                "icon": "",
                "allow_markdown": "False",
                "positive_feedback": "None",
                "state": "complete",
                "targets": []
              },
              "category": "message",
              "content_blocks": []
            }
          },
          "outputs": {
            "message": {
              "message": {
                "timestamp": "2024-12-23T19:10:57",
                "sender": "User",
                "sender_name": "User",
                "session_id": "01ce083d-748b-4b8d-97b6-33adbb6a528a",
                "text": "Hello",
                "files": [],
                "error": false,
                "edit": false,
                "properties": {
                  "text_color": "",
                  "background_color": "",
                  "edited": false,
                  "source": {
                    "id": null,
                    "display_name": null,
                    "source": null
                  },
                  "icon": "",
                  "allow_markdown": false,
                  "positive_feedback": null,
                  "state": "complete",
                  "targets": []
                },
                "category": "message",
                "content_blocks": [],
                "id": "c95bed34-f906-4aa6-84e4-68553f6db772",
                "flow_id": "01ce083d-748b-4b8d-97b6-33adbb6a528a"
              },
              "type": "object"
            }
          },
          "logs": { "message": [] },
          "message": {
            "message": "Hello",
            "sender": "User",
            "sender_name": "User",
            "files": [],
            "type": "object"
          },
          "artifacts": {
            "message": "Hello",
            "sender": "User",
            "sender_name": "User",
            "files": [],
            "type": "object"
          },
          "timedelta": 0.015060124918818474,
          "duration": "15 ms",
          "used_frozen_result": false
        },
        "artifacts": {
          "message": "Hello",
          "sender": "User",
          "sender_name": "User",
          "files": [],
          "type": "object"
        },
        "params": "- Files: []\n  Message: Hello\n  Sender: User\n  Sender Name: User\n  Type: object\n",
        "valid": true,
        "build_id": "40aa200e-74db-4651-b698-f80301d2b26b",
        "id": "ChatInput-NCmix",
        "timestamp": "2024-12-23T19:10:58.772766Z",
        "flow_id": "01ce083d-748b-4b8d-97b6-33adbb6a528a"
      }
    ],
    "Prompt-BEn9c": [
      {
        "data": {
          "results": {},
          "outputs": {
            "prompt": {
              "message": "Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh.",
              "type": "text"
            }
          },
          "logs": { "prompt": [] },
          "message": {
            "prompt": {
              "repr": "Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh.",
              "raw": "Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh.",
              "type": "text"
            }
          },
          "artifacts": {
            "prompt": {
              "repr": "Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh.",
              "raw": "Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh.",
              "type": "text"
            }
          },
          "timedelta": 0.0057758750626817346,
          "duration": "6 ms",
          "used_frozen_result": false
        },
        "artifacts": {
          "prompt": {
            "repr": "Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh.",
            "raw": "Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh.",
            "type": "text"
          }
        },
        "params": "None",
        "valid": true,
        "build_id": "39bbbfde-97fd-42a5-a9ed-d42a5c5d532b",
        "id": "Prompt-BEn9c",
        "timestamp": "2024-12-23T19:10:58.781019Z",
        "flow_id": "01ce083d-748b-4b8d-97b6-33adbb6a528a"
      }
    ],
    "OpenAIModel-7AjrN": [
      {
        "data": {
          "results": {},
          "outputs": {
            "text_output": {
              "message": "Hello! ðŸŒŸ I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!",
              "type": "text"
            },
            "model_output": { "message": "", "type": "unknown" }
          },
          "logs": { "text_output": [] },
          "message": {
            "text_output": {
              "repr": "Hello! ðŸŒŸ I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!",
              "raw": "Hello! ðŸŒŸ I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!",
              "type": "text"
            }
          },
          "artifacts": {
            "text_output": {
              "repr": "Hello! ðŸŒŸ I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!",
              "raw": "Hello! ðŸŒŸ I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!",
              "type": "text"
            }
          },
          "timedelta": 1.034765167045407,
          "duration": "1.03 seconds",
          "used_frozen_result": false
        },
        "artifacts": {
          "text_output": {
            "repr": "Hello! ðŸŒŸ I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!",
            "raw": "Hello! ðŸŒŸ I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!",
            "type": "text"
          }
        },
        "params": "None",
        "valid": true,
        "build_id": "4f0ae730-a266-4d35-b89f-7b825c620a0f",
        "id": "OpenAIModel-7AjrN",
        "timestamp": "2024-12-23T19:10:58.790484Z",
        "flow_id": "01ce083d-748b-4b8d-97b6-33adbb6a528a"
      }
    ],
    "ChatOutput-sfUhT": [
      {
        "data": {
          "results": {
            "message": {
              "text_key": "text",
              "data": {
                "timestamp": "2024-12-23 19:10:58",
                "sender": "Machine",
                "sender_name": "AI",
                "session_id": "01ce083d-748b-4b8d-97b6-33adbb6a528a",
                "text": "Hello! ðŸŒŸ I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!",
                "files": [],
                "error": "False",
                "edit": "False",
                "properties": {
                  "text_color": "",
                  "background_color": "",
                  "edited": "False",
                  "source": {
                    "id": "OpenAIModel-7AjrN",
                    "display_name": "OpenAI",
                    "source": "gpt-4o-mini"
                  },
                  "icon": "OpenAI",
                  "allow_markdown": "False",
                  "positive_feedback": "None",
                  "state": "complete",
                  "targets": []
                },
                "category": "message",
                "content_blocks": [],
                "id": "5688356d-9f30-40ca-9907-79a7a2fc16fd",
                "flow_id": "01ce083d-748b-4b8d-97b6-33adbb6a528a"
              },
              "default_value": "",
              "text": "Hello! ðŸŒŸ I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!",
              "sender": "Machine",
              "sender_name": "AI",
              "files": [],
              "session_id": "01ce083d-748b-4b8d-97b6-33adbb6a528a",
              "timestamp": "2024-12-23 19:10:58+00:00",
              "flow_id": "01ce083d-748b-4b8d-97b6-33adbb6a528a",
              "error": "False",
              "edit": "False",
              "properties": {
                "text_color": "",
                "background_color": "",
                "edited": "False",
                "source": {
                  "id": "OpenAIModel-7AjrN",
                  "display_name": "OpenAI",
                  "source": "gpt-4o-mini"
                },
                "icon": "OpenAI",
                "allow_markdown": "False",
                "positive_feedback": "None",
                "state": "complete",
                "targets": []
              },
              "category": "message",
              "content_blocks": []
            }
          },
          "outputs": {
            "message": {
              "message": {
                "timestamp": "2024-12-23T19:10:58",
                "sender": "Machine",
                "sender_name": "AI",
                "session_id": "01ce083d-748b-4b8d-97b6-33adbb6a528a",
                "text": "Hello! ðŸŒŸ I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!",
                "files": [],
                "error": false,
                "edit": false,
                "properties": {
                  "text_color": "",
                  "background_color": "",
                  "edited": false,
                  "source": {
                    "id": "OpenAIModel-7AjrN",
                    "display_name": "OpenAI",
                    "source": "gpt-4o-mini"
                  },
                  "icon": "OpenAI",
                  "allow_markdown": false,
                  "positive_feedback": null,
                  "state": "complete",
                  "targets": []
                },
                "category": "message",
                "content_blocks": [],
                "id": "5688356d-9f30-40ca-9907-79a7a2fc16fd",
                "flow_id": "01ce083d-748b-4b8d-97b6-33adbb6a528a"
              },
              "type": "object"
            }
          },
          "logs": { "message": [] },
          "message": {
            "message": "Hello! ðŸŒŸ I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!",
            "sender": "Machine",
            "sender_name": "AI",
            "files": [],
            "type": "object"
          },
          "artifacts": {
            "message": "Hello! ðŸŒŸ I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!",
            "sender": "Machine",
            "sender_name": "AI",
            "files": [],
            "type": "object"
          },
          "timedelta": 0.017838125000707805,
          "duration": "18 ms",
          "used_frozen_result": false
        },
        "artifacts": {
          "message": "Hello! ðŸŒŸ I'm excited to help you get started on your journey to building something fresh! What do you have in mind? Whether it's a project, an idea, or a concept, let's dive in and make it happen!",
          "sender": "Machine",
          "sender_name": "AI",
          "files": [],
          "type": "object"
        },
        "params": "- Files: []\n  Message: Hello! ðŸŒŸ I'm excited to help you get started on your journey to building\n    something fresh! What do you have in mind? Whether it's a project, an idea, or\n    a concept, let's dive in and make it happen!\n  Sender: Machine\n  Sender Name: AI\n  Type: object\n",
        "valid": true,
        "build_id": "1e8b908b-aba7-403b-9e9b-eca92bb78668",
        "id": "ChatOutput-sfUhT",
        "timestamp": "2024-12-23T19:10:58.813268Z",
        "flow_id": "01ce083d-748b-4b8d-97b6-33adbb6a528a"
      }
    ]
  }
}
```

</details>

### Delete Vertex builds

Delete Vertex builds for a specific flow.

```bash
curl -X DELETE \
  "$LANGFLOW_URL/api/v1/monitor/builds?flow_id=$FLOW_ID" \
  -H "accept: */*" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

<details>
<summary>Result</summary>

```text
204 No Content
```

</details>

## Messages endpoints

The `/monitor/messages` endpoints store, retrieve, edit, and delete records in the message table in [`langflow.db`](/memory)
Typically, these are called implicitly when running flows that produce message history, or when inspecting and modifying **Playground** memories.

### Get messages

Retrieve a list of all messages:

```bash
curl -X GET \
  "$LANGFLOW_URL/api/v1/monitor/messages" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

To filter messages, use the `flow_id`, `session_id`, `sender`, and `sender_name` query parameters.

To sort the results, use the `order_by` query parameter.

This example retrieves messages sent by `Machine` and `AI` in a given chat session (`session_id`) and orders the messages by timestamp.

```bash
curl -X GET \
  "$LANGFLOW_URL/api/v1/monitor/messages?flow_id=$FLOW_ID&session_id=01ce083d-748b-4b8d-97b6-33adbb6a528a&sender=Machine&sender_name=AI&order_by=timestamp" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

<details>
<summary>Result</summary>

```json
[
  {
    "id": "1c1d6134-9b8b-4079-931c-84dcaddf19ba",
    "flow_id": "01ce083d-748b-4b8d-97b6-33adbb6a528a",
    "timestamp": "2024-12-23 19:20:11 UTC",
    "sender": "Machine",
    "sender_name": "AI",
    "session_id": "01ce083d-748b-4b8d-97b6-33adbb6a528a",
    "text": "Hello! It's great to see you here! What exciting project or idea are you thinking about diving into today? Whether it's something fresh and innovative or a classic concept with a twist, I'm here to help you get started! Let's brainstorm together!",
    "files": "[]",
    "edit": false,
    "properties": {
      "text_color": "",
      "background_color": "",
      "edited": false,
      "source": {
        "id": "OpenAIModel-7AjrN",
        "display_name": "OpenAI",
        "source": "gpt-4o-mini"
      },
      "icon": "OpenAI",
      "allow_markdown": false,
      "positive_feedback": null,
      "state": "complete",
      "targets": []
    },
    "category": "message",
    "content_blocks": []
  }
]
```

</details>

### Delete messages

Delete specific messages by their IDs.

This example deletes the message retrieved in the previous `GET /messages` example.

```bash
curl -v -X DELETE \
  "$LANGFLOW_URL/api/v1/monitor/messages" \
  -H "accept: */*" \
  -H "Content-Type: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -d '["MESSAGE_ID_1", "MESSAGE_ID_2"]'
```

<details>
<summary>Result</summary>

```text
204 No Content
```

</details>

### Update message

Update a specific message by its ID.

This example updates the `text` value of message `3ab66cc6-c048-48f8-ab07-570f5af7b160`.

```bash
curl -X PUT \
  "$LANGFLOW_URL/api/v1/monitor/messages/3ab66cc6-c048-48f8-ab07-570f5af7b160" \
  -H "accept: application/json" \
  -H "Content-Type: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -d '{
  "text": "testing 1234"
}'
```

<details>
<summary>Result</summary>

```json
{
  "timestamp": "2024-12-23T18:49:06",
  "sender": "string",
  "sender_name": "string",
  "session_id": "01ce083d-748b-4b8d-97b6-33adbb6a528a",
  "text": "testing 1234",
  "files": ["string"],
  "error": true,
  "edit": true,
  "properties": {
    "text_color": "string",
    "background_color": "string",
    "edited": false,
    "source": { "id": "string", "display_name": "string", "source": "string" },
    "icon": "string",
    "allow_markdown": false,
    "positive_feedback": true,
    "state": "complete",
    "targets": []
  },
  "category": "message",
  "content_blocks": [],
  "id": "3ab66cc6-c048-48f8-ab07-570f5af7b160",
  "flow_id": "01ce083d-748b-4b8d-97b6-33adbb6a528a"
}
```

</details>

### Update session ID

Update the session ID for messages.

This example updates the `session_ID` value `01ce083d-748b-4b8d-97b6-33adbb6a528a` to `different_session_id`.

```bash
curl -X PATCH \
  "$LANGFLOW_URL/api/v1/monitor/messages/session/01ce083d-748b-4b8d-97b6-33adbb6a528a?new_session_id=different_session_id" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

<details>
<summary>Result</summary>

```json
[
  {
    "id": "8dd7f064-e63a-4773-b472-ca0475249dfd",
    "flow_id": "01ce083d-748b-4b8d-97b6-33adbb6a528a",
    "timestamp": "2024-12-23 18:49:55 UTC",
    "sender": "User",
    "sender_name": "User",
    "session_id": "different_session_id",
    "text": "message",
    "files": "[]",
    "edit": false,
    "properties": {
      "text_color": "",
      "background_color": "",
      "edited": false,
      "source": {
        "id": null,
        "display_name": null,
        "source": null
      },
      "icon": "",
      "allow_markdown": false,
      "positive_feedback": null,
      "state": "complete",
      "targets": []
    },
    "category": "message",
    "content_blocks": []
  }
]
```

</details>

### Delete messages by session

Delete all messages for a specific session.

```bash
curl -X DELETE \
  "$LANGFLOW_URL/api/v1/monitor/messages/session/different_session_id_2" \
  -H "accept: */*" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

<details>
<summary>Result</summary>

```text
HTTP/1.1 204 No Content
```

</details>

## Get transactions

Retrieve all transactions, which are interactions between components, for a specific flow.
This information is also available in [flow logs](/logging).

```bash
curl -X GET \
  "$LANGFLOW_URL/api/v1/monitor/transactions?flow_id=$FLOW_ID&page=1&size=50" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

<details>
<summary>Result</summary>

```json
{
  "items": [
    {
      "timestamp": "2024-12-23T20:05:01.061Z",
      "vertex_id": "string",
      "target_id": "string",
      "inputs": {},
      "outputs": {},
      "status": "string",
      "error": "string",
      "flow_id": "3fa85f64-5717-4562-b3fc-2c963f66afa6",
      "id": "3fa85f64-5717-4562-b3fc-2c963f66afa6"
    }
  ],
  "total": 0,
  "page": 1,
  "size": 1,
  "pages": 0
}
```

</details>

## See also

- [Use voice mode](/concepts-voice-mode)
- [Session ID](/session-id)


================================================================================
filename: API-Reference/api-openai-responses.mdx
================================================================================

---
title: OpenAI Responses API
slug: /api-openai-responses
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

Langflow includes an endpoint that is compatible with the [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses).
It is available at `POST /api/v1/responses`.

This endpoint allows you to use existing OpenAI client libraries with minimal code changes.
You only need to replace the `model` name, such as `gpt-4`, with your `flow_id`.
You can find Flow IDs in the code snippets on the [**API access** pane](/concepts-publish#api-access) or in a flow's URL.

## Prerequisites

To be compatible with Langflow's OpenAI Responses API endpoint, your flow and request must adhere to the following requirements:

- **Chat Input**: Your flow must contain a **Chat Input** component.
Flows without this component return an error when passed to this endpoint.
The component types `ChatInput` and `Chat Input` are recognized as chat inputs.
- **Tools**: The `tools` parameter isn't supported, and returns an error if provided.
- **Model Names**: In your request, the `model` field must contain a valid flow ID or endpoint name.
- **Authentication**: All requests require an API key passed in the `x-api-key` header.
For more information, see [API keys and authentication](/api-keys-and-authentication).

### Additional configuration for OpenAI client libraries

This endpoint is compatible with OpenAI's API, but requires special configuration when using OpenAI client libraries.
Langflow uses `x-api-key` headers for authentication, while OpenAI uses `Authorization: Bearer` headers.
When sending requests to Langflow with OpenAI client libraries, you must configure custom headers and include an `api_key` configuration.
The `api_key` parameter can have any value, such as `"dummy-api-key"` in the client examples, as the actual authentication is handled through the `default_headers` configuration.

In the following examples, replace the values for `LANGFLOW_SERVER_URL`, `LANGFLOW_API_KEY`, and `FLOW_ID` with values from your deployment.
<Tabs groupId="client">
<TabItem value="Python" label="OpenAI Python Client" default>

```python
from openai import OpenAI

client = OpenAI(
  base_url="LANGFLOW_SERVER_URL/api/v1/",
  default_headers={"x-api-key": "LANGFLOW_API_KEY"},
  api_key="dummy-api-key" # Required by OpenAI SDK but not used by Langflow
)

response = client.responses.create(
  model="FLOW_ID",
  input="There is an event that happens on the second wednesday of every month. What are the event dates in 2026?",
)

print(response.output_text)
```

</TabItem>
<TabItem value="JavaScript" label="OpenAI TypeScript Client">

```typescript
import OpenAI from "openai";

const client = new OpenAI({
  baseURL: "LANGFLOW_SERVER_URL/api/v1/",
  defaultHeaders: {
    "x-api-key": "LANGFLOW_API_KEY"
  },
  apiKey: "dummy-api-key" // Required by OpenAI SDK but not used by Langflow
});

const response = await client.responses.create({
  model: "FLOW_ID",
  input: "There is an event that happens on the second wednesday of every month. What are the event dates in 2026?"
});

console.log(response.output_text);
```

</TabItem>
</Tabs>

<details closed>
<summary>Response</summary>
```text
Here are the event dates for the second Wednesday of each month in 2026:
- January 14, 2026
- February 11, 2026
- March 11, 2026
- April 8, 2026
- May 13, 2026
- June 10, 2026
- July 8, 2026
- August 12, 2026
- September 9, 2026
- October 14, 2026
- November 11, 2026
- December 9, 2026
If you need these in a different format or want a downloadable calendar, let me know!
```
</details>

## Example request

```bash
curl -X POST \
  "$LANGFLOW_SERVER_URL/api/v1/responses" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "$YOUR_FLOW_ID",
    "input": "Hello, how are you?",
    "stream": false
  }'
```

### Headers

| Header | Required | Description | Example |
|--------|----------|-------------|---------|
| `x-api-key` | Yes | Your Langflow API key for authentication | `"sk-..."` |
| `Content-Type` | Yes | Specifies the JSON format | `"application/json"` |
| `X-LANGFLOW-GLOBAL-VAR-*` | No | Global variables for the flow | `"X-LANGFLOW-GLOBAL-VAR-API_KEY: sk-..."` For more, see [Pass global variables to your flows in headers](#global-var). |

### Request body

| Field | Type | Required | Default | Description |
|-------|------|----------|---------|-------------|
| `model` | `string` | Yes | - | The flow ID or endpoint name to execute. |
| `input` | `string` | Yes | - | The input text to process. |
| `stream` | `boolean` | No | `false` | Whether to stream the response. |
| `background` | `boolean` | No | `false` | Whether to process in background. |
| `tools` | `list[Any]` | No | `null` | Tools are not supported yet. |
| `previous_response_id` | `string` | No | `null` | ID of previous response to continue conversation. For more, see [Continue conversations with response and session IDs](#response-id). |
| `include` | `list[string]` | No | `null` | Additional response data to include, such as `['tool_call.results']`. For more, see [Retrieve tool call results](#tool-call-results). |

## Example response

```json
{
  "id": "e5e8ef8a-7efd-4090-a110-6aca082bceb7",
  "object": "response",
  "created_at": 1756837941,
  "status": "completed",
  "model": "ced2ec91-f325-4bf0-8754-f3198c2b1563",
  "output": [
    {
      "type": "message",
      "id": "msg_e5e8ef8a-7efd-4090-a110-6aca082bceb7",
      "status": "completed",
      "role": "assistant",
      "content": [
        {
          "type": "output_text",
          "text": "Hello! I'm here and ready to help. How can I assist you today?",
          "annotations": []
        }
      ]
    }
  ],
  "parallel_tool_calls": true,
  "previous_response_id": null,
  "reasoning": {"effort": null, "summary": null},
  "store": true,
  "temperature": 1.0,
  "text": {"format": {"type": "text"}},
  "tool_choice": "auto",
  "tools": [],
  "top_p": 1.0,
  "truncation": "disabled",
  "usage": null,
  "user": null,
  "metadata": {}
}
```

### Response body

The response contains fields that Langflow sets dynamically and fields that use OpenAI-compatible defaults.

The OpenAI-compatible default values shown above are currently fixed and cannot be modified via the request.
They are included to maintain API compatibility and provide a consistent response format.

For your requests, you will only be setting the dynamic fields.
The default values are documented here for completeness and to show the full response structure.

Fields set dynamically by Langflow:

| Field | Type | Description |
|-------|------|-------------|
| `id` | `string` | Unique response identifier. |
| `created_at` | `int` | Unix timestamp of response creation. |
| `model` | `string` | The flow ID that was executed. |
| `output` | `list[dict]` | Array of output items (messages, tool calls, etc.). |
| `previous_response_id` | `string` | ID of previous response if continuing conversation. |

<details>
<summary>Fields with OpenAI-compatible default values</summary>

| Field | Type | Default Value | Description |
|-------|------|---------------|-------------|
| `object` | `string` | `"response"` | Always `"response"`. |
| `status` | `string` | `"completed"` | Response status: `"completed"`, `"in_progress"`, or `"failed"`. |
| `error` | `dict` | `null` | Error details (if any). |
| `incomplete_details` | `dict` | `null` | Incomplete response details (if any). |
| `instructions` | `string` | `null` | Response instructions (if any). |
| `max_output_tokens` | `int` | `null` | Maximum output tokens (if any). |
| `parallel_tool_calls` | `boolean` | `true` | Whether parallel tool calls are enabled. |
| `reasoning` | `dict` | `{"effort": null, "summary": null}` | Reasoning information with effort and summary. |
| `store` | `boolean` | `true` | Whether response is stored. |
| `temperature` | `float` | `1.0` | Temperature setting. |
| `text` | `dict` | `{"format": {"type": "text"}}` | Text format configuration. |
| `tool_choice` | `string` | `"auto"` | Tool choice setting. |
| `tools` | `list[dict]` | `[]` | Available tools. |
| `top_p` | `float` | `1.0` | Top-p setting. |
| `truncation` | `string` | `"disabled"` | Truncation setting. |
| `usage` | `dict` | `null` | Usage statistics (if any). |
| `user` | `string` | `null` | User identifier (if any). |
| `metadata` | `dict` | `{}` | Additional metadata. |

</details>

## Example streaming request

When you set `"stream": true` with your request, the API returns a stream where each chunk contains a small piece of the response as it's generated. This provides a real-time experience where users can see the AI's output appear word by word, similar to ChatGPT's typing effect.

```bash
curl -X POST \
  "$LANGFLOW_SERVER_URL/api/v1/responses" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "$FLOW_ID",
    "input": "Tell me a story about a robot",
    "stream": true
  }'
```

<details>
<summary>Result</summary>

```json
{
  "id": "f7fcea36-f128-41c4-9ac1-e683137375d5",
  "object": "response.chunk",
  "created": 1756838094,
  "model": "ced2ec91-f325-4bf0-8754-f3198c2b1563",
  "delta": {
    "content": "Once"
  },
  "status": null
}
```

</details>

### Streaming response body

| Field | Type | Description |
|-------|------|-------------|
| `id` | `string` | Unique response identifier. |
| `object` | `string` | Always `"response.chunk"`. |
| `created` | `int` | Unix timestamp of chunk creation. |
| `model` | `string` | The flow ID that was executed. |
| `delta` | `dict` | The new content chunk. |
| `status` | `string` | Response status: `"completed"`, `"in_progress"`, or `"failed"` (optional). |

The stream continues until a final chunk with `"status": "completed"` indicates the response is finished.

<details>
<summary>Final completion chunk</summary>

```
{
  "id": "f7fcea36-f128-41c4-9ac1-e683137375d5",
  "object": "response.chunk",
  "created": 1756838094,
  "model": "ced2ec91-f325-4bf0-8754-f3198c2b1563",
  "delta": {},
  "status": "completed"
}
```
</details>

## Continue conversations with response and session IDs {#response-id}

Conversation continuity allows you to maintain context across multiple API calls, enabling multi-turn conversations with your flows. This is essential for building chat applications where users can have ongoing conversations.

When you make a request, the API returns a response with an `id` field. You can use this `id` as the `previous_response_id` in your next request to continue the conversation from where it left off.

First Message:

```bash
curl -X POST \
  "http://$LANGFLOW_SERVER_URL/api/v1/responses" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "$FLOW_ID",
    "input": "Hello, my name is Alice"
  }'
```

<details>
<summary>Result</summary>

```json
{
  "id": "c45f4ac8-772b-4675-8551-c560b1afd590",
  "object": "response",
  "created_at": 1756839042,
  "status": "completed",
  "model": "ced2ec91-f325-4bf0-8754-f3198c2b1563",
  "output": [
    {
      "type": "message",
      "id": "msg_c45f4ac8-772b-4675-8551-c560b1afd590",
      "status": "completed",
      "role": "assistant",
      "content": [
        {
          "type": "output_text",
          "text": "Hello, Alice! How can I assist you today?",
          "annotations": []
        }
      ]
    }
  ],
  "previous_response_id": null
}
```

</details>

Follow-up message:

```bash
curl -X POST \
  "http://$LANGFLOW_SERVER_URL/api/v1/responses" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "ced2ec91-f325-4bf0-8754-f3198c2b1563",
    "input": "What's my name?",
    "previous_response_id": "c45f4ac8-772b-4675-8551-c560b1afd590"
  }'
```

<details>
<summary>Result</summary>

```json
{
  "id": "c45f4ac8-772b-4675-8551-c560b1afd590",
  "object": "response",
  "created_at": 1756839043,
  "status": "completed",
  "model": "ced2ec91-f325-4bf0-8754-f3198c2b1563",
  "output": [
    {
      "type": "message",
      "id": "msg_c45f4ac8-772b-4675-8551-c560b1afd590",
      "status": "completed",
      "role": "assistant",
      "content": [
        {
          "type": "output_text",
          "text": "Your name is Alice. How can I help you today?",
          "annotations": []
        }
      ]
    }
  ],
  "previous_response_id": "c45f4ac8-772b-4675-8551-c560b1afd590"
}
```

</details>

Optionally, you can use your own session ID values for the `previous_response_id`:

```bash
curl -X POST \
  "http://$LANGFLOW_SERVER_URL/api/v1/responses" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "ced2ec91-f325-4bf0-8754-f3198c2b1563",
    "input": "What's my name?",
    "previous_response_id": "session-alice-1756839048"
  }'
```

<details>
<summary>Result</summary>

This example uses the same flow as the other `previous_response_id` examples, but the LLM had not yet been introduced to Alice in the specified session:

```json
{
  "id": "session-alice-1756839048",
  "object": "response",
  "created_at": 1756839048,
  "status": "completed",
  "model": "ced2ec91-f325-4bf0-8754-f3198c2b1563",
  "output": [
    {
      "type": "message",
      "id": "msg_session-alice-1756839048",
      "status": "completed",
      "role": "assistant",
      "content": [
        {
          "type": "output_text",
          "text": "I don't have access to your name unless you tell me. If you'd like, you can share your name, and I'll remember it for this conversation!",
          "annotations": []
        }
      ]
    }
  ],
  "previous_response_id": "session-alice-1756839048"
}
```

</details>

## Retrieve tool call results {#tool-call-results}

When you send a request to the `/api/v1/responses` endpoint to run a flow that includes tools or function calls, you can retrieve the raw tool execution details by adding `"include": ["tool_call.results"]` to the request payload.

Without the `include` parameter, tool calls return basic function call information, but not the raw tool results.
For example:

```json
{
  "id": "fc_1",
  "type": "function_call",
  "status": "completed",
  "name": "evaluate_expression",
  "arguments": "{\"expression\": \"15*23\"}"
},
```

To get the raw `results` of each tool execution, add  `include: ["tool_call.results"]` to the request payload:

```bash
curl -X POST \
  "http://$LANGFLOW_SERVER_URL/api/v1/responses" \
  -H "Content-Type: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -d '{
    "model": "FLOW_ID",
    "input": "Calculate 23 * 15 and show me the result",
    "stream": false,
    "include": ["tool_call.results"]
  }'
```


The response now includes the tool call's results.
For example:

```json
{
  "id": "evaluate_expression_1",
  "type": "tool_call",
  "tool_name": "evaluate_expression",
  "queries": ["15*23"],
  "results": {"result": "345"}
}
```

<details>
<summary>Result</summary>

```json
{
  "id": "a6e5511e-71f8-457a-88d2-7d8c6ea34e36",
  "object": "response",
  "created_at": 1756835379,
  "status": "completed",
  "error": null,
  "incomplete_details": null,
  "instructions": null,
  "max_output_tokens": null,
  "model": "ced2ec91-f325-4bf0-8754-f3198c2b1563",
  "output": [
    {
      "id": "evaluate_expression_1",
      "queries": [
        "15*23"
      ],
      "status": "completed",
      "tool_name": "evaluate_expression",
      "type": "tool_call",
      "results": {
        "result": "345"
      }
    },
    {
      "type": "message",
      "id": "msg_a6e5511e-71f8-457a-88d2-7d8c6ea34e36",
      "status": "completed",
      "role": "assistant",
      "content": [
        {
          "type": "output_text",
          "text": "The result of 23 * 15 is 345.",
          "annotations": []
        }
      ]
    }
  ],
  "parallel_tool_calls": true,
  "previous_response_id": null,
  "reasoning": {
    "effort": null,
    "summary": null
  },
  "store": true,
  "temperature": 1.0,
  "text": {
    "format": {
      "type": "text"
    }
  },
  "tool_choice": "auto",
  "tools": [],
  "top_p": 1.0,
  "truncation": "disabled",
  "usage": null,
  "user": null,
  "metadata": {}
}
```
</details>

## Pass global variables to your flows in headers {#global-var}

Global variables allow you to pass dynamic values to your flows that can be used by components within that flow run.
This is useful for passing API keys, user IDs, or any other configuration that might change between requests.

The `/responses` endpoint accepts global variables as custom HTTP headers with the format `X-LANGFLOW-GLOBAL-VAR-{VARIABLE_NAME}`.
Variables are only available during this specific request execution and aren't persisted.
Variable names are automatically converted to uppercase.

This example demonstrates passing an `OPENAI_API_KEY` variable, which is a variable Langflow automatically detects from environment variables, with two custom variables for `USER_ID` and `ENVIRONMENT`. The variables don't have to be created in Langflow's Global Variables section - you can pass any variable name in the `X-LANGFLOW-GLOBAL-VAR-{VARIABLE_NAME}` header format.

```bash
curl -X POST \
  "$LANGFLOW_SERVER_URL/api/v1/responses" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -H "Content-Type: application/json" \
  -H "X-LANGFLOW-GLOBAL-VAR-OPENAI_API_KEY: sk-..." \
  -H "X-LANGFLOW-GLOBAL-VAR-USER_ID: user123" \
  -H "X-LANGFLOW-GLOBAL-VAR-ENVIRONMENT: production" \
  -d '{
    "model": "your-flow-id",
    "input": "Hello"
  }'
```

<details>
<summary>Result</summary>

```json
{
  "id": "4a4d2f24-bb45-4a55-a499-0191305264be",
  "object": "response",
  "created_at": 1756839935,
  "status": "completed",
  "model": "ced2ec91-f325-4bf0-8754-f3198c2b1563",
  "output": [
    {
      "type": "message",
      "id": "msg_4a4d2f24-bb45-4a55-a499-0191305264be",
      "status": "completed",
      "role": "assistant",
      "content": [
        {
          "type": "output_text",
          "text": "Hello! How can I assist you today?",
          "annotations": []
        }
      ]
    }
  ],
  "previous_response_id": null
}
```

</details>

Variables passed with `X-LANGFLOW-GLOBAL-VAR-{VARIABLE_NAME}` are always available to your flow, regardless of whether they exist in the database.

If your flow components reference variables that aren't provided in headers or your Langflow database, the flow fails by default.
To avoid this, you can set the `FALLBACK_TO_ENV_VARS` environment variable is `true`, which allows the flow to use values from the `.env` file if they aren't otherwise specified.

In the above example, `OPENAI_API_KEY` will fall back to the database variable if not provided in the header.
`USER_ID` and `ENVIRONMENT` will fall back to environment variables if `FALLBACK_TO_ENV_VARS` is enabled.
Otherwise, the flow fails.


================================================================================
filename: API-Reference/api-projects.mdx
================================================================================

---
title: Projects endpoints
slug: /api-projects
---

Use the `/projects` endpoint to create, read, update, and delete [Langflow projects](/concepts-flows#projects).

## Read projects

Get a list of Langflow projects, including project IDs, names, and descriptions.

```bash
curl -X GET \
  "$LANGFLOW_URL/api/v1/projects/" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

<details>
<summary>Result</summary>

```json
[
  {
    "name": "Starter Project",
    "description": "Manage your own projects. Download and upload projects.",
    "id": "1415de42-8f01-4f36-bf34-539f23e47466",
    "parent_id": null
  }
]
```

</details>

## Create project

Create a new project.

```bash
curl -X POST \
  "$LANGFLOW_URL/api/v1/projects/" \
  -H "Content-Type: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -d '{
  "name": "new_project_name",
  "description": "string",
  "components_list": [],
  "flows_list": []
}'
```

<details>
<summary>Result</summary>

```json
{
  "name": "new_project_name",
  "description": "string",
  "id": "b408ddb9-6266-4431-9be8-e04a62758331",
  "parent_id": null
}
```

</details>

To add flows and components at project creation, retrieve the `components_list` and `flows_list` values from the [`/all`](/api-reference-api-examples#get-all-components) and [/flows/read](/api-flows#read-flows) endpoints and add them to the request body.

Adding a flow to a project moves the flow from its previous location. The flow isn't copied.

```bash
curl -X POST \
  "$LANGFLOW_URL/api/v1/projects/" \
  -H "accept: application/json" \
  -H "Content-Type: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -d '{
  "name": "new_project_name",
  "description": "string",
  "components_list": [
    "3fa85f64-5717-4562-b3fc-2c963f66afa6"
  ],
  "flows_list": [
    "3fa85f64-5717-4562-b3fc-2c963f66afa6"
  ]
}'
```

## Read project

Retrieve details of a specific project.

To find the UUID of your project, call the [read projects](#read-projects) endpoint.

```bash
curl -X GET \
  "$LANGFLOW_URL/api/v1/projects/$PROJECT_ID" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

<details>
<summary>Result</summary>

```json
[
  {
    "name": "Starter Project",
    "description": "Manage your own projects. Download and upload projects.",
    "id": "3fa85f64-5717-4562-b3fc-2c963f66afa6",
    "parent_id": null
  }
]
```

</details>

## Update project

Update the information of a specific project with a `PATCH` request.

Each PATCH request updates the project with the values you send.
Only the fields you include in your request are updated.
If you send the same values multiple times, the update is still processed, even if the values are unchanged.

```bash
curl -X PATCH \
  "$LANGFLOW_URL/api/v1/projects/b408ddb9-6266-4431-9be8-e04a62758331" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -d '{
  "name": "string",
  "description": "string",
  "parent_id": "3fa85f64-5717-4562-b3fc-2c963f66afa6",
  "components": [
    "3fa85f64-5717-4562-b3fc-2c963f66afa6"
  ],
  "flows": [
    "3fa85f64-5717-4562-b3fc-2c963f66afa6"
  ]
}'
```

<details>
<summary>Result</summary>

```json
{
  "name": "string",
  "description": "string",
  "id": "b408ddb9-6266-4431-9be8-e04a62758331",
  "parent_id": null
}
```

</details>

## Delete project

Delete a specific project.

```bash
curl -X DELETE \
  "$LANGFLOW_URL/api/v1/projects/$PROJECT_ID" \
  -H "accept: */*" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

<details>
<summary>Result</summary>

```text
204 No Content
```

</details>

## Export a project

Download all flows from a project as a zip file.

The `--output` flag is optional.

```bash
curl -X GET \
  "$LANGFLOW_URL/api/v1/projects/download/$PROJECT_ID" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  --output langflow-project.zip
```

## Import a project

Import a project and its flows by uploading a Langflow project zip file:

```bash
curl -X POST \
  "$LANGFLOW_URL/api/v1/projects/upload/" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -F "file=@20241230_135006_langflow_flows.zip;type=application/zip"
```


================================================================================
filename: API-Reference/api-reference-api-examples.mdx
================================================================================

---
title: Get started with the Langflow API
slug: /api-reference-api-examples
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

You can use the Langflow API for programmatic interactions with Langflow, such as the following:

* Create and edit flows, including file management for flows.
* Develop applications that use your flows.
* Develop custom components.
* Build Langflow as a dependency of a larger application, codebase, or service.
* Contribute to the overall Langflow codebase.

To view and test all available endpoints, you can access the Langflow API's OpenAPI specification at your Langflow deployment's `/docs` endpoint, such as `http://localhost:7860/docs`.

:::tip Try it
For an example of the Langflow API in a script, see the [Langflow quickstart](/get-started-quickstart).

The quickstart demonstrates how to get automatically generated code snippets for your flows, use a script to run a flow, and extract data from the Langfow API response.
:::

## Form Langflow API requests

While individual options vary by endpoint, all Langflow API requests share some commonalities, like a URL, method, parameters, and authentication.

As an example of a Langflow API request, the following curl command calls the `/v1/run` endpoint, and it passes a runtime override (`tweaks`) to the flow's **Chat Output** component:

```bash
curl --request POST \
  --url "$LANGFLOW_SERVER_URL/api/v1/run/$FLOW_ID?stream=false" \
  --header "Content-Type: application/json" \
  --header "x-api-key: $LANGFLOW_API_KEY" \
  --data '{
  "input_value": "hello world!",
  "output_type": "chat",
  "input_type": "chat",
  "tweaks": {
    "ChatOutput-6zcZt": {
      "should_store_message": true
    }
  }
}'
```

### Base URL

By default, local deployments serve the Langflow API at `http://localhost:7860/api`.

Remotely hosted Langflow deployments are available at the domain set by the hosting service, such as `http://IP_OR_DNS/api` or `http://IP_OR_DNS:LANGFLOW_PORT/api`.

You can configure the Langflow port number in the `LANGFLOW_PORT` [environment variable](/environment-variables).

* `https://UUID.ngrok.app/api`
* `http://IP_OR_DNS/api`
* `http://IP_OR_DNS:LANGFLOW_PORT/api`

### Authentication

In Langflow versions 1.5 and later, most API endpoints require authentication with a Langflow API key in either an `x-api-key` header or query parameter.
For more information, see [API keys and authentication](/api-keys-and-authentication).

As with any API, follow industry best practices for storing and referencing sensitive credentials.
For example, you can [set environment variables](#set-environment-variables) for your API keys, and then reference those environment variables in your API requests.

### Methods, paths, and parameters

Langflow API requests use various methods, paths, path parameters, query parameters, and body parameters.
The specific requirements and options depend on the endpoint that you want to call.

For example, to create a flow, you pass a JSON-formatted flow definition to `POST /v1/flows`.
Then, to run your flow, you call `POST /v1/run/$FLOW_ID` with optional run parameters in the request body.

### API versions

The Langflow API serves `/v1` and `/v2` endpoints.

Some endpoints only exist under a single version and some exist under both the `/v1` and `/v2` versions.

If a request fails or has an unexpected result, make sure your endpoint path has the correct version.

## Set environment variables

You can store commonly used values in environment variables to facilitate reuse, simplify token rotation, and securely reference sensitive values.

You can use any method you prefer to set environment variables, such as `export`, `.env`, `zshrc`, or `.curlrc`.
Then, reference those environment variables in your API requests.
For example:

```bash
# Set environment variables
export LANGFLOW_API_KEY="sk..."
export LANGFLOW_SERVER_URL="https://localhost:7860"
export FLOW_ID="359cd752-07ea-46f2-9d3b-a4407ef618da"
export PROJECT_ID="1415de42-8f01-4f36-bf34-539f23e47466"
export LANGFLOW_API_KEY="sk-..."

# Use environment variables in API requests
curl --request POST \
  --url "$LANGFLOW_SERVER_URL/api/v1/run/$FLOW_ID$?stream=false" \
  --header "Content-Type: application/json" \
  --header "x-api-key: $LANGFLOW_API_KEY" \
  --data '{
  "input_value": "hello world!",
  "output_type": "chat",
  "input_type": "chat",
  "tweaks": {
    "ChatOutput-6zcZt": {
      "should_store_message": true
    }
  }
}'
```

Commonly used values in Langflow API requests include your [Langflow server URL](#base-url), [Langflow API keys](#authentication), flow IDs, and [project IDs](/api-projects#read-projects).

You can retrieve flow IDs from the [**API access** pane](/concepts-publish#api-access), in a flow's URL, and with [`GET /flows`](/api-flows#read-flows).

## Try some Langflow API requests

Once you have your Langflow server URL, try calling these endpoints that return Langflow metadata.

### Health check

Returns the health status of the Langflow database and chat services:

```bash
curl -X GET \
  "$LANGFLOW_SERVER_URL/health_check" \
  -H "accept: application/json"
```

<details>
<summary>Result</summary>

```json
{
  "status": "ok",
  "chat": "ok",
  "db": "ok"
}
```

</details>

Langflow provides an additional `GET /health` endpoint.
This endpoint is served by uvicorn before Langflow is fully initialized, so it's not reliable for checking Langflow service health.

### Get version

Returns the current Langflow API version:

```bash
curl -X GET \
  "$LANGFLOW_SERVER_URL/api/v1/version" \
  -H "accept: application/json"
```

<details>
<summary>Result</summary>

```text
{
    "version": "1.6.0",
    "main_version": "1.6.0",
    "package": "Langflow"
}
```

</details>

### Get configuration

Returns configuration details for your Langflow deployment:

```bash
curl -X GET \
  "$LANGFLOW_SERVER_URL/api/v1/config" \
  -H "accept: application/json"
```

<details>
<summary>Result</summary>

```json
{
  "feature_flags": {
    "mvp_components": false
  },
  "serialization_max_items_length": 1000,
  "serialization_max_text_length": 6000,
  "frontend_timeout": 0,
  "auto_saving": true,
  "auto_saving_interval": 1000,
  "health_check_max_retries": 5,
  "max_file_size_upload": 1024,
  "webhook_polling_interval": 5000,
  "public_flow_cleanup_interval": 3600,
  "public_flow_expiration": 86400,
  "event_delivery": "streaming",
  "webhook_auth_enable": false,
  "voice_mode_available": false,
  "default_folder_name": "Starter Project",
  "hide_getting_started_progress": false
}
```

</details>

### Get all components

Returns a dictionary of all Langflow components.
Requires a [Langflow API key](/api-keys-and-authentication).

```bash
curl -X GET \
  "$LANGFLOW_SERVER_URL/api/v1/all" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

## Available endpoints

Because you can run Langflow as either an IDE (frontend and backend) or a runtime (headless, backend-only), it serves endpoints that support frontend and backend operations.
Many endpoints are for orchestration between the frontend and backend, reading and writing to the Langflow database, or enabling frontend functionality, like the **Playground**.
Unless you are contributing to the Langflow codebase, you won't directly call most of the Langflow endpoints.

For application development, the most commonly used endpoints are the `/run` and `/webhook` [flow trigger endpoints](/api-flows-run).
For some use cases, you might use some other endpoints, such as the `/files` endpoints to use files in flows.

To help you explore the available endpoints, the following lists are sorted by primary use case, although some endpoints might support multiple use cases.

<Tabs>
<TabItem value="Application development" label="Application development" default>

The following endpoints are useful for developing applications with Langflow and administering Langflow deployments with one or more users.
You will most often use the flow trigger endpoints.
Other endpoints are helpful for specific use cases, such as administration and flow management in runtime deployments that don't have a visual editor.

* [Flow trigger endpoints](/api-flows-run):
  * POST `/v1/run/{flow_id_or_name}`: Run a flow.
  * POST `/v1/run/advanced/{flow_id}`: Advanced run with explicit `inputs`, `outputs`, `tweaks`, and optional `session_id`.
  * POST `/v1/webhook/{flow_id_or_name}`: Trigger a flow via webhook payload.

* [OpenAI Responses API](/api-openai-responses):
  * POST `/v1/responses`: Execute flows using an OpenAI-compatible request format.

* Deployment details:
  * GET `/v1/version`: Return Langflow version. See [Get version](/api-reference-api-examples#get-version).
  * GET `/v1/config`: Return deployment configuration. See [Get configuration](/api-reference-api-examples#get-configuration).
  * GET `/health_check`: Health check endpoint that validates database and chat service connectivity. Returns 500 status if any service is unavailable.

* [Projects endpoints](/api-projects):
  * POST `/v1/projects/`: Create a project.
  * GET `/v1/projects/`: List projects.
  * GET `/v1/projects/{project_id}`: Read a project (with paginated flows support).
  * PATCH `/v1/projects/{project_id}`: Update project info and membership.
  * DELETE `/v1/projects/{project_id}`: Delete a project.
  * GET `/v1/projects/download/{project_id}`: Export all flows in a project as ZIP.
  * POST `/v1/projects/upload/`: Import a project ZIP (creates project and flows).
  * GET `/v1/starter-projects/`: Return a list of templates.

* [Files endpoints](/api-files):
  * Files (v1)
    * POST `/v1/files/upload/{flow_id}`: Upload a file to a specific flow.
    * GET `/v1/files/download/{flow_id}/{file_name}`: Download a file from a flow.
    * GET `/v1/files/images/{flow_id}/{file_name}`: Stream an image from a flow.
    * GET `/v1/files/profile_pictures/{folder_name}/{file_name}`: Get a profile picture asset.
    * GET `/v1/files/profile_pictures/list`: List available profile picture assets.
    * GET `/v1/files/list/{flow_id}`: List files for a flow.
    * DELETE `/v1/files/delete/{flow_id}/{file_name}`: Delete a file from a flow.
  * Files (v2)
    * POST `/v2/files` (alias `/v2/files/`): Upload a file owned by the current user.
    * GET `/v2/files` (alias `/v2/files/`): List files owned by the current user.
    * DELETE `/v2/files/batch/`: Delete multiple files by IDs.
    * POST `/v2/files/batch/`: Download multiple files as a ZIP by IDs.
    * GET `/v2/files/{file_id}`: Download a file by ID (or return raw content internally).
    * PUT `/v2/files/{file_id}`: Edit a file name by ID.
    * DELETE `/v2/files/{file_id}`: Delete a file by ID.
    * DELETE `/v2/files` (alias `/v2/files/`): Delete all files for the current user.

* [API keys and authentication](/api-keys-and-authentication):
  * GET `/v1/api_key/`: List API keys for the current user.
  * POST `/v1/api_key/`: Create a new API key.
  * DELETE `/v1/api_key/{api_key_id}`: Delete an API key.
  * POST `/v1/api_key/store`: Save an encrypted Store API key (cookie set).

* [Flow management endpoints](/api-flows):
  * POST `/v1/flows/`: Create a flow.
  * GET `/v1/flows/`: List flows (supports pagination and filters).
  * GET `/v1/flows/{flow_id}`: Read a flow by ID.
  * GET `/v1/flows/public_flow/{flow_id}`: Read a public flow by ID.
  * PATCH `/v1/flows/{flow_id}`: Update a flow.
  * DELETE `/v1/flows/{flow_id}`: Delete a flow.
  * POST `/v1/flows/batch/`: Create multiple flows.
  * POST `/v1/flows/upload/`: Import flows from a JSON file.
  * DELETE `/v1/flows/`: Delete multiple flows by IDs.
  * POST `/v1/flows/download/`: Export flows to a ZIP file.
  * GET `/v1/flows/basic_examples/`: List basic example flows.

* [Users endpoints](/api-users):
  * POST `/v1/users/`: Add a user (superuser required when auth enabled).
  * GET `/v1/users/whoami`: Return the current authenticated user.
  * GET `/v1/users/`: List all users (superuser required).
  * PATCH `/v1/users/{user_id}`: Update a user (with role checks).
  * PATCH `/v1/users/{user_id}/reset-password`: Reset own password.
  * DELETE `/v1/users/{user_id}`: Delete a user (cannot delete yourself).

* Custom components: You might use these endpoints when developing custom Langflow components for your own use or to share with the Langflow community:
  * GET `/v1/all`: Return all available Langflow component types. See [Get all components](/api-reference-api-examples#get-all-components).
  * POST `/v1/custom_component`: Build a custom component from code and return its node.
  * POST `/v1/custom_component/update`: Update an existing custom component's build config and outputs.
  * POST `/v1/validate/code`: Validate a Python code snippet for a custom component.

</TabItem>
<TabItem value="Codebase contribution" label="Codebase development">

The following endpoints are most often used when contributing to the Langflow codebase, and you need to understand or call endpoints that support frontend-to-backend orchestration or other internal functionality.

* Base (metadata):
  * GET `/v1/all`: Return all available Langflow component types. See [Get all components](/api-reference-api-examples#get-all-components).
  * GET `/v1/version`: Return Langflow version. See [Get version](/api-reference-api-examples#get-version).
  * GET `/v1/config`: Return deployment configuration. See [Get configuration](/api-reference-api-examples#get-configuration).
  * GET `/v1/starter-projects/`: Return a list of templates.

* [Build endpoints](/api-build) (internal editor support):
  * POST `/v1/build/{flow_id}/flow`: Start a flow build and return a job ID.
  * GET `/v1/build/{job_id}/events`: Stream or fetch build events.
  * POST `/v1/build/{job_id}/cancel`: Cancel a build job.
  * POST `/v1/build_public_tmp/{flow_id}/flow`: Build a public flow without auth.
  * POST `/v1/validate/prompt`: Validate a prompt payload.

* [API keys and authentication](/api-keys-and-authentication):
  * POST `/v1/login`: Login and set tokens as cookies.
  * GET `/v1/auto_login`: Auto-login (if enabled) and set tokens.
  * POST `/v1/refresh`: Refresh tokens using refresh cookie.
  * POST `/v1/logout`: Logout and clear cookies.

* [Monitor endpoints](/api-monitor):
  * GET `/v1/monitor/builds`: Get vertex builds for a flow.
  * DELETE `/v1/monitor/builds`: Delete vertex builds for a flow.
  * GET `/v1/monitor/messages/sessions`: List message session IDs (auth required).
  * GET `/v1/monitor/messages`: List messages with optional filters.
  * DELETE `/v1/monitor/messages`: Delete messages by IDs (auth required).
  * PUT `/v1/monitor/messages/{message_id}`: Update a message.
  * PATCH `/v1/monitor/messages/session/{old_session_id}`: Change a session ID for all messages in that session.
  * DELETE `/v1/monitor/messages/session/{session_id}`: Delete messages by session.
  * GET `/v1/monitor/transactions`: List transactions for a flow (paginated).

* Variables:
  * POST `/v1/variables/`: Create a variable, such as an API key, for the user.
  * GET `/v1/variables/`: List variables for the user.
  * PATCH `/v1/variables/{variable_id}`: Update a variable.
  * DELETE `/v1/variables/{variable_id}`: Delete a variable.

* [Use voice mode](/concepts-voice-mode):
  * WS `/v1/voice/ws/flow_as_tool/{flow_id}`: Bi-directional voice session exposing the flow as a tool.
  * WS `/v1/voice/ws/flow_as_tool/{flow_id}/{session_id}`: Same as above with explicit session ID.
  * WS `/v1/voice/ws/flow_tts/{flow_id}`: Voice-to-text session that runs a flow and returns TTS.
  * WS `/v1/voice/ws/flow_tts/{flow_id}/{session_id}`: Same as above with explicit session ID.
  * GET `/v1/voice/elevenlabs/voice_ids`: List available ElevenLabs voice IDs for the user.

* MCP servers: The following endpoints are for managing Langflow MCP servers and MCP server connections.
They aren't typically called directly; instead, they are used to drive internal functionality in the Langflow frontend and when running flows that call MCP servers.
  * HEAD `/v1/mcp/sse`: Health check for MCP SSE.
  * GET `/v1/mcp/sse`: Open SSE stream for MCP server events.
  * POST `/v1/mcp/`: Post messages to the MCP server.
  * GET `/v1/mcp/project/{project_id}`: List MCP-enabled tools and project auth settings.
  * HEAD `/v1/mcp/project/{project_id}/sse`: Health check for project SSE.
  * GET `/v1/mcp/project/{project_id}/sse`: Open project-scoped MCP SSE.
  * POST `/v1/mcp/project/{project_id}`: Post messages to project MCP server.
  * POST `/v1/mcp/project/{project_id}/` (trailing slash): Same as above.
  * PATCH `/v1/mcp/project/{project_id}`: Update MCP settings for flows and project auth settings.
  * POST `/v1/mcp/project/{project_id}/install`: Install MCP client config for Cursor/Windsurf/Claude (local only).
  * GET `/v1/mcp/project/{project_id}/installed`: Check which clients have MCP config installed.

* Custom components: You might use these endpoints when developing custom Langflow components for your own use or to share with the Langflow community:
  * GET `/v1/all`: Return all available Langflow component types. See [Get all components](/api-reference-api-examples#get-all-components).
  * POST `/v1/custom_component`: Build a custom component from code and return its node.
  * POST `/v1/custom_component/update`: Update an existing custom component's build config and outputs.
  * POST `/v1/validate/code`: Validate a Python code snippet for a custom component.

</TabItem>
<TabItem value="Deprecated" label="Deprecated">

The following endpoints are deprecated:

* POST `/v1/predict/{flow_id}`: Use [`/v1/run/{flow_id}`](/api-flows-run) instead.
* POST `/v1/process/{flow_id}`: Use [`/v1/run/{flow_id}`](/api-flows-run) instead.
* GET `/v1/task/{task_id}`: Deprecated functionality.
* POST `/v1/upload/{flow_id}`: Use [`/files`](/api-files) instead.
* POST `/v1/build/{flow_id}/vertices`: Replaced by [`/monitor/builds`](/api-monitor).
* POST `/v1/build/{flow_id}/vertices/{vertex_id}`: Replaced by [`/monitor/builds`](/api-monitor).
* GET `/v1/build/{flow_id}/{vertex_id}/stream`: Replaced by [`/monitor/builds`](/api-monitor).
* GET `/v1/store/check/`: Return whether the Store feature is enabled.
* GET `/v1/store/check/api_key`: Check if a Store API key exists and is valid.
* POST `/v1/store/components/`: Share a component to the Store.
* PATCH `/v1/store/components/{component_id}`: Update a shared component.
* GET `/v1/store/components/`: List available Store components (filters supported).
* GET `/v1/store/components/{component_id}`: Download a component from the Store.
* GET `/v1/store/tags`: List Store tags.
* GET `/v1/store/users/likes`: List components liked by the current user.
* POST `/v1/store/users/likes/{component_id}`: Like a component.

</TabItem>
</Tabs>

## Next steps

* Use the Langflow API to [run a flow](/api-flows-run).
* Use the Langflow API to [upload files](/api-files).
* Use the Langflow API to [get flow logs](/api-logs).
* Explore all endpoints in the [Langflow API specification](/api).


================================================================================
filename: API-Reference/api-users.mdx
================================================================================

---
title: Users endpoints
slug: /api-users
---

Use the `/users` endpoint to manage user accounts in Langflow.

## Add user

Create a new user account with a given username and password.

Requires authentication as a superuser if the Langflow server has authentication enabled.

```bash
curl -X POST \
  "$LANGFLOW_URL/api/v1/users/" \
  -H "Content-Type: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -d '{
    "username": "newuser2",
    "password": "securepassword123"
  }'
```

The request returns an object describing the new user.
The user's UUID is stored in `user_id` in the Langflow database, and returned as `id` in the `/users` API response.
This `user_id` key is specifically for Langflow user management.

<details>
<summary>Result</summary>

```json
{
  "id": "10c1c6a2-ab8a-4748-8700-0e4832fd5ce8",
  "username": "newuser2",
  "profile_image": null,
  "store_api_key": null,
  "is_active": false,
  "is_superuser": false,
  "create_at": "2025-05-29T16:02:20.132436",
  "updated_at": "2025-05-29T16:02:20.132442",
  "last_login_at": null,
  "optins": {
    "github_starred": false,
    "dialog_dismissed": false,
    "discord_clicked": false
  }
}
```

</details>

## Get current user

Retrieve information about the authenticated user.

```bash
curl -X GET \
  "$LANGFLOW_URL/api/v1/users/whoami" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

<details>
<summary>Result</summary>

```json
{
  "id": "07e5b864-e367-4f52-b647-a48035ae7e5e",
  "username": "langflow",
  "profile_image": null,
  "store_api_key": null,
  "is_active": true,
  "is_superuser": true,
  "create_at": "2025-05-08T17:59:07.855965",
  "updated_at": "2025-05-29T15:06:56.157860",
  "last_login_at": "2025-05-29T15:06:56.157016",
}
```

</details>

## List all users

Get a paginated list of all users in the system.

Requires authentication as a superuser if the Langflow server has authentication enabled.

```bash
curl -X GET \
  "$LANGFLOW_URL/api/v1/users/?skip=0&limit=10" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

<details>
<summary>Result</summary>

```json
{
  "total_count": 3,
  "users": [
    {
      "id": "07e5b864-e367-4f52-b647-a48035ae7e5e",
      "username": "langflow",
      "profile_image": null,
      "store_api_key": null,
      "is_active": true,
      "is_superuser": true,
      "create_at": "2025-05-08T17:59:07.855965",
      "updated_at": "2025-05-29T15:06:56.157860",
      "last_login_at": "2025-05-29T15:06:56.157016",
      "optins": {
        "github_starred": false,
        "dialog_dismissed": true,
        "discord_clicked": false,
        "mcp_dialog_dismissed": true
      }
    },
    {
      "id": "c48a1f68-cc7e-491a-a507-a1a627708470",
      "username": "newuser",
      "profile_image": null,
      "store_api_key": null,
      "is_active": false,
      "is_superuser": false,
      "create_at": "2025-05-29T16:00:33.483386",
      "updated_at": "2025-05-29T16:00:33.483392",
      "last_login_at": null,
      "optins": {
        "github_starred": false,
        "dialog_dismissed": false,
        "discord_clicked": false
      }
    },
    {
      "id": "10c1c6a2-ab8a-4748-8700-0e4832fd5ce8",
      "username": "newuser2",
      "profile_image": null,
      "store_api_key": null,
      "is_active": false,
      "is_superuser": false,
      "create_at": "2025-05-29T16:02:20.132436",
      "updated_at": "2025-05-29T16:02:20.132442",
      "last_login_at": null,
      "optins": {
        "github_starred": false,
        "dialog_dismissed": false,
        "discord_clicked": false
      }
    }
  ]
}
```

</details>

## Update user

Modify an existing user's information with a PATCH request.

Requires authentication as a superuser if the Langflow server has authentication enabled.

This example activates the specified user's account and makes them a superuser:

```bash
curl -X PATCH \
  "$LANGFLOW_URL/api/v1/users/10c1c6a2-ab8a-4748-8700-0e4832fd5ce8" \
  -H "Content-Type: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -d '{
    "is_active": true,
    "is_superuser": true
  }'
```

<details>
<summary>Result</summary>

```json
{
  "id": "10c1c6a2-ab8a-4748-8700-0e4832fd5ce8",
  "username": "newuser2",
  "profile_image": null,
  "store_api_key": null,
  "is_active": true,
  "is_superuser": true,
  "create_at": "2025-05-29T16:02:20.132436",
  "updated_at": "2025-05-29T16:19:03.514527Z",
  "last_login_at": null,
  "optins": {
    "github_starred": false,
    "dialog_dismissed": false,
    "discord_clicked": false
  }
}
```

</details>

## Reset password

Change the specified user's password to a new secure value.

Requires authentication as the target user.

```bash
curl -X PATCH \
  "$LANGFLOW_URL/api/v1/users/10c1c6a2-ab8a-4748-8700-0e4832fd5ce8/reset-password" \
  -H "Content-Type: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -d '{
    "password": "newsecurepassword123"
  }'
```

<details>
<summary>Result</summary>

```json
{
  "id": "10c1c6a2-ab8a-4748-8700-0e4832fd5ce8",
  "username": "langflow",
  "profile_image": null,
  "store_api_key": null,
  "is_active": true,
  "is_superuser": true,
  "create_at": "2025-05-08T17:59:07.855965",
  "updated_at": "2025-05-29T15:06:56.157860",
  "last_login_at": "2025-05-29T15:06:56.157016",
  "optins": {
    "github_starred": false,
    "dialog_dismissed": true,
    "discord_clicked": false
  }
}
```

</details>

## Delete user

Remove a user account from the system.

Requires authentication as a superuser if the Langflow server has authentication enabled.

```bash
curl -X DELETE \
  "$LANGFLOW_URL/api/v1/users/10c1c6a2-ab8a-4748-8700-0e4832fd5ce8" \
  -H "accept: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY"
```

<details>
<summary>Result</summary>

```json
{
  "detail": "User deleted"
}
```

</details>


================================================================================
filename: API-Reference/typescript-client.mdx
================================================================================

---
title: Langflow TypeScript client
slug: /typescript-client
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

The Langflow TypeScript client allows your TypeScript applications to programmatically interact with the Langflow API.

For the client code repository, see [langflow-client-ts](https://github.com/datastax/langflow-client-ts/).

For the npm package, see [@datastax/langflow-client](https://www.npmjs.com/package/@datastax/langflow-client).

## Install the Langflow TypeScript package

To install the Langflow typescript client package, use one of the following commands:

<Tabs>
<TabItem value="npm" label="npm" default>

```bash
npm install @datastax/langflow-client
```

</TabItem>
<TabItem value="yarn" label="yarn">

```bash
yarn add @datastax/langflow-client
```

</TabItem>
<TabItem value="pnpm" label="pnpm">

```bash
pnpm add @datastax/langflow-client
```

</TabItem>
</Tabs>

## Initialize the Langflow TypeScript client

1. Import the client into your code.

    ```tsx
    import { LangflowClient } from "@datastax/langflow-client";
    ```

2. Initialize a `LangflowClient` object to interact with your server:

    ```tsx
    const baseUrl = "BASE_URL";
    const apiKey = "API_KEY";
    const client = new LangflowClient({ baseUrl, apiKey });
    ```

    Replace `BASE_URL` and `API_KEY` with values from your deployment.
    The default Langflow base URL is `http://localhost:7860`.
    To create an API key, see [API keys and authentication](/api-keys-and-authentication).

## Connect to your server and get responses

1. With your Langflow client initialized, test the connection by calling your Langflow server.

    The following example runs a flow (`runFlow`) by sending the flow ID and a chat input string:

    ```tsx
    import { LangflowClient } from "@datastax/langflow-client";

    const baseUrl = "http://localhost:7860";
    const client = new LangflowClient({ baseUrl });

    async function runFlow() {
        const flowId = "aa5a238b-02c0-4f03-bc5c-cc3a83335cdf";
        const flow = client.flow(flowId);
        const input = "Is anyone there?";

        const response = await flow.run(input);
        console.log(response);
    }

    runFlow().catch(console.error);
    ```

    Replace the following:

    * `baseUrl`: The URL of your Langflow server.
    * `flowId`: The ID of the flow you want to run.
    * `input`: The chat input message you want to send to trigger the flow.
    This is only valid for flows with a **Chat Input** component.

2. Review the result to confirm that the client connected to your Langflow server.

    The following example shows the response from a well-formed `runFlow` request that reached the Langflow server and successfully started the flow:

    ```
    FlowResponse {
      sessionId: 'aa5a238b-02c0-4f03-bc5c-cc3a83335cdf',
      outputs: [ { inputs: [Object], outputs: [Array] } ]
    }
    ```

    In this case, the response includes a [`sessionID`](/session-id) that is a unique identifier for the client-server session and an `outputs` array that contains information about the flow run.

3. Optional: If you want to get full response objects from the server, change `console.log` to stringify the returned JSON object:

    ```tsx
    console.log(JSON.stringify(response, null, 2));
    ```

    The exact structure of the returned `inputs` and `outputs` objects depends on the components and configuration of your flow.

4. Optional: If you want the response to include only the chat message from the **Chat Output** component, change `console.log` to use the `chatOutputText` convenience function:

    ```tsx
    console.log(response.chatOutputText());
    ```

## Use advanced TypeScript client features

The TypeScript client can do more than just connect to your server and run a flow.

This example builds on the quickstart with additional features for interacting with Langflow:

1. Pass [tweaks](/concepts-publish#input-schema) as an object with the request.
Tweaks are programmatic run-time overrides for component settings.

    This example changes the LLM used by a language model component in a flow::

    ```tsx
    const tweaks = { model_name: "gpt-4o-mini" };
    ```

2. Pass a [session ID](/session-id) with the request to separate the conversation from other flow runs, and to be able to continue this conversation by calling the same session ID in the future:

    ```tsx
    const session_id = "aa5a238b-02c0-4f03-bc5c-cc3a83335cdf";
    ```

3. Instead of calling `run` on the Flow object, call `stream` with the same arguments to get a streaming response:

    ```tsx
    const response = await client.flow(flowId).stream(input);

    for await (const event of response) {
      console.log(event);
    }
    ```

    The response is a [`ReadableStream`](https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream) of objects.
    For more information on streaming Langflow responses, see the [`/run` endpoint](/api-flows-run#run-flow).

4. Run the modified TypeScript application to run the flow with `tweaks`, `session_id`, and streaming:

    ```tsx
    import { LangflowClient } from "@datastax/langflow-client";

    const baseUrl = "http://localhost:7860";
    const client = new LangflowClient({ baseUrl });

    async function runFlow() {
        const flowId = "aa5a238b-02c0-4f03-bc5c-cc3a83335cdf";
        const input = "Is anyone there?";
        const tweaks = { model_name: "gpt-4o-mini" };
        const session_id = "test-session";

        const response = await client.flow(flowId).stream(input, {
            session_id,
            tweaks,
          });

        for await (const event of response) {
            console.log(event);
        }

    }
    runFlow().catch(console.error);
    ```

    Replace the following:

    * `baseUrl`: The URL of your Langflow server.
    * `flowId`: The ID of the flow you want to run.
    * `input`: The chat input message you want to send to trigger the flow, assuming the flow has a **Chat Input** component.
    * `tweaks`: Any tweak modifiers to apply to the flow run.
    This example changes the LLM used by a component in the flow.
    * `session_id`: Pass a custom session ID.
    If omitted or empty, the flow ID is the default session ID.

    <details>
    <summary>Result</summary>

    With streaming enabled, the response includes the flow metatadata and timestamped events for flow activity.
    For example:

    ```text
    {
      event: 'add_message',
      data: {
        timestamp: '2025-05-23 15:52:48 UTC',
        sender: 'User',
        sender_name: 'User',
        session_id: 'test-session',
        text: 'Is anyone there?',
        files: [],
        error: false,
        edit: false,
        properties: {
          text_color: '',
          background_color: '',
          edited: false,
          source: [Object],
          icon: '',
          allow_markdown: false,
          positive_feedback: null,
          state: 'complete',
          targets: []
        },
        category: 'message',
        content_blocks: [],
        id: '7f096715-3f2d-4d84-88d6-5e2f76bf3fbe',
        flow_id: 'aa5a238b-02c0-4f03-bc5c-cc3a83335cdf',
        duration: null
      }
    }
    {
      event: 'token',
      data: {
        chunk: 'Absolutely',
        id: 'c5a99314-6b23-488b-84e2-038aa3e87fb5',
        timestamp: '2025-05-23 15:52:48 UTC'
      }
    }
    {
      event: 'token',
      data: {
        chunk: ',',
        id: 'c5a99314-6b23-488b-84e2-038aa3e87fb5',
        timestamp: '2025-05-23 15:52:48 UTC'
      }
    }
    {
      event: 'token',
      data: {
        chunk: " I'm",
        id: 'c5a99314-6b23-488b-84e2-038aa3e87fb5',
        timestamp: '2025-05-23 15:52:48 UTC'
      }
    }
    {
      event: 'token',
      data: {
        chunk: ' here',
        id: 'c5a99314-6b23-488b-84e2-038aa3e87fb5',
        timestamp: '2025-05-23 15:52:48 UTC'
      }
    }

    // this response is abbreviated

    {
      event: 'end',
      data: { result: { session_id: 'test-session', outputs: [Array] } }
    }
    ```

    </details>

## Retrieve Langflow logs with the TypeScript client

To retrieve [Langflow logs](/logging), you must enable log retrieval on your Langflow server by including the following values in your Langflow `.env` file:

```text
LANGFLOW_ENABLE_LOG_RETRIEVAL=True
LANGFLOW_LOG_RETRIEVER_BUFFER_SIZE=10000
LANGFLOW_LOG_LEVEL=DEBUG
```

The following example script starts streaming logs in the background, and then runs a flow so you can monitor the flow run:

```tsx
import { LangflowClient } from "@datastax/langflow-client";

const baseUrl = "http://localhost:7863";
const flowId = "86f0bf45-0544-4e88-b0b1-8e622da7a7f0";

async function runFlow(client: LangflowClient) {
    const input = "Is anyone there?";
    const response = await client.flow(flowId).run(input);
    console.log('Flow response:', response);
}

async function main() {
    const client = new LangflowClient({ baseUrl: baseUrl });

    // Start streaming logs
    console.log('Starting log stream...');
    for await (const log of await client.logs.stream()) {
        console.log('Log:', log);
    }

    // Run the flow
    await runFlow(client);

}

main().catch(console.error);
```

Replace the following:

* `baseUrl`: The URL of your Langflow server.
* `flowId`: The ID of the flow you want to run.
* `input`: The chat input message you want to send to trigger the flow, assuming the flow has a **Chat Input** component.

Logs begin streaming indefinitely, and the flow runs once.

<details>
<summary>Result</summary>

The following example result is truncated for readability, but you can follow the messages to see how the flow instantiates its components, configures its model, and processes the outputs.

The `FlowResponse` object, at the end of the stream, is returned to the client with the flow result in the `outputs` array.

```text
Starting log stream...
Log: Log {
  timestamp: 2025-05-30T11:49:16.006Z,
  message: '2025-05-30T07:49:16.006127-0400 DEBUG Instantiating ChatInput of type component\n'
}
Log: Log {
  timestamp: 2025-05-30T11:49:16.029Z,
  message: '2025-05-30T07:49:16.029957-0400 DEBUG Instantiating Prompt of type component\n'
}
Log: Log {
  timestamp: 2025-05-30T11:49:16.049Z,
  message: '2025-05-30T07:49:16.049520-0400 DEBUG Instantiating ChatOutput of type component\n'
}
Log: Log {
  timestamp: 2025-05-30T11:49:16.069Z,
  message: '2025-05-30T07:49:16.069359-0400 DEBUG Instantiating OpenAIModel of type component\n'
}
Log: Log {
  timestamp: 2025-05-30T11:49:16.086Z,
  message: "2025-05-30T07:49:16.086426-0400 DEBUG Running layer 0 with 2 tasks, ['ChatInput-xjucM', 'Prompt-I3pxU']\n"
}
Log: Log {
  timestamp: 2025-05-30T11:49:16.101Z,
  message: '2025-05-30T07:49:16.101766-0400 DEBUG Building Chat Input\n'
}
Log: Log {
  timestamp: 2025-05-30T11:49:16.113Z,
  message: '2025-05-30T07:49:16.113343-0400 DEBUG Building Prompt\n'
}
Log: Log {
  timestamp: 2025-05-30T11:49:16.131Z,
  message: '2025-05-30T07:49:16.131423-0400 DEBUG Logged vertex build: 6bd9fe9c-5eea-4f05-a96d-f6de9dc77e3c\n'
}
Log: Log {
  timestamp: 2025-05-30T11:49:16.143Z,
  message: '2025-05-30T07:49:16.143295-0400 DEBUG Logged vertex build: 39c68ec9-3859-4fff-9b14-80b3271f8fbf\n'
}
Log: Log {
  timestamp: 2025-05-30T11:49:16.188Z,
  message: "2025-05-30T07:49:16.188730-0400 DEBUG Running layer 1 with 1 tasks, ['OpenAIModel-RtlZm']\n"
}
Log: Log {
  timestamp: 2025-05-30T11:49:16.201Z,
  message: '2025-05-30T07:49:16.201946-0400 DEBUG Building OpenAI\n'
}
Log: Log {
  timestamp: 2025-05-30T11:49:16.216Z,
  message: '2025-05-30T07:49:16.216622-0400 INFO Model name: gpt-4.1-mini\n'
}
Flow response: FlowResponse {
  sessionId: '86f0bf45-0544-4e88-b0b1-8e622da7a7f0',
  outputs: [ { inputs: [Object], outputs: [Array] } ]
}
Log: Log {
  timestamp: 2025-05-30T11:49:18.094Z,
  message: `2025-05-30T07:49:18.094364-0400 DEBUG Vertex OpenAIModel-RtlZm, result: <langflow.graph.utils.UnbuiltResult object at 0x364d24dd0>, object: {'text_output': "Hey there! I'm here and ready to help you build something awesome with AI. What are you thinking about creating today?"}\n`
}
```

</details>

For more information, see [Logs endpoints](/api-logs).


================================================================================
filename: Agents/agents-tools.mdx
================================================================================

---
title: Configure tools for agents
slug: /agents-tools
---

import Icon from "@site/src/components/icon";

By default, [Langflow agents](/agents) only include the functionality built-in to their base LLM.

You can attach tools to agents to provide access to additional, targeted functionality.
For example, tools can be used to create domain-specific agents, such as a customer support agent that can access a company's knowledge base, a financial agent that can retrieve stock prices, or a math tutor agent that can use advanced math functions to solve complex equations.

## Attach tools

To attach a tool to an agent, you connect any component's **Tool** output to the **Agent** component's **Tools** input.

Some components emit **Tool** output by default.
For all other components, you must enable **Tool Mode** in the [component's header menu](/concepts-components#component-menus).
Then, you can connect the tool to the agent.

You can connect multiple tools to one agent, and each tool can have multiple actions (functions) that the agent can call.

When you run your flow, the agent decides when to call on certain tools, if it determines that a tool can help it respond to the user's prompt.

### Edit a tool's actions {#edit-a-tools-actions}

When you attach components to an agent as tools, each tool can have multiple actions (functions) that the agent can call.
Available actions are listed in each tool component's **Actions** list.

You can change each action's labels, descriptions, and availability to help the agent understand how to use the tool and prevent it from using irrelevant or undesired actions.

:::tip
If an agent seems to be using a tool incorrectly, try editing the actions metadata to clarify the tool's purpose and disable unnecessary actions.

You can also try using a **Prompt Template** component to pass additional instructions or examples to the agent.
:::

To view and edit a tool's actions, click <Icon name="Settings2" aria-hidden="true"/> **Edit Tool Actions** on the tool component.

The following information is provided for each action:

* **Enabled**: A checkbox that determines whether the action is available to the agent.
If selected, the action is enabled.
If not selected, the action is disabled.

* **Name**: A human-readable string name for the action, such as `Fetch Content`. This cannot be changed.

* **Description**: A human-readable description of the action's purpose, such as `Fetch content from web pages recursively`.
To edit this value, double-click the action's row to open the edit pane.
Changes are saved automatically when you click out of the field or close the dialog.

* **Slug**: An encoded name for the action, usually the same as the name but in snake case, such as `fetch_content`.
To edit this value, double-click the action's row to open the edit pane.
Changes are saved automatically when you click out of the field or close the dialog.

Some actions allow you to provide fixed values for their inputs.
Typically, you want to leave these blank so the agent can provide its own values.
However, you might use a fixed value if you're trying to debug an agent's behavior or your use case requires a fixed input for an action.

## Use an agent as a tool

To create multi-agent flows, you can set another **Agent** component to **Tool Mode**, and then attach that agent as a tool for your primary **Agent** component.

To try this for yourself, add an additional agent to the **Simple Agent** template:

1. Create a flow based on the **Simple Agent** template.
2. Add a second **Agent** component to the flow.
3. Add your **OpenAI API Key** to both **Agent** components.
4. In the second **Agent** component, change the model to `gpt-4.1`, and then enable **Tool Mode**.
5. Click <Icon name="Settings2" aria-hidden="true"/> **Edit Tool Actions** to [edit the tool's actions](#edit-a-tools-actions).

    For this example, change the action's slug to `Agent-gpt-41`, and set the description to `Use the gpt-4.1 model for complex problem solving`.
    This lets the primary agent know that this tool uses the `gpt-4.1` model, which could be helpful for tasks requiring a larger context window, such as large scrape and search tasks.

    As another example, you could attach several specialized models to a primary agent, such as agents that are trained on certain tasks or domains, and then the primary agent would call each specialized agent as needed to respond to queries.

    You can also enable and disable tools if you want to limit the available toolset.

6. Connect the new agent's **Toolset** port to the existing agent's **Tools** port.

    ![Agent as a tool](/img/agent-example-agent-as-tool.png)

## Add custom components as tools {#components-as-tools}

An agent can use [custom components](/components-custom-components) as tools.

1. To add a custom component to an agent flow, click **New Custom Component** in the <Icon name="Component" aria-hidden="true" /> **Core components** or <Icon name="Blocks" aria-hidden="true" /> **Bundles** menus.

2. Enter Python code into the **Code** pane to create the custom component.

    If you don't already have code for a custom component, you can use the following code snippet as an example before creating your own.

    <details>
    <summary>Text Analyzer custom component</summary>

    This code creates a text analyzer component.

    ```python
    from langflow.custom import Component
    from langflow.io import MessageTextInput, Output
    from langflow.schema import Data
    import re

    class TextAnalyzerComponent(Component):
        display_name = "Text Analyzer"
        description = "Analyzes and transforms input text."
        documentation: str = "http://docs.langflow.org/components/custom"
        icon = "chart-bar"
        name = "TextAnalyzerComponent"

        inputs = [
            MessageTextInput(
                name="input_text",
                display_name="Input Text",
                info="Enter text to analyze",
                value="Hello, World!",
                tool_mode=True,
            ),
        ]

        outputs = [
            Output(display_name="Analysis Result", name="output", method="analyze_text"),
        ]

        def analyze_text(self) -> Data:
            text = self.input_text

            # Perform text analysis
            word_count = len(text.split())
            char_count = len(text)
            sentence_count = len(re.findall(r'\w+[.!?]', text))

            # Transform text
            reversed_text = text[::-1]
            uppercase_text = text.upper()

            analysis_result = {
                "original_text": text,
                "word_count": word_count,
                "character_count": char_count,
                "sentence_count": sentence_count,
                "reversed_text": reversed_text,
                "uppercase_text": uppercase_text
            }

            data = Data(value=analysis_result)
            self.status = data
            return data
    ```
    </details>

3. Enable **Tool Mode** in the custom component.
4. Connect the custom component's tool output to the **Agent** component's **Tools** input.
5. Open the <Icon name="Play" aria-hidden="true" /> **Playground** and instruct the agent, `Use the text analyzer on this text: "Agents really are thinking machines!"`

    Based on your instruction, the agent should call the `analyze_text` action and return the result.
    For example:

    ```
    gpt-4o
    Finished
    0.6s
    Here is the analysis of the text "Agents really are thinking machines!":
    Original Text: Agents really are thinking machines!
    Word Count: 5
    Character Count: 36
    Sentence Count: 1
    Reversed Text: !senihcam gnikniht era yllaer stnegA
    Uppercase Text: AGENTS REALLY ARE THINKING MACHINES!
    ```

## Make any component a tool

If the component you want to use as a tool doesn't have a **Tool Mode** button, add `tool_mode=True` to one of the component's inputs, and connect the new **Toolset** output to the agent's **Tools** input.

Langflow supports **Tool Mode** for the following data types:

* `DataInput`
* `DataFrameInput`
* `PromptInput`
* `MessageTextInput`
* `MultilineInput`
* `DropdownInput`

For example, the example code in [Use custom components as tools](#components-as-tools) included `tool_mode=True` to the `MessageTextInput` input so the custom component could be used as a tool:

```python
inputs = [
    MessageTextInput(
        name="input_text",
        display_name="Input Text",
        info="Enter text to analyze",
        value="Hello, World!",
        tool_mode=True,
    ),
]
```

## Use flows as tools

An agent can use your other flows as tools with the [**Run Flow** component](/components-logic#run-flow).

1. Add a **Run Flow** component to your flow.
2. Select the flow you want the agent to use as a tool.
3. Enable **Tool Mode**.
The selected flow becomes an [action](#edit-a-tools-actions) in the **Run Flow** component.
4. Connect the **Run Flow** component's **Tool** output to the **Agent** component's **Tools** input.
5. Open the **Playground**, and then ask the agent, `What tools are you using to answer my questions?`
Your flow should be visible in the response as an available tool.
6. Ask the agent a question that specifically uses the connected flow as a tool.
The connected flow returns an answer based on your question.

![Run Flow component connected to an Agent component as a tool](/img/agent-example-run-flow-as-tool.png)

## See also

* [Agent components](/components-agents)
* [Use Langflow as an MCP client](/mcp-client)
* [Use Langflow as an MCP server](/mcp-server)


================================================================================
filename: Agents/agents.mdx
================================================================================

---
title: Use Langflow agents
slug: /agents
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';
import PartialAgentsWork from '@site/docs/_partial-agents-work.mdx';

Langflow's [**Agent** component](/components-agents) is critical for building agent flows.
This component provides everything you need to create an agent, including multiple Large Language Model (LLM) providers, tool calling, and custom instructions.
It simplifies agent configuration so you can focus on application development.

<PartialAgentsWork />

## Use the Agent component in a flow

The following steps explain how to create an agent flow in Langflow from a blank flow.
For a prebuilt example, use the **Simple Agent** template or the [Langflow quickstart](/get-started-quickstart).

1. Click **New Flow**, and then click **Blank Flow**.

2. Add an **Agent** component to your flow.

3. Select the provider and model that you want to use.
The default model for the **Agent** component is an OpenAI model.
If you want to use a different provider, edit the **Model Provider** and **Model Name** fields accordingly.
If your preferred model isn't listed, type the complete model name into the **Model Name** field, and then select it from the **Model Name** menu.
Make sure that the model is enabled/verified in your model provider account.
For more information, see [Agent component parameters](#agent-component-parameters).

4. Enter a valid credential for your selected model provider.
Make sure that the credential has permission to call the selected model.

5. Add [**Chat Input** and **Chat Output** components](/components-io) to your flow, and then connect them to the **Agent** component.

    At this point, you have created a basic LLM-based chat flow that you can test in the <Icon name="Play" aria-hidden="true" /> **Playground**.
    However, this flow only chats with the LLM.
    To enhance this flow and make it truly agentic, add some tools, as explained in the next steps.

    ![A basic agent chat flow with Chat Input, Agent, and Chat Output components.](/img/agent-example-add-chat.png)

6. Add **News Search**, **URL**, and **Calculator** components to your flow.
7. Enable **Tool Mode** in the **News Search**, **URL**, and **Calculator** components:

    1. Click the **News Search** component to expose the [component's header menu](/concepts-components#component-menus), and then enable **Tool Mode**.
    2. Repeat for the **URL** and **Calculator** components.
    3. Connect the **Toolset** port for each tool component to the **Tools** port on the **Agent** component.

    **Tool Mode** makes a component into a tool by modifying the component's inputs.
    With **Tool Mode** enabled, a component can accept requests from an **Agent** component to use the component's available actions as tools.

    When in **Tool Mode**, a component has a **Toolset** port that you must connect to an **Agent** component's **Tools** port if you want to allow the agent to use that component's actions as tools.

    For more information, see [Configure tools for agents](/agents-tools).

    ![A more complex agent chat flow where three components are connected to the Agent component as tools](/img/agent-example-add-tools.png)

8. Open the <Icon name="Play" aria-hidden="true" /> **Playground**, and then ask the agent, `What tools are you using to answer my questions?`

    The agent should respond with a list of the connected tools.
    It may also include built-in tools.

    ```text
    I use a combination of my built-in knowledge (up to June 2024) and a set of external tools to answer your questions. Here are the main types of tools I can use:
    Web Search & Content Fetching: I can fetch and summarize content from web pages, including crawling links recursively.
    News Search: I can search for recent news articles using Google News via RSS feeds.
    Calculator: I can perform arithmetic calculations and evaluate mathematical expressions.
    Date & Time: I can provide the current date and time in various time zones.
    These tools help me provide up-to-date information, perform calculations, and retrieve specific data from the internet when needed. If you have a specific question, let me know, and I'll use the most appropriate tool(s) to help!
    ```

9. To test a specific tool, ask the agent a question that uses one of the tools, such as `Summarize today's tech news`.

    To help you debug and test your flows, the **Playground** displays the agent's tool calls, the provided input, and the raw output the agent received before generating the summary.
    With the given example, the agent should call the **News Search** component's `search_news` action.

You've successfully created a basic agent flow that uses some generic tools.

To continue building on this tutorial, try connecting other tool components or [use Langflow as an MCP client](/mcp-client) to support more complex and specialized tasks.

For a multi-agent example, see [Use an agent as a tool](/agents-tools#use-an-agent-as-a-tool).

## Agent component parameters

You can configure the **Agent** component to use your preferred provider and model, custom instructions, and tools.

<PartialParams />

### Provider and model

Use the **Model Provider** (`agent_llm`) and **Model Name** (`llm_model`) settings to select the model provider and LLM that you want the agent to use.

The **Agent** component includes many models from several popular model providers.
To access other providers or models, you can do either of the following:

* Set **Model Provider** to **Connect other models**, and then connect any [language model component](/components-models).
* Select your preferred provider, type the complete model name into the **Model Name** field, and then select your custom option from the **Model Name** menu.
Make sure that the model is enabled/verified in your model provider account.

If you need to generate embeddings in your flow, use an [embedding model component](/components-embedding-models).

### Model provider API key

In the **API Key** field, enter a valid authentication key for your selected model provider, if you are using a built-in provider.
For example, to use the default OpenAI model, you must provide a valid OpenAI API key for an OpenAI account that has credits and permission to call OpenAI LLMs.

You can enter the key directly, but it is recommended that you follow industry best practices for storing and referencing API keys.
For example, you can use a <Icon name="Globe" aria-hidden="true"/> [global variable](/configuration-global-variables) or [environment variables](/environment-variables).
For more information, see [Add component API keys to Langflow](/api-keys-and-authentication#component-api-keys).

If you select **Connect other models** as the model provider, authentication is handled in the incoming language model component.

### Agent instructions and input

In the **Agent Instructions** (`system_prompt`) field, you can provide custom instructions that you want the **Agent** component to use for every conversation.

These instructions are applied in addition to the **Input** (`input_value`), which can be entered directly or provided through another component, such as a **Chat Input** component.

### Tools

Agents are most useful when they have the appropriate tools available to complete requests.

An **Agent** component can use any Langflow component as a tool, including other agents and MCP servers.

To attach a component as a tool, you must enable **Tool Mode** on the component that you want to attach, and then attach it to the **Agent** component's **Tools** port.
For more information, see [Configure tools for agents](/agents-tools).

:::tip
To allow agents to use tools from MCP servers, use the [**MCP Tools** component](/components-agents#mcp-connection).
:::

### Agent memory

Langflow agents have built-in chat memory that is enabled by default.
This memory allows them to retrieve and reference messages from previous conversations, maintaining a rolling context window for each chat session ID.

Chat memories are grouped by [session ID (`session_id`)](/session-id).
It is recommended to use custom session IDs if you need to segregate chat memory for different users or applications that run the same flow.

By default, the **Agent** component uses your Langflow installation's storage, and it retrieves a limited number of chat messages, which you can configure with the **Number of Chat History Messages** parameter.

The **Message History** component isn't required for default chat memory, but it is required if you want to use external chat memory like Mem0.
Additionally, the **Message History** component provides more options for sorting, filtering, and limiting memories. Although, most of these options are built-in to the **Agent** component with default values.

For more information, see [Store chat memory](/memory#store-chat-memory) and [**Message History** component](/components-helpers#message-history).

### Additional parameters

With the **Agent** component, the available parameters can change depending on the selected provider and model, including support for additional modes, arguments, or features like chat memory and temperature.
For example:

* **Current Date** (`add_current_date_tool`): When enabled (`true`), this setting adds a tool to the agent that can retrieve the current date.
* **Handle Parse Errors** (`handle_parsing_errors`): When enabled (`true`), this setting allows the agent to fix errors, like typos, when analyzing user input.
* **Verbose** (`verbose`): When enabled (`true`), this setting records detailed logging output for debugging and analysis.

<PartialParams />

## Agent component output

The **Agent** component outputs a **Response** (`response`) that is [`Message` data](/data-types#message) containing the agent's raw response to the query.

Typically, this is passed to a **Chat Output** component to return the response in a human-readable format.
It can also be passed to other components if you need to process the response further before, or in addition to, returning it to the user.

## See also

* [**Agent** and **MCP Tools** components](/components-agents)
* [Configure tools for agents](/agents-tools)


================================================================================
filename: Agents/mcp-client.mdx
================================================================================

---
title: Use Langflow as an MCP client
slug: /mcp-client
---

import Icon from "@site/src/components/icon";
import McpIcon from '@site/static/logos/mcp-icon.svg';
import PartialMcpNodeTip from '@site/docs/_partial-mcp-node-tip.mdx';

Langflow integrates with the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) as both an MCP server and an MCP client.

This page describes how to use Langflow as an MCP client with the [**MCP Tools** component](#use-the-mcp-tools-component) and connected [MCP servers](#manage-connected-mcp-servers).

For information about using Langflow as an MCP server, see [Use Langflow as an MCP server](/mcp-server).

## Use the MCP tools component

The **MCP Tools** component connects to an MCP server so that a [Langflow agent](/agents) can use the server's tools when responding to user queries.

This component has two modes, depending on the type of server you want to access:

* [Connect to a non-Langflow MCP server](#mcp-stdio-mode) with a JSON configuration file, server start command, or SSE URL to access tools provided by external, non-Langflow MCP servers.
* [Connect to a Langflow MCP server](#mcp-sse-mode) to use flows from your [Langflow projects](/concepts-flows#projects) as MCP tools.

### Connect to a non-Langflow MCP server {#mcp-stdio-mode}

1. Add an **MCP Tools** component to your flow.

2. In the **MCP Server** field, select a previously connected server or click <Icon name="Plus" aria-hidden="true"/> **Add MCP Server**.

    There are multiple ways to add a new server:

    * **JSON**: Paste the MCP server's JSON configuration object into the field, including required and optional parameters that you want to use, and then click **Add Server**.
    * **STDIO**: Enter the MCP server's **Name**, **Command**, and any **Arguments** and **Environment Variables** the server uses, and then click **Add Server**.
    For example, to start a [Fetch](https://github.com/modelcontextprotocol/servers/tree/main/src/fetch) server, the **Command** is `uvx mcp-server-fetch`.
    * **SSE**: Enter your Langflow MCP server's **Name**, **SSE URL**, and any **Headers** and **Environment Variables** the server uses, and then click **Add Server**.
    The default **SSE URL** is `http://localhost:7860/api/v1/mcp/sse`. For more information, see [Use SSE mode](#mcp-sse-mode).

    <PartialMcpNodeTip />


3. To use environment variables in your server command, enter each variable in the **Env** fields as key-value pairs.

    :::tip
    Langflow passes environment variables from the `.env` file to MCP, but it doesn't pass global variables declared in your Langflow **Settings**.
    To define an MCP server environment variable as a global variable, add it to Langflow's `.env` file at startup.
    For more information, see [global variables](/configuration-global-variables).
    :::

4. In the **Tool** field, select a tool that you want this component to use, or leave the field blank to allow access to all tools provided by the MCP server.

    If you select a specific tool, you might need to configure additional tool-specific fields. For information about tool-specific fields, see your MCP server's documentation.

    At this point, the **MCP Tools** component is serving a tool, but nothing is using the tool. The next steps explain how to make the tool available to an [**Agent** component](/components-agents) so that the agent can use the tool in its responses.

5. In the [component's header menu](/concepts-components#component-menus), enable **Tool mode** so you can use the component with an agent.

6. Connect the **MCP Tools** component's **Toolset** port to an **Agent** component's **Tools** port.

    If not already present in your flow, make sure you also attach **Chat Input** and **Chat Output** components to the **Agent** component.

    ![MCP Tools component in STDIO mode](/img/component-mcp-stdio.png)

7.  Test your flow to make sure the MCP server is connected and the selected tool is used by the agent. Open the **Playground**, and then enter a prompt that uses the tool you connected through the **MCP Tools** component.

    For example, if you use `mcp-server-fetch` with the `fetch` tool, you could ask the agent to summarize recent tech news. The agent calls the MCP server function `fetch`, and then returns the response.

8. If you want the agent to be able to use more tools, repeat these steps to add more tools components with different servers or tools.

### Connect a Langflow MCP server {#mcp-sse-mode}

Every Langflow project runs a separate MCP server that exposes the project's flows as MCP tools.
For more information about your projects' MCP servers, including exposing flows as MCP tools, see [Use Langflow as an MCP server](/mcp-server).

To leverage flows-as-tools, use the **MCP Tools** component in **Server-Sent Events (SSE)** mode to connect to a project's `/api/v1/mcp/sse` endpoint:

1. Add an **MCP Tools** component to your flow, click <Icon name="Plus" aria-hidden="true"/> **Add MCP Server**, and then select **SSE** mode.
2. In the **MCP SSE URL** field, modify the default address to point at your Langflow server's SSE endpoint. The default value for other Langflow installations is `http://localhost:7860/api/v1/mcp/sse`.
In SSE mode, all flows available from the targeted server are treated as tools.
3. In the [component's header menu](/concepts-components#component-menus), enable **Tool Mode** so you can use the component with an agent.
4. Connect the **MCP Tools** component's **Toolset** port to an **Agent** component's **Tools** port.
5. If not already present in your flow, make sure you also attach **Chat Input** and **Chat Output** components to the **Agent** component.

    ![MCP component with SSE mode enabled](/img/component-mcp-sse-mode.png)

6. Test your flow to make sure the agent uses your flows to respond to queries. Open the **Playground**, and then enter a prompt that uses a flow that you connected through the **MCP Tools** component.

7. If you want the agent to be able to use more tools, repeat these steps to add more tools components with different servers or tools.

## MCP Tools parameters

| Name | Type | Description |
|------|------|-------------|
| command | String | Input parameter. Stdio mode only. The MCP server startup command. Default: `uvx mcp-sse-shim@latest`. |
| sse_url | String | Input parameter. SSE mode only. The SSE URL for a Langflow project's MCP server. Default for Langflow Desktop: `http://localhost:7868/`. Default for other installations: `http://localhost:7860/api/v1/mcp/sse` |
| tools | List[Tool] | Output parameter. [`Tool`](/data-types#tool) object containing a list of tools exposed by the MCP server. |

## Manage connected MCP servers

To manage all MCP server connections for your Langflow client, click <McpIcon /> **MCP servers** in the visual editor, or click your profile icon, select **Settings**, and then click **MCP Servers**.

To add a new MCP server, click **Add MCP Server**, and then follow the steps in [Use the MCP Tools component](#use-the-mcp-tools-component) to configure the connection and use the server in a flow.

Click <Icon name="Ellipsis" aria-hidden="true"/> **More** to edit or delete an MCP server connection.

## See also

- [Use Langflow as an MCP server](/mcp-server)
- [Use a DataStax Astra DB MCP server with the MCP Tools component](/mcp-component-astra)


================================================================================
filename: Agents/mcp-component-astra.mdx
================================================================================

---
title: Connect an Astra DB MCP server to Langflow
slug: /mcp-component-astra
---

import Icon from "@site/src/components/icon";
import PartialMcpNodeTip from '@site/docs/_partial-mcp-node-tip.mdx';

This guide demonstrates how to [use Langflow as an MCP client](/mcp-client) by using the **MCP Tools** component to run a [DataStax Astra DB MCP server](https://github.com/datastax/astra-db-mcp) in an agent flow.

1. Install an LTS release of [Node.js](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm).

<PartialMcpNodeTip />

2. Create an [OpenAI](https://platform.openai.com/) API key.

3. Create an [Astra DB Serverless (Vector) database](https://docs.datastax.com/en/astra-db-serverless/databases/create-database.html#create-vector-database), if you don't already have one.

4. Get your database's Astra DB API endpoint and an Astra application token with the **Database Administrator** role. For more information, see [Generate an application token for a database](https://docs.datastax.com/en/astra-db-serverless/administration/manage-application-tokens.html#database-token).

5. To follow along with this guide, create a flow based on the **Simple Agent** template.

    You can also use an existing flow or create a blank flow.

6. Remove the **URL** tool, and then replace it with an [**MCP Tools** component](/mcp-client).

7. Configure the **MCP Tools** component as follows:

    1. Select **Stdio** mode.
    2. In the **MCP server** field, add the following code to connect to an Astra DB MCP server:

        ```bash
        npx -y @datastax/astra-db-mcp
        ```

    3. In the **Env** fields, add variables for `ASTRA_DB_APPLICATION_TOKEN` and `ASTRA_DB_API_ENDPOINT` with the values from your Astra database.

        :::info
        Environment variables declared in your Langflow `.env` file can be referenced in your MCP server commands, but you cannot reference global variables declared in Langflow.

        If you want to use variables for `ASTRA_DB_APPLICATION_TOKEN` and `ASTRA_DB_API_ENDPOINT`, add them to Langflow's `.env` file, and then restart Langflow.
        For more information, see [global variables](/configuration-global-variables).
        :::

        Add each variable separately.
        To add another variable field click <Icon name="Plus" aria-hidden="true"/> **Add More**.

        ```bash
        ASTRA_DB_APPLICATION_TOKEN=AstraCS:...
        ```

        ```bash
        ASTRA_DB_API_ENDPOINT=https://...-us-east-2.apps.astra.datastax.com
        ```

8. In the **Agent** component, add your OpenAI API key.

    The default model is an OpenAI model.
    If you want to use a different model, edit the **Model Provider**, **Model Name**, and **API Key** fields accordingly.

    ![The Simple Agent flow with the URL tool replaced by an MCP Tools component, and the MCP Tools component launching an Astra DB MCP server](/img/component-mcp-astra-db.png)

9. Open the **Playground**, and then ask the agent, `What collections are available?`

    Since Langflow is connected to your Astra DB database through the MCP server, the agent chooses the correct tool and connects to your database to retrieve the answer.
    For example:

    ```text
    The available collections in your database are:
    collection_002
    hardware_requirements
    load_collection
    nvidia_collection
    software_requirements
    ```


================================================================================
filename: Agents/mcp-server.mdx
================================================================================

---
title: Use Langflow as an MCP server
slug: /mcp-server
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import Icon from "@site/src/components/icon";

Langflow integrates with the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) as both an MCP server and an MCP client.

This page describes how to use Langflow as an MCP server that exposes your flows as [tools](https://modelcontextprotocol.io/docs/concepts/tools) that [MCP clients](https://modelcontextprotocol.io/clients) can use when generating responses.

For information about using Langflow as an MCP client and managing MCP server connections within flows, see [Use Langflow as an MCP client](/mcp-client).

## Prerequisites

* A [Langflow project](/concepts-flows#projects) with at least one flow that has a [**Chat Output** component](/components-io#chat-output).

    The **Chat Output** component is required to use a flow as an MCP tool.

* Any LTS version of [Node.js](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) installed on your computer if you want to use MCP Inspector to [test and debug flows](#test-and-debug-flows).

* [ngrok installed](https://ngrok.com/docs/getting-started/#1-install-ngrok) and an [ngrok authtoken](https://dashboard.ngrok.com/get-started/your-authtoken) if you want to [deploy a public Langflow server](/deployment-public-server).

## Serve flows as MCP tools {#select-flows-to-serve}

Each [Langflow project](/concepts-flows#projects) has an MCP server that exposes the project's flows as tools for use by MCP clients.

By default, all flows in a project are exposed as tools on the project's MCP server.
You can change the exposed flows and tool metadata by managing the MCP server settings:

1. Click the **MCP Server** tab on the [**Projects** page](/concepts-flows#projects), or, when editing a flow, click **Share**, and then select **MCP Server**.

    ![MCP server projects page](/img/mcp-server.png)

    The **Flows/Tools** section lists the flows that are currently being served as tools on this MCP server.

2. To toggle exposed flows, click <Icon name="Settings2" aria-hidden="true"/> **Edit Tools**, and then select the flows that you want exposed as tools.
To prevent a flow from being used as a tool, clear the checkbox in the first column.

3. Close the **MCP Server Tools** dialog to save your changes.

    ![MCP Server Tools](/img/mcp-server-tools.png)

### Edit flow tool names and descriptions

Tool names and descriptions help MCP clients determine which actions your flows provide and when to use those actions.
It is recommended to provide clear, descriptive names and descriptions for all tools that you serve to MCP clients.

To edit the names and descriptions of flow tools on a Langflow MCP server, do the following:

1. Click the **MCP Server** tab on the [**Projects** page](/concepts-flows#projects), or, when editing a flow, click **Share**, and then select **MCP Server**.

2. Click <Icon name="Settings2" aria-hidden="true"/> **Edit Tools**.

3. Click the **Description** or **Tool** that you want to edit:

    - **Tool name**: Enter a name that makes it clear what the flow does when used as a tool by an agent.

    - **Tool description**: Enter a description that completely and accurately describes the specific actions the flow performs.

4. Close the **MCP Server Tools** dialog to save your changes.

#### Importance of tool names and descriptions

MCP clients use tool names and descriptions to determine which actions to use when generating responses.

Because MCP clients treat your Langflow project as a single MCP server with all of your enabled flows listed as tools, unclear names and descriptions can cause the agent to select tools incorrectly or inconsistently.

For example, a flow's default tool name is the flow ID, such as `adbbf8c7-0a34-493b-90ea-5e8b42f78b66`.
This provides no information to an agent about the type of flow or its purpose.

To provide more context about your flows, make sure to name and describe your flows clearly when configuring your Langflow project's MCP server.

Think of these names and descriptions as function names and code comments.
Use clear statements to describe the problems your flows solve.

<details>
<summary>Example: Tool name and description usage</summary>

For example, assume you create a flow based on the **Document Q&A** template that uses an LLM to chat about resumes, and then you give the flow the following name and description:

- **Tool name**: `document_qa_for_resume`

- **Tool description**: `A flow for analyzing Emily's resume.`

After connecting your Langflow MCP server to Cursor, you can ask Cursor about the resume, such as `What job experience does Emily have?`.
Using the context provided by your tool name and description, the agent can decide to use the `document_qa_for_resume` MCP tool to create a response about Emily's resume.
If necessary, the agent asks permission to use the flow tool before generating the response.

If you ask about a different resume, such as `What job experience does Alex have?`, the agent can decide that `document_qa_for_resume` isn't relevant to this request, because the tool description specifies that the flow is for Emily's resume.
In this case, the agent might use another available tool, or it can inform you that it doesn't have access to information about Alex's.
For example:

```
I notice you're asking about Alex's job experience.
Based on the available tools, I can see there is a Document QA for Resume flow that's designed for analyzing resumes.
However, the description mentions it's for "Emily's resume" not Alex's. I don't have access to Alex's resume or job experience information.
```

</details>

{/* The anchor on this section (connect-clients-to-use-the-servers-actions) is currently a link target in the Langflow UI. Do not change. */}
## Connect clients to your Langflow MCP server {#connect-clients-to-use-the-servers-actions}

Langflow provides automatic installation and code snippets to help you deploy your Langflow MCP servers to your local MCP clients.

<Tabs>
<TabItem value="JSON" label="JSON" default>

The JSON option allows you to connect a Langflow MCP server to any local or remote MCP client.
You can modify this process for any [MCP-compatible client](https://modelcontextprotocol.io/clients).

1. Install any [MCP-compatible client](https://modelcontextprotocol.io/clients).

    These steps use Cursor as an example, but the process is generally the same for all clients, with slight differences in client-specific details like file names.

2. In your client, add a new MCP server using the client's UI or configuration file.

    For example, in Cursor, go to **Cursor Settings**, select **MCP**, and then click **Add New Global MCP Server** to open Cursor's global `mcp.json` configuration file.

3. Recommended: Configure [authentication](#authentication) for your MCP server.

4. In Langflow, on the **Projects** page, click the **MCP Server** tab.

5. Click the **JSON** tab, copy the code snippet for your operating system, and then paste it into your client's MCP configuration file.
For example:

    ```json
    {
      "mcpServers": {
        "PROJECT_NAME": {
          "command": "uvx",
          "args": [
            "mcp-proxy",
            "http://LANGFLOW_SERVER_ADDRESS/api/v1/mcp/project/PROJECT_ID/sse"
          ]
        }
      }
    }
    ```

    The **MCP Server** tab automatically populates the `PROJECT_NAME`, `LANGFLOW_SERVER_ADDRESS`, and `PROJECT_ID` values.

    The default Langflow server address is `http://localhost:7860`.
    If you are using a [public Langflow server](/deployment-public-server), the server address is automatically included.

    If your Langflow server requires authentication, you must include your Langflow API key or OAuth settings in the configuration.
    For more information, see [MCP server authentication](#authentication).

6. To include other environment variables with your MCP server command, add an `env` object with key-value pairs of environment variables:

    ```json
    {
      "mcpServers": {
        "PROJECT_NAME": {
          "command": "uvx",
          "args": [
            "mcp-proxy",
            "http://LANGFLOW_SERVER_ADDRESS/api/v1/mcp/project/PROJECT_ID/sse"
          ],
          "env": {
            "KEY": "VALUE"
          }
        }
      }
    }
    ```

7. Save and close your client's MCP configuration file.

8. Confirm that your Langflow MCP server is on the client's list of MCP servers.
If necessary, restart your client to apply the modified configuration file.

</TabItem>
<TabItem value="auto" label="Auto install">

:::info
The auto install option is available only for specific MCP clients.
Auto install requires the client to be installed locally so Langflow can write to the client's configuration file.
If your client isn't supported, is installed remotely, or you need to pass additional environment variables, use the **JSON** option.
:::

1. Install [Cursor](https://docs.cursor.com/get-started/installation), [Claude](https://claude.ai/download), or [Windsurf](https://windsurf.com/download/editor) on the same computer where your Langflow server is running.

2. Recommended: Configure [authentication](#authentication) for your MCP server.

3. In Langflow, on the **Projects** page, click the **MCP Server** tab.

4. On the **Auto install** tab, find your MCP client provider, and then click <Icon name="Plus" aria-hidden="true"/> **Add**.

    Your Langflow project's MCP server is automatically added to the configuration file for your local Cursor, Claude, or Windsurf client.
    For example, with Cursor, the server configuration is added to the `mcp.json` configuration file.

    Langflow attempts to add this configuration even if the selected client isn't installed.
    To verify the installation, check the available MCP servers in your client.

</TabItem>
</Tabs>

Once your MCP client is connected to your Langflow project's MCP server, your flows are registered as tools.
Cursor determines when to use tools based on your queries, and requests permissions when necessary.
For more information, see the MCP documentation for your client, such as [Cursor's MCP documentation](https://docs.cursor.com/context/model-context-protocol).

## MCP server authentication {#authentication}

Each [Langflow project](/concepts-flows#projects) has its own MCP server with its own MCP server authentication settings.

To configure authentication for a Langflow MCP server, go to the **Projects** page in Langflow, click the **MCP Server** tab, click <Icon name="Fingerprint" aria-hidden="true"/> **Edit Auth**, and then select your preferred authentication method:

<Tabs groupId="auth-type">
<TabItem value="API key" label="API key">

When authenticating your MCP server with a Langflow API key, your project's MCP server **JSON** code snippets and **Auto install** configuration automatically include the `--headers` and `x-api-key` arguments.

Click <Icon name="key" aria-hidden="true"/> **Generate API key** to automatically insert a new Langflow API key into the code template.
Alternatively, you can replace `YOUR_API_KEY` with an existing Langflow API key.

</TabItem>
<TabItem value="OAuth" label="OAuth">

When OAuth is enabled, Langflow automatically starts an [MCP Composer](https://pypi.org/project/mcp-composer) instance for your project, creating a secure client-side proxy between MCP clients and the `mcp-proxy` on your server.

OAuth integration allows your Langflow MCP server to authenticate users and applications through any OAuth 2.0 compliant service. When users or applications connect to your MCP server, they are redirected to your chosen OAuth provider to authenticate. Upon successful authentication, they are granted access to your flows as MCP tools.

Before configuring OAuth in Langflow, you must first set up an OAuth application with an external OAuth 2.0 service provider.
You must register your Langflow server as an OAuth client, and then obtain the required values to complete the configuration in Langflow.

The following table describes the required values.
[GitHub OAuth](https://github.com/settings/developers) is used for example purposes.
Be sure to use the actual values from your own deployment.
For more information, see your OAuth provider's documentation.

| Field | Description | Source | Example |
|-------|-------------|--------|---------|
| **Host** | OAuth server host | MCP Composer default. | `localhost` |
| **Port** | OAuth server port | MCP Composer default. | `9000` |
| **Server URL** | Full OAuth server URL | Combines the MCP Composer default OAuth host and port. | `http://localhost:9000` |
| **Callback Path** | OAuth callback URL on your server | You define this address during OAuth app registration. | `http://localhost:9000/auth/idaas/callback` |
| **Client ID** | Your OAuth client identifier | From your OAuth provider. | `Ov23li9vx2grVL61qjb` |
| **Client Secret** | Your OAuth client secret | From your OAuth provider. | `1234567890abcdef1234567890abcdef12345678` |
| **Authorization URL** | OAuth authorization endpoint | From your OAuth provider. | `https://github.com/login/oauth/authorize` |
| **Token URL** | OAuth token endpoint for getting refresh tokens | From your OAuth provider. | `https://github.com/login/oauth/access_token` |
| **MCP Scope** | Scope for MCP operations | You define this. As of Langflow 1.6, `user` is the only available value. | `user` |
| **Provider Scope** | OAuth provider scope | You define this. As of Langflow 1.6, `openid` is the only available value. | `openid` |

To configure OAuth authentication:

1. Select **OAuth** as the authentication type.
2. Configure the OAuth settings with the values from your OAuth deployment.
    All values are required.

    The OAuth credentials are encrypted and stored securely in your Langflow database.

3. Click **Save**.

    Your MCP server's **JSON** code snippets and **Auto install** configuration are automatically updated with OAuth values. These are automatically used for new installations after enabling OAuth. However, you must manually update any existing installations, as explained in the next step.

4. If you already installed your Langflow MCP server in your MCP client, you must update your MCP client configuration to use the new OAuth settings after enabling OAuth on your MCP server.
The client update method depends on how you installed the server on the client:

    - **Auto install**: Manually update your client's config file using the updated JSON snippet from the **JSON** tab, or repeat the steps in [Auto-install](#connect-clients-to-use-the-servers-actions) to re-install the client with the updated settings.
    - **JSON option**: Copy the updated JSON snippet from the **JSON** tab and replace your existing configuration.
    - **New connections**: Use either the **Auto install** or **JSON** option. The OAuth settings are included automatically.

After you enable OAuth and update your client configuration, an OAuth callback window opens each time your MCP client attempts to authenticate with the server.
A successful authentication returns `Authentication complete. You may close this window.`
If your client doesn't open the OAuth window, try restarting the client to retrieve the updated configuration.

</TabItem>
<TabItem value="None" label="None">

When no authentication is configured, your MCP server becomes a public endpoint that anyone can access without providing credentials.
Only use this option when Langflow is running in a trusted environment.

</TabItem>
</Tabs>

## MCP server environment variables

The following environment variables set behaviors related to your Langflow projects' MCP servers:

| Variable | Format | Default | Description |
|----------|--------|---------|-------------|
| `LANGFLOW_MCP_SERVER_ENABLED` | Boolean | `True` | Whether to initialize an MCP server for each of your Langflow projects. If `false`, Langflow doesn't initialize MCP servers. |
| `LANGFLOW_MCP_SERVER_ENABLE_PROGRESS_NOTIFICATIONS` | Boolean | `False` | If `true`, Langflow MCP servers send progress notifications. |
| `LANGFLOW_MCP_SERVER_TIMEOUT` | Integer | `20` | The number of seconds to wait before an MCP server operation expires due to poor connectivity or long-running requests. |
| `LANGFLOW_MCP_MAX_SESSIONS_PER_SERVER` | Integer | `10` | Maximum number of MCP sessions to keep per unique server. |

{/* The anchor on this section (deploy-your-server-externally) is currently a link target in the Langflow UI. Do not change. */}
### Deploy your Langflow MCP server externally {#deploy-your-server-externally}

To deploy your Langflow MCP server externally, see [Deploy a public Langflow server](/deployment-public-server).

## Use MCP Inspector to test and debug flows {#test-and-debug-flows}

:::info Node prerequisite
MCP Inspector requires any LTS version of [Node.js](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm) installed on your computer.
:::
[MCP Inspector](https://modelcontextprotocol.io/docs/tools/inspector) is a common tool for testing and debugging MCP servers.
You can use MCP Inspector to monitor your flows and get insights into how they are being consumed by the MCP server.

1. Install MCP Inspector:

    ```bash
    npx @modelcontextprotocol/inspector
    ```

    For more information about configuring MCP Inspector, including specifying a proxy port, see the [MCP Inspector GitHub project](https://github.com/modelcontextprotocol/inspector).

2. Open a web browser and navigate to the MCP Inspector UI.
The default address is `http://localhost:6274`.

3. In the MCP Inspector UI, enter the connection details for your Langflow project's MCP server.
    The field values depend on your server's method of [authentication](#authentication).
    <Tabs groupId="auth-type">
      <TabItem value="API key" label="API key" default>

        - **Transport Type**: Select **STDIO**.
        - **Command**: `uvx`
        - **Arguments**: Enter the following list of arguments, separated by spaces. Replace the values for `YOUR_API_KEY`, `LANGFLOW_SERVER_ADDRESS`, and `PROJECT_ID` with the values from your Langflow MCP server. For example:
            ```bash
            mcp-proxy --headers x-api-key YOUR_API_KEY http://LANGFLOW_SERVER_ADDRESS/api/v1/mcp/project/PROJECT_ID/sse
            ```

      </TabItem>
      <TabItem value="OAuth" label="OAuth">

        - **Transport Type**: Select **STDIO**.
        - **Command**: `uvx`
        - **Arguments**: Enter the following list of arguments, separated by spaces. Replace the value for `OAUTH_SERVER_URL` with the URL of your OAuth server. For example:
            ```bash
            mcp-composer --mode stdio --sse-url http://localhost:9000/sse --disable-composer-tools --client_auth_type oauth
            ```

      </TabItem>
      <TabItem value="None" label="None">

        - **Transport Type**: Select **SSE**.
        - **URL**: Enter the Langflow MCP server's `sse` endpoint. For example:
            ```bash
            http://localhost:7860/api/v1/mcp/project/d359cbd4-6fa2-4002-9d53-fa05c645319c/sse
            ```

      </TabItem>
    </Tabs>

4. Click **Connect**.

    If the connection was successful, you should see your project's flows in the **Tools** tab.
    From this tab, you can monitor how your flows are being registered as tools by MCP, as well as test the tools with custom input values.

5. To quit MCP Inspector, press <kbd>Control+C</kbd> in the same terminal window where you started it.

## Troubleshoot Langflow MCP servers {#troubleshooting-mcp-server}

For troubleshooting advice for MCP servers and clients, see [Troubleshoot Langflow: MCP issues](/troubleshoot#mcp).

## See also

* [Use Langflow as an MCP client](/mcp-client)
* [Use a DataStax Astra DB MCP server](/mcp-component-astra)
* [MCP server environment variables](/environment-variables#mcp)


================================================================================
filename: Components/_bundles-cometapi.mdx
================================================================================

---
title: CometAPI
slug: /bundles-cometapi
description: Use CometAPI models in Langflow to generate text or output a Language Model for downstream Components.
---

import Icon from "@site/src/components/icon";

[Bundles](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **CometAPI** bundle.

For more information about CometAPI features and functionality used by CometAPI components, see the [CometAPI documentation](https://www.cometapi.com/).

## CometAPI text generation

This component generates text using CometAPI's language models through the CometAPI OpenAI-compatible endpoints.

It can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).

Use the **Language Model** output when you want to use a CometAPI model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Transform** component.

For more information, see [Language model components](/components-models).

### CometAPI parameters

import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| api_key | SecretString | Input parameter. Your CometAPI API Key for authentication. |
| model_name | String | Input parameter. The id of the CometAPI model to use. The component automatically fetches the latest available models from CometAPI when you provide a valid API key. For information about supported models, see the [CometAPI documentation](https://www.cometapi.com/). |
| input_value | String | Input parameter. The input text to send to the model. |
| system_message | String | Input parameter. A system message that helps set the behavior of the assistant. |
| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Set to 0 for unlimited tokens. |
| temperature | Float | Input parameter. Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.1. |
| seed | Integer | Input parameter. The seed controls the reproducibility of the job (advanced). |
| model_kwargs | Dict | Input parameter. Additional keyword arguments to pass to the model (advanced). |
| json_mode | Boolean | Input parameter. If True, it will output JSON regardless of passing a schema (advanced). |
| stream | Boolean | Input parameter. Whether to stream the response. Default: false. |
| output_parser | OutputParser | Input parameter. The parser to use to parse the output of the model (advanced). |
| model | LanguageModel | Output parameter. An instance of ChatOpenAI configured with CometAPI parameters. |

## Use CometAPI in a flow

1. Sign up for a [CometAPI account](https://www.cometapi.com/).
2. Obtain your API key from the CometAPI dashboard.
3. In Langflow, add the **CometAPI** component to your flow.
4. Enter your API key in the **CometAPI API Key** field.
5. Select your preferred model from the **Model Name** menu.
6. Configure other parameters as needed for your use case.
7. Add other components to your flow as needed.

    To perform a basic test, add **Chat Input** and **Chat Output** components to your flow, connect them to the **CometAPI** component accordingly, and then click **Playground** to test the connection and chat with your model.

    For more advanced use cases, you can connect the CometAPI component to other components like **Prompt Template**, **Agent**, or **Smart Transform** components.


================================================================================
filename: Components/bundles-aiml.mdx
================================================================================

---
title: AI/ML API
slug: /bundles-aiml
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **AI/ML** bundle.

## AI/ML API text generation

This component creates a `ChatOpenAI` model instance using the AI/ML API.
The output is exclusively a **Language Model** ([`LanguageModel`](/data-types#languagemodel)) that you can connect to another LLM-driven component, such as a **Smart Function** component.

For more information, see the [AI/ML API Langflow integration documentation](https://docs.aimlapi.com/integrations/langflow) and [Language model components](/components-models).

### AI/ML API text generation parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Set to 0 for unlimited tokens. Range: 0-128000. |
| model_kwargs | Dictionary | Input parameter. Additional keyword arguments for the model. |
| model_name | String | Input parameter. The name of the AIML model to use. Options are predefined in `AIML_CHAT_MODELS`. |
| aiml_api_base | String | Input parameter. The base URL of the AIML API. Defaults to `https://api.aimlapi.com`. |
| api_key | SecretString | Input parameter. The AIML API Key to use for the model. |
| temperature | Float | Input parameter. Controls randomness in the output. Default: `0.1`. |

## AI/ML API Embeddings

The **AI/ML API Embeddings** component generates embeddings using the [AI/ML API](https://docs.aimlapi.com/api-overview/embeddings).

The output is [`Embeddings`](/data-types#embeddings).
Specifically, an instance of `AIMLEmbeddingsImpl`.

For more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).

### AI/ML API Embeddings parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| model_name | String | Input parameter. The name of the AI/ML embedding model to use. |
| aiml_api_key | SecretString | Input parameter. The API key required for authenticating with the AI/ML service. |


================================================================================
filename: Components/bundles-amazon.mdx
================================================================================

---
title: Amazon
slug: /bundles-amazon
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Amazon** bundle.

## Amazon Bedrock

This component generates text using [Amazon Bedrock LLMs](https://docs.aws.amazon.com/bedrock).

It can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).
Specifically, the **Language Model** output is an instance of [`ChatBedrock`](https://docs.langchain.com/oss/python/integrations/chat/bedrock) configured according to the component's parameters.

Use the **Language Model** output when you want to use an Amazon Bedrock model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.

For more information, see [Language model components](/components-models).

### Amazon Bedrock parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| input | String | Input parameter. The input string for text generation. |
| system_message | String | Input parameter. A system message to pass to the model. |
| stream | Boolean | Input parameter. Whether to stream the response. Only works in chat. Default: `false`. |
| model_id | String | Input parameter. The Amazon Bedrock model to use. |
| aws_access_key_id | SecretString | Input parameter. AWS Access Key for authentication. |
| aws_secret_access_key | SecretString | Input parameter. AWS Secret Key for authentication. |
| aws_session_token | SecretString | Input parameter. The session key for your AWS account. |
| credentials_profile_name | String | Input parameter. Name of the AWS credentials profile to use. |
| region_name | String | Input parameter. AWS region where your Bedrock resources are located. Default: `us-east-1`. |
| model_kwargs | Dictionary | Input parameter. Additional keyword arguments to pass to the model. |
| endpoint_url | String | Input parameter. Custom endpoint URL for a Bedrock service. |

## Amazon Bedrock Embeddings

The **Amazon Bedrock Embeddings** component is used to load embedding models from [Amazon Bedrock](https://aws.amazon.com/bedrock/).

For more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).

### Amazon Bedrock Embeddings parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| model_id | String | Input parameter. The ID of the model to call, such as `amazon.titan-embed-text-v1`. This is equivalent to the `modelId` property in the `list-foundation-models` API. |
| aws_access_key_id | SecretString | Input parameter. AWS Access Key for authentication. |
| aws_secret_access_key | SecretString | Input parameter. AWS Secret Key for authentication. |
| aws_session_token | SecretString | Input parameter. The session key for your AWS account. |
| credentials_profile_name | String | Input parameter. The name of the AWS credentials profile in `~/.aws/credentials` or `~/.aws/config`, which has access keys or role information. |
| region_name | String | Input parameter. The AWS region to use, such as `us-west-2`. Falls back to the `AWS_DEFAULT_REGION` environment variable or region specified in `~/.aws/config` if not provided. |
| endpoint_url | String | Input parameter. The URL to set a specific service endpoint other than the default AWS endpoint. |

## S3 Bucket Uploader

The **S3 Bucket Uploader** component uploads files to an Amazon S3 bucket.
It is designed to process `Data` input from a **File** or **Directory** component.
If you upload `Data` from other components, test the results before running the flow in production.

Requires the `boto3` package, which is included in your Langflow installation.

The component produces logs but it doesn't emit output to the flow.

### S3 Bucket Uploader parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| **AWS Access Key ID** | SecretString | Input parameter. AWS Access Key ID for authentication. |
| **AWS Secret Key** | SecretString | Input parameter. AWS Secret Key for authentication. |
| **Bucket Name** | String | Input parameter. The name of the S3 bucket to upload files to. |
| **Strategy for file upload** | String | Input parameter. The file upload strategy. **Store Data** (default) iterates over `Data` inputs, logs the file path and text content, and uploads each file to the specified S3 bucket if both file path and text content are available. **Store Original File** iterates through the list of data inputs, retrieves the file path from each data item, uploads the file to the specified S3 bucket if the file path is available, and logs the file path being uploaded. |
| **Data Inputs** | Data | Input parameter. The `Data` input to iterate over and upload as files in the specified S3 bucket. |
| **S3 Prefix** | String | Input parameter. Optional prefix (folder path) within the S3 bucket where files will be uploaded. |
| **Strip Path** | Boolean | Input parameter. Whether to strip the file path when uploading. Default: `false`. |


================================================================================
filename: Components/bundles-anthropic.mdx
================================================================================

---
title: Anthropic
slug: /bundles-anthropic
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Anthropic** bundle.

For more information about Anthropic features and functionality used by Anthropic components, see the [Anthropic documentation](https://docs.anthropic.com/en/docs/intro).

## Anthropic text generation

The **Anthropic** component generates text using Anthropic Chat and Language models like Claude.

It can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).
Specifically, the **Language Model** output is an instance of [`ChatAnthropic`](https://docs.langchain.com/oss/python/integrations/chat/anthropic) configured according to the component's parameters.

Use the **Language Model** output when you want to use an Anthropic model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.

For more information, see [Language model components](/components-models).

### Anthropic text generation parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Set to 0 for unlimited tokens. Default: `4096`. |
| model | String | Input parameter. The name of the Anthropic model to use. Options include various Claude 3 models. |
| anthropic_api_key | SecretString | Input parameter. Your Anthropic API key for authentication. |
| temperature | Float | Input parameter. Controls randomness in the output. Default: `0.1`. |
| anthropic_api_url | String | Input parameter. Endpoint of the Anthropic API. Defaults to `https://api.anthropic.com` if not specified. |
| prefill | String | Input parameter. Prefill text to guide the model's response. |


================================================================================
filename: Components/bundles-apify.mdx
================================================================================

---
title: Apify
slug: /bundles-apify
---

import Icon from "@site/src/components/icon";

[Apify](https://apify.com/) is a web scraping and data extraction platform with more than 3,000 ready-made cloud tools called **Actors**.

Your flows can use the **Apify Actors** component to run **Actors** to accomplish tasks like data extraction, content analysis, and SQL operations.

## Use the Apify Actors component in a flow

1. Add an **Apify Actors** component to your flow, and then configure it as follows:

    * **Apify Token**: Enter your [Apify API token](https://docs.apify.com/platform/integrations/api).
    * **Actor**: Enter the ID of the Actor you want to run from the [Apify Actor Store](https://apify.com/store). For example, the [Website Content Crawler](https://apify.com/apify/website-content-crawler) has the Actor ID `apify/website-content-crawler`.
    * **Run Input**: Enter the [JSON input for configuring the Actor run](https://docs.apify.com/platform/actors/running-actors#input).
    * Configure additional parameters and commands depending on the Actor you chose and your use case.

2. Connect the component to other components in your flow.
The component can be used to perform tasks as a standalone step in a flow or as a tool for an agent.

    To enable **Tool Mode** for this component, change the component's output type from **Output** to **Tool**, and then connect it to the **Tools** port on an **Agent** component.

    **Apify Actors** components output the results of the Actor run as a JSON object in Langflow's [`Data` type](/data-types#data).

## Example Apify Actors flows

The following scenarios provide a few examples of how you could use the **Apify Actors** components in Langflow:

* **Extract website text content in Markdown**: Use the [Website Content Crawler Actor](https://apify.com/apify/website-content-crawler) to extract text content in Markdown format from a website, and then connect the **Output** to a **Parser** component's input for further processing.

    ![Apify Flow - Website Content Crawler](/img/apify_flow_wcc.png)

* **Process web content with an agent**: Attach a [Website Content Crawler Actor](https://apify.com/apify/website-content-crawler) to an **Agent** component as a tool so the agent can decide whether to extract website content based on the chat input. The agent could take the extracted data and transform it into summaries, insights, or structured responses to make the information more actionable.

    ![Apify Agent Flow - Simple](/img/apify_agent_flow_simple.png)

* **Analyze social media profiles with multiple actors**: Perform comprehensive social media research with multiple Apify Actors:

    * Use the [Google Search Results Scraper Actor](https://apify.com/apify/google-search-scraper) to find relevant social media profiles.
    * Use the [TikTok Data Extractor Actor](https://apify.com/clockworks/free-tiktok-scraper) to gather data and videos.
    * Attach both actors as tools for an **Agent** component to collect the links from Google and content from TikTok, and then analyze the data to provide insights about a person, brand, or topic.

    ![Apify Agent Flow](/img/apify_agent_flow.png)


================================================================================
filename: Components/bundles-arxiv.mdx
================================================================================

---
title: arXiv
slug: /bundles-arxiv
---

import Icon from "@site/src/components/icon";

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **arXiv** bundle.

## arXiv search

This component searches and retrieves papers from [arXiv.org](https://arXiv.org).

It returns a list of search results as a [`DataFrame`](/data-types#dataframe).

### arXiv search parameters

| Name | Type | Description |
|------|------|-------------|
| search_query | String | Input parameter. The search query for arXiv papers. For example, `quantum computing`. |
| search_type | String | Input parameter. The field to search in. |
| max_results | Integer | Input parameter. The maximum number of results to return. |

## See also

* [**Web Search** component](/components-data#web-search)


================================================================================
filename: Components/bundles-assemblyai.mdx
================================================================================

---
title: AssemblyAI
slug: /bundles-assemblyai
---

import Icon from "@site/src/components/icon";

The AssemblyAI components allow you to apply powerful Speech AI models to your app for tasks like:

- Transcribing audio and video files
- Formatting transcripts
- Generating subtitles
- Applying LLMs to audio files

For more information about AssemblyAI features and functionality used by AssemblyAI components, see the [AssemblyAI API Docs](https://www.assemblyai.com/docs).

## Prerequisites

* An [AssemblyAI account](https://www.assemblyai.com/dashboard/signup) and an AssemblyAI API key.

    Enter the key in the *AssemblyAI API Key* field in all Langflow components that require the AssemblyAI key.

* Optional: To use LeMUR, you need a paid AssemblyAI account because LeMUR isn't included in the free account.

## Components

![AssemblyAI components](/img/assemblyai-components.png)

### AssemblyAI Start Transcript

This component allows you to submit an audio or video file for transcription.

**Tip**: You can freeze the path of this component to only submit the file once.

- **Input**:
    - AssemblyAI API Key: Your API key.
    - Audio File: The audio or video file to transcribe.
    - Speech Model (Optional): Select the class of models. Default is *Best*. See [speech models](https://www.assemblyai.com/docs/speech-to-text/speech-recognition#select-the-speech-model-with-best-and-nano) for more info.
    - Automatic Language Detection (Optional): Enable automatic language detection.
    - Language (Optional): The language of the audio file. Can be set manually if automatic language detection is disabled.
            See [supported languages](https://www.assemblyai.com/docs/getting-started/supported-languages) for a list of supported language codes.
    - Enable Speaker Labels (Optional): Detect speakers in an audio file and what each speaker said.
    - Expected Number of Speakers (Optional): Set the expected number of speakers, if Speaker Labels is enabled.
    - Audio File URL (Optional): The URL of the audio or video file to transcribe. Can be used instead of *Audio File*.
    - Punctuate (Optional): Apply punctuation. Default is `true`.
    - Format Text (Optional): Apply casing and text formatting. Default is `true`.

- **Output**:
    - Transcript ID: The id of the transcript


### AssemblyAI Poll Transcript

This components allows you to poll the transcripts. It checks the status of the transcript every few seconds until the transcription is completed.

- **Input**:
    - AssemblyAI API Key: Your API key.
    - Polling Interval (Optional): The polling interval in seconds. Default is 3.

- **Output**:
    - Transcription Result: The AssemblyAI JSON response of a completed transcript. Contains the text and other info.


### AssemblyAI Get Subtitles

This component allows you to generate subtitles in SRT or VTT format.

- **Input**:
    - AssemblyAI API Key: Your API key.
    - Transcription Result: The output of the **Poll Transcript** component.
    - Subtitle Format: The format of the captions (SRT or VTT).
    - Character per Caption (Optional): The maximum number of characters per caption (0 for no limit).

- **Output**:
    - Subtitles: A JSON response with the `subtitles` field containing the captions in SRT or VTT format.


### AssemblyAI LeMUR

This component allows you to apply Large Language Models to spoken data using the [AssemblyAI LeMUR framework](https://www.assemblyai.com/docs/lemur).

LeMUR automatically ingests the transcript as additional context, making it easy to apply LLMs to audio data. You can use it for tasks like summarizing audio, extracting insights, or asking questions.

- **Input**:
    - AssemblyAI API Key: Your API key.
    - Transcription Result: The output of the **Poll Transcript** component.
    - Input Prompt: The text to prompt the model. You can type your prompt in this field or connect it to a **Prompt Template** component.
    - Final Model: The model that is used for the final prompt after compression is performed. Default is Claude 3.5 Sonnet.
    - Temperature (Optional): The temperature to use for the model. Default is 0.0.
    - Max Output Size (Optional): Max output size in tokens, up to 4000. Default is 2000.
    - Endpoint (Optional): The LeMUR endpoint to use. Default is "task". For "summary" and "question-answer", no prompt input is needed. See [LeMUR API docs](https://www.assemblyai.com/docs/api-reference/lemur/) for more info.
    - Questions (Optional): Comma-separated list of your questions. Only used if *Endpoint* is "question-answer".
    - Transcript IDs (Optional): Comma-separated list of transcript IDs. LeMUR can perform actions over multiple transcripts. If provided, the *Transcription Result* is ignored.

- **Output**:
    - LeMUR Response: The generated LLM response.

### AssemblyAI List Transcripts

This component can be used as a standalone component to list all previously generated transcripts.

- **Input**:
    - AssemblyAI API Key: Your API key.
    - Limit (Optional): Maximum number of transcripts to retrieve. Default is 20, use 0 for all.
    - Filter (Optional): Filter by transcript status.
    - Created On (Optional): Only get transcripts created on this date (YYYY-MM-DD).
    - Throttled Only (Optional): Only get throttled transcripts, overrides the status filter

- **Output**:
    - Transcript List: A list of all transcripts with info such as the transcript ID, the status, and the data.


## Flow Process

1. The user inputs an audio or video file.
2. The user can also input an LLM prompt. In this example, we want to generate a summary of the transcript.
3. The flow submits the audio file for transcription.
4. The flow checks the status of the transcript every few seconds until transcription is completed.
5. The flow parses the transcription result and outputs the transcribed text.
6. The flow also generates subtitles.
7. The flow applies the LLM prompt to generate a summary.
8. As a standalone component, all transcripts can be listed.

## Run the Transcription and Speech AI Flow

1. Build the flow manually or import a pre-build JSON file:

    * Recommended: [Download the AssemblyAI Transcription and Speech AI flow JSON](/files/AssemblyAI_Flow.json), and then [import the flow](/concepts-flows-import) into Langflow.
    * Create a blank flow, and then add the previously described components to your flow, connecting them as shown in the flow diagram.

2. Input your AssemblyAI API key in all components that require the key (**Start Transcript**, **Poll Transcript**, **Get Subtitles**, **LeMUR**, **List Transcripts**).

3. Select an audio or video file for the **Start Transcript** component.

    Optional: After adding a file to the **Start Transcript** component, run and [freeze the component](/concepts-components#freeze-a-component) so you only submit the file once, no matter how many times you run the flow.
    To do this, click <Icon name="Play" aria-hidden="true" /> **Run component** to preload the file, and then click <Icon name="Ellipsis" aria-hidden="true" /> **Show More** and select **Freeze** to lock the result.
    Subsequent flow runs use the frozen component's cached output.

4. Test the transcription by clicking <Icon name="Play" aria-hidden="true" /> **Run component** on the **Parser** component. Make sure that the specified template is `{text}`.

    Running one component runs all upstream components as well as the selected component and then stops the flow run.
    In this case, the **Start Transcript** and **Poll Transcript** components are upstream from the **Parser** component.
    If you froze the **Start Transcript** component, the flow sends the cached output from **Start Transcript**, runs the **Poll Transcript** component, to get the transcription result.
    Check the flow logs or inspect the output of the **Parser** component to see the transcribed text result.

5. To generate subtitles and run the full flow, click <Icon name="Play" aria-hidden="true" /> **Run component** on the **List Transcript** component.

## Customization

The flow can be customized by:

1. Modifying the parameters in the **Start Transcript** component.
2. Modifying the subtitle format in the **Get Subtitles** component.
3. Modifying the LLM prompt for input of the **LeMUR** component.
4. Modifying the LLM parameters (e.g., temperature) in the **LeMUR** component.

## Troubleshooting

If you encounter issues:

1. Ensure the API key is correctly set in all components that require the key.
2. To use LeMUR, you need to upgrade your AssemblyAI account, since this isn't included in the free account.
3. Verify that all components are properly connected in the flow.
4. Review the Langflow logs for any error messages.
5. Check the [AssemblyAI API documentation](https://www.assemblyai.com/docs/).
6. Contact [AssemblyAI support](https://www.assemblyai.com/contact/support).


================================================================================
filename: Components/bundles-azure.mdx
================================================================================

---
title: Azure
slug: /bundles-azure
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Azure** bundle.

## Azure OpenAI

This component generates text using [Azure OpenAI LLMs](https://learn.microsoft.com/en-us/azure/ai-services/openai/).

It can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).
Specifically, the **Language Model** output is an instance of [`AzureChatOpenAI`](https://docs.langchain.com/oss/python/integrations/chat/azure_chat_openai) configured according to the component's parameters.

Use the **Language Model** output when you want to use an Azure OpenAI model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.

For more information, see [Language model components](/components-models).

### Azure OpenAI parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| Model Name | String | Input parameter. Specifies the name of the Azure OpenAI model to be used for text generation. |
| Azure Endpoint | String | Input parameter. Your Azure endpoint, including the resource. |
| Deployment Name | String | Input parameter. Specifies the name of the deployment. |
| API Version | String | Input parameter. Specifies the version of the Azure OpenAI API to be used. |
| API Key | SecretString | Input parameter. Your Azure OpenAI API key. |
| Temperature | Float | Input parameter. Specifies the sampling temperature. Defaults to `0.7`. |
| Max Tokens | Integer | Input parameter. Specifies the maximum number of tokens to generate. Defaults to `1000`. |
| Input Value | String | Input parameter. Specifies the input text for text generation. |
| Stream | Boolean | Input parameter. Specifies whether to stream the response from the model. Default to `false`. |

## Azure OpenAI Embeddings

The **Azure OpenAI Embeddings** component generates embeddings using Azure OpenAI models.

For more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).

### Azure OpenAI Embeddings parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| Model | String | Input parameter. The name of the model to use. Default: `text-embedding-3-small`. |
| Azure Endpoint | String | Input parameter. Your Azure endpoint, including the resource, such as `https://example-resource.azure.openai.com/`. |
| Deployment Name | String | Input parameter. The name of the deployment. |
| API Version | String | Input parameter. The API version to use, with options including various dates. |
| API Key | String | Input parameter. The API key required to access the Azure OpenAI service. |


================================================================================
filename: Components/bundles-baidu.mdx
================================================================================

---
title: Baidu
slug: /bundles-baidu
---

import Icon from "@site/src/components/icon";

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Baidu** bundle.

## Qianfan

The **Qianfan** component generates text using Qianfan's language models.

It can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).

Use the **Language Model** output when you want to use a Qianfan model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.

For more information, see [Language model components](/components-models) and the [Qianfan documentation](https://github.com/baidubce/bce-qianfan-sdk).


================================================================================
filename: Components/bundles-bing.mdx
================================================================================

---
title: Bing
slug: /bundles-bing
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Bing** bundle.

## Bing Search API

This component allows you to call the Bing Search API.

It returns a list of search results as a [`DataFrame`](/data-types#dataframe).

### Bing Search API parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| bing_subscription_key | SecretString | Input parameter. A Bing API subscription key. |
| input_value | String | Input parameter. The search query input. |
| bing_search_url | String | Input parameter. A custom Bing Search URL. |
| k | Integer | Input parameter. The number of search results to return. |

## See also

* [**Web Search** component](/components-data#web-search)
* [**SearchApi** bundle](/bundles-searchapi)


================================================================================
filename: Components/bundles-cassandra.mdx
================================================================================

---
title: Cassandra
slug: /bundles-cassandra
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';
import PartialConditionalParams from '@site/docs/_partial-conditional-params.mdx';
import PartialVectorSearchResults from '@site/docs/_partial-vector-search-results.mdx';
import PartialVectorStoreInstance from '@site/docs/_partial-vector-store-instance.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Cassandra** bundle, including components that read and write to Apache Cassandra clusters, such as OSS Cassandra and Astra DB databases.

## Cassandra vector store

Use the **Cassandra** component to read or write to a Cassandra-based vector store using a `CassandraVectorStore` instance.

<details>
<summary>About vector store instances</summary>

<PartialVectorStoreInstance />

</details>

<PartialVectorSearchResults />

:::tip
For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).
:::

### Cassandra parameters

You can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.

<PartialParams />

<PartialConditionalParams />

For information about accepted values and functionality, see [Vector search in Cassandra](https://cassandra.apache.org/doc/latest/cassandra/vector-search/overview.html) or inspect [component code](/concepts-components#component-code).

| Name | Type | Description |
|------|------|-------------|
| database_ref | String | Input parameter. Contact points for the database or an Astra database ID. |
| username | String | Input parameter. Username for the database. Leave empty for Astra DB. |
| token | SecretString | Input parameter. User password for the database or an Astra application token. |
| keyspace | String | Input parameter. The name of the keyspace containing the vector store specified in **Table Name** (`table_name`). |
| table_name | String | Input parameter. The name of the table or collection that is the vector store. |
| ttl_seconds | Integer | Input parameter. Time-to-live for added texts, if supported by the cluster. Only relevant for writes. |
| batch_size | Integer | Input parameter. Amount of records to process in a single batch. |
| setup_mode | String | Input parameter. Configuration mode for setting up a Cassandra table. |
| cluster_kwargs | Dict | Input parameter. Additional keyword arguments for a Cassandra cluster. |
| search_query | String | Input parameter. Query string for similarity search. Only relevant for reads. |
| ingest_data | Data | Input parameter. Data to be loaded into the vector store as raw chunks and embeddings. Only relevant for writes. |
| embedding | Embeddings | Input parameter. Embedding function to use. |
| number_of_results | Integer | Input parameter. Number of results to return in search. Only relevant for reads. |
| search_type | String | Input parameter. Type of search to perform. Only relevant for reads. |
| search_score_threshold | Float | Input parameter. Minimum similarity score for search results. Only relevant for reads. |
| search_filter | Dict | Input parameter. An optional dictionary of metadata search filters to apply in addition to vector search. Only relevant for reads. |
| body_search | String | Input parameter. Document textual search terms. Only relevant for reads. |
| enable_body_search | Boolean | Input parameter. Flag to enable body search. Only relevant for reads. |

## Cassandra Chat Memory

The **Cassandra Chat Memory** component retrieves and stores chat messages using an Apache Cassandra-based database.

Chat memories are passed between memory storage components as the [`Memory`](/data-types#memory) data type.
Specifically, the component creates an instance of `CassandraChatMessageHistory`, which is a LangChain chat message history class that uses a Cassandra database for storage.

For more information about using external chat memory in flows, see the [**Message History** component](/components-helpers#message-history).

### Cassandra Chat Memory parameters

<PartialParams />

| Name           | Type          | Description   |
|----------------|---------------|-----------------------------|
| database_ref   | MessageText   | Input parameter. The contact points for the Cassandra database or Astra DB database ID. Required. |
| username       | MessageText   | Input parameter. The username for Cassandra. Leave empty for Astra DB. |
| token          | SecretString  | Input parameter. The password for Cassandra or the token for Astra DB. Required. |
| keyspace       | MessageText   | Input parameter. The keyspace in Cassandra or namespace in Astra DB. Required. |
| table_name     | MessageText   | Input parameter. The name of the table or collection for storing messages. Required. |
| session_id     | MessageText   | Input parameter. The unique identifier for the chat session. Optional. |
| cluster_kwargs | Dictionary    | Input parameter. Additional keyword arguments for the Cassandra cluster configuration. Optional. |

## Cassandra Graph

The **Cassandra Graph** component uses `CassandraGraphVectorStore`, an instance of [LangChain graph vector store](https://python.langchain.com/api_reference/community/graph_vectorstores.html), for graph traversal and graph-based document retrieval in a compatible Cassandra-based cluster.
It also supports writing to the vector store.

<PartialVectorSearchResults />

### Cassandra Graph parameters

<PartialParams />

| Name | Display Name | Info |
|------|--------------|------|
| database_ref | Contact Points / Astra Database ID | Input parameter. The contact points for the database or an Astra database ID. Required. |
| username | Username | Input parameter. The username for the database. Leave empty for Astra DB. |
| token | Password / Astra DB Token | Input parameter. The user password for the database or an Astra application token. Required. |
| keyspace | Keyspace | Input parameter. The name of the keyspace containing the vector store specified in **Table Name** (`table_name`). Required. |
| table_name | Table Name | Input parameter. The name of the table or collection that is the vector store. Required. |
| setup_mode | Setup Mode | Input parameter. The configuration mode for setting up the Cassandra table. The options are `Sync` (default) or `Off`. |
| cluster_kwargs | Cluster arguments | Input parameter. An optional dictionary of additional keyword arguments for the Cassandra cluster. |
| search_query | Search Query | Input parameter. The query string for similarity search. Only relevant for reads. |
| ingest_data | Ingest Data | Input parameter. Data to be loaded into the vector store as raw chunks and embeddings. Only relevant for writes. |
| embedding | Embedding | Input parameter. The embedding model to use. |
| number_of_results | Number of Results | Input parameter. The number of results to return in similarity search. Only relevant for reads. Default: 4. |
| search_type | Search Type | Input parameter. The search type to use. The options are `Traversal` (default), `MMR Traversal`, `Similarity`, `Similarity with score threshold`, or `MMR (Max Marginal Relevance)`. |
| depth | Depth of traversal | Input parameter. The maximum depth of edges to traverse. Only relevant if **Search Type** (`search_type`) is `Traversal` or `MMR Traversal`. Default: 1. |
| search_score_threshold | Search Score Threshold | Input parameter. The minimum similarity score threshold for search results. Only relevant for reads using the `Similarity with score threshold` search type. |
| search_filter | Search Metadata Filter | Input parameter. An optional dictionary of metadata search filters to apply in addition to graph traversal and similarity search. |

### See also

* [**DataStax** bundle](/bundles-datastax)


================================================================================
filename: Components/bundles-chroma.mdx
================================================================================

---
title: Chroma
slug: /bundles-chroma
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';
import PartialConditionalParams from '@site/docs/_partial-conditional-params.mdx';
import PartialVectorSearchResults from '@site/docs/_partial-vector-search-results.mdx';
import PartialVectorStoreInstance from '@site/docs/_partial-vector-store-instance.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Chroma** bundle.

## Chroma DB

You can use the **Chroma DB** component to read and write to a Chroma database using an instance of `Chroma` vector store.
Includes support for remote or in-memory instances with or without persistence.

<details>
<summary>About vector store instances</summary>

<PartialVectorStoreInstance />

</details>

When writing, the component can create a new database or collection at the specified location.

:::tip
An ephemeral (non-persistent) local Chroma vector store is helpful for testing vector search flows where you don't need to retain the database.
:::

<PartialVectorSearchResults />

### Use the Chroma DB component in a flow

The following example flow uses one **Chroma DB** component for both reads and writes:

![ChromaDB receiving split text](/img/component-chroma-db.png)

* When writing, it splits `Data` from a [**URL** component](/components-data#url) into chunks, computes embeddings with attached **Embedding Model** component, and then loads the chunks and embeddings into the Chroma vector store.
To trigger writes, click <Icon name="Play" aria-hidden="true"/> **Run component** on the **Chroma DB** component.

* When reading, it uses chat input to perform a similarity search on the vector store, and then print the search results to the chat.
To trigger reads, open the **Playground** and enter a chat message.

After running the flow once, you can click <Icon name="TextSearch" aria-hidden="true"/> **Inspect Output** on each component to understand how the data transformed as it passed from component to component.

### Chroma DB parameters

You can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.

<PartialParams />

<PartialConditionalParams />

For information about accepted values and functionality, see the provider's documentation or inspect [component code](/concepts-components#component-code).

| Name | Type | Description |
|------|------|-------------|
| **Collection Name** (`collection_name`) | String | Input parameter.  The name of your Chroma vector store collection. Default: `langflow`. |
| **Persist Directory** (`persist_directory`) | String | Input parameter. To persist the Chroma database, enter a relative or absolute path to a directory to store the `chroma.sqlite3` file. Leave empty for an ephemeral database. When reading or writing to an existing persistent database, specify the path to the persistent directory. |
| **Ingest Data** (`ingest_data`) | Data or DataFrame | Input parameter. `Data` or `DataFrame` input containing the records to write to the vector store. Only relevant for writes. |
| **Search Query** (`search_query`) | String | Input parameter. The query to use for vector search. Only relevant for reads. |
| **Cache Vector Store** (`cache_vector_store`) | Boolean | Input parameter. If `true`, the component caches the vector store in memory for faster reads. Default: Enabled (`true`). |
| **Embedding** (`embedding`) | Embeddings | Input parameter. The embedding function to use for the vector store. By default, Chroma DB uses its built-in embeddings model, or you can attach an **Embedding Model** component to use a different provider or model. |
| **CORS Allow Origins** (`chroma_server_cors_allow_origins`) | String | Input parameter. The allowed CORS origins for the Chroma server. |
| **Chroma Server Host** (`chroma_server_host`) | String | Input parameter. The host for the Chroma server. |
| **Chroma Server HTTP Port** (`chroma_server_http_port`) | Integer | Input parameter. The HTTP port for the Chroma server. |
| **Chroma Server gRPC Port** (`chroma_server_grpc_port`) | Integer | Input parameter. The gRPC port for the Chroma server. |
| **Chroma Server SSL Enabled** (`chroma_server_ssl_enabled`) | Boolean | Input parameter. Enable SSL for the Chroma server. |
| **Allow Duplicates** (`allow_duplicates`) | Boolean | Input parameter. If `true` (default), writes don't check for existing duplicates in the collection, allowing you to store multiple copies of the same content. If `false`, writes won't add documents that match existing documents already present in the collection. If `false`, it can strictly enforce deduplication by searching the entire collection or only search the number of records, specified in `limit`. Only relevant for writes.|
| **Search Type** (`search_type`) | String | Input parameter. The type of search to perform, either `Similarity` or `MMR`. Only relevant for reads. |
| **Number of Results** (`number_of_results`) | Integer | Input parameter. The number of search results to return. Default: `10`. Only relevant for reads. |
| **Limit** (`limit`) | Integer | Input parameter. Limit the number of records to compare when **Allow Duplicates** is `false`. This can help improve performance when writing to large collections, but it can result in some duplicate records. Only relevant for writes. |

## See also

* [**Local DB** component](/components-bundle-components#vector-stores-bundle)


================================================================================
filename: Components/bundles-cleanlab.mdx
================================================================================

---
title: Cleanlab
slug: /bundles-cleanlab
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

[Cleanlab](https://www.cleanlab.ai/) adds automation and trust to every data point going in and every prediction coming out of AI and RAG solutions.

Use the Cleanlab components to integrate Cleanlab Evaluations with Langflow and unlock trustworthy agentic, RAG, and LLM pipelines with Cleanlab's evaluation and remediation suite.

You can use these components to quantify the trustworthiness of any LLM response with a score between `0` and `1`, and explain why a response may be good or bad. For RAG or agent pipelines with context, you can evaluate context sufficiency, groundedness, helpfulness, and query clarity with quantitative scores. Additionally, you can remediate low-trust responses with warnings or fallback answers.

Authentication is required with a Cleanlab API key.

## Cleanlab Evaluator

The **Cleanlab Evaluator** component evaluates and explains the trustworthiness of a prompt and response pair using Cleanlab. For more information on how the score works, see the [Cleanlab documentation](https://help.cleanlab.ai/tlm/).

### Cleanlab Evaluator parameters

<PartialParams />

| Name                    | Type       | Description                        |
|-------------------------|------------|------------------------------------|
| system_prompt           | Message    | Input parameter. The system message prepended to the prompt. Optional. |
| prompt                  | Message    | Input parameter. The user-facing input to the LLM.  |
| response                | Message    | Input parameter. The model's response to evaluate.    |
| cleanlab_api_key        | Secret     | Input parameter. Your Cleanlab API key.  |
| cleanlab_evaluation_model | Dropdown   | Input parameter. Evaluation model used by Cleanlab, such as GPT-4 or Claude. This doesn't need to be the same model that generated the response. |
| quality_preset          | Dropdown   | Input parameter. Tradeoff between evaluation speed and accuracy. |

### Cleanlab Evaluator outputs

The **Cleanlab Evaluator** component has three possible outputs.

| Name                    | Type       | Description            |
|-------------------------|------------|-------------------------|
| score                   | number, float | Displays the trust score between 0 and 1.  |
| explanation             | `Message`    | Provides an explanation of the trust score. |
| response                | `Message`    | Returns the original response for easy chaining to the **Cleanlab Remediator** component. |

## Cleanlab Remediator

The **Cleanlab Remediator** component uses the trust score from the [**Cleanlab Evaluator** component](#cleanlab-evaluator) to determine whether to show, warn about, or replace an LLM response.

This component has parameters for the score threshold, warning text, and fallback message that you can customize as needed.

The output is **Remediated Response** (`remediated_response`), which is a `Message` containing the final message shown to the user after remediation logic is applied.

### Cleanlab Remediator parameters

| Name                        | Type       | Description |
|-----------------------------|------------|---------|
| response                    | Message    | Input parameter. The response to potentially remediate.  |
| score                       | Number     | Input parameter. The trust score from `CleanlabEvaluator`. |
| explanation                 | Message    | Input parameter. The explanation to append if a warning is shown. Optional.|
| threshold                   | Float      | Input parameter. The minimum trust score to pass a response unchanged.  |
| show_untrustworthy_response | Boolean      | Input parameter. Whether to display or hide the original response with a warning if a response is deemed untrustworthy. |
| untrustworthy_warning_text  | Prompt     | Input parameter. The warning text for untrustworthy responses. |
| fallback_text              | Prompt     | Input parameter. The fallback message if the response is hidden. |

## Cleanlab RAG Evaluator

The **Cleanlab RAG Evaluator** component evaluates RAG and LLM pipeline outputs for trustworthiness, context sufficiency, response groundedness, helpfulness, and query ease using [Cleanlab's evaluation metrics](https://help.cleanlab.ai/tlm/use-cases/tlm_rag/).

You can pair this component with the [**Cleanlab Remediator** component](#cleanlab-remediator) to remediate low-trust responses coming from the RAG pipeline.

### Cleanlab RAG Evaluator parameters

<PartialParams />

| Name                        | Type       | Description |
|-----------------------------|------------|------------|
| cleanlab_api_key           | Secret     | Input parameter. Your Cleanlab API key.    |
| cleanlab_evaluation_model  | Dropdown   | Input parameter. The evaluation model used by Cleanlab, such as GPT-4, or Claude. This doesn't need to be the same model that generated the response. |
| quality_preset             | Dropdown   | Input parameter. The tradeoff between evaluation speed and accuracy.  |
| context                    | Message    | Input parameter. The retrieved context from your RAG system.   |
| query                      | Message    | Input parameter. The original user query.   |
| response                   | Message    | Input parameter. The model's response based on the context and query. |
| run_context_sufficiency    | Boolean      | Input parameter. Evaluate whether context supports answering the query.  |
| run_response_groundedness  | Boolean      | Input parameter. Evaluate whether the response is grounded in the context. |
| run_response_helpfulness   | Boolean      | Input parameter. Evaluate how helpful the response is.  |
| run_query_ease            | Boolean      | Input parameter. Evaluate if the query is vague, complex, or adversarial. |

### Cleanlab RAG Evaluator outputs

The **Cleanlab RAG Evaluator** component has the following output options:

| Name               | Type       | Description              |
|--------------------|------------|--------------------------|
| trust_score        | Number     | The overall trust score. |
| trust_explanation  | Message    | The explanation for the trust score. |
| other_scores       | Dictionary | A dictionary of optional enabled RAG evaluation metrics. |
| evaluation_summary | Message    | A Markdown summary of query, context, response, and evaluation results. |
| response           | Message    | Returns the original response for easy chaining to the **Cleanlab Remediator** component. |

## Example Cleanlab flows

The following example flows show how to use the **Cleanlab Evaluator** and **Cleanlab Remediator** components to evaluate and remediate responses from any LLM, and how to use the **Cleanlab RAG Evaluator** component to evaluate RAG pipeline outputs.

### Evaluate and remediate responses from an LLM

This flow evaluates and remediates the trustworthiness of a response from any LLM using the **Cleanlab Evaluator** and **Cleanlab Remediator** components.

You can [download the Evaluate and Remediate flow](/files/eval_and_remediate_cleanlab.json), and then [import the flow](/concepts-flows-import) to your Langflow instance.
Or, you can build the flow from scratch by connecting the following components:

* Connect the `Message` output from any **Language Model** or **Agent** component to the **Response** input of the **Cleanlab Evaluator** component.
* Connect a **Prompt Template** component to the **Cleanlab Evaluator** component's **Prompt** input.

<!-- Components missing per image -->

![Evaluate response trustworthiness](/img/eval_response.png)

When you run the flow, the **Cleanlab Evaluator** component returns a trust score and explanation from the flow.

The **Cleanlab Remediator** component uses this trust score to determine whether to output the original response, warn about it, or replace it with a fallback answer.

This example shows a response that was determined to be untrustworthy (a score of `.09`) and flagged with a warning by the **Cleanlab Remediator** component.

![Cleanlab Remediator Example](/img/cleanlab_remediator_example.png)

To hide untrustworthy responses, configure the **Cleanlab Remediator** component to replace the response with a fallback message.

![Cleanlab Remediator Example](/img/cleanlab_remediator_example_fallback.png)

### Evaluate RAG pipeline

As an example, create a flow based on the **Vector Store RAG** template, and then add the **Cleanlab RAG Evaluator** component to evaluate the flow's context, query, and response.
Connect the **context**, **query**, and **response** outputs from the other components in the RAG flow to the **Cleanlab RAG Evaluator** component.

![Evaluate RAG pipeline](/img/eval_rag.png)

Here is an example of the `Evaluation Summary` output from the **Cleanlab RAG Evaluator** component:

![Evaluate RAG pipeline](/img/eval_summary_rag.png)

The `Evaluation Summary` includes the query, context, response, and all evaluation results. In this example, the `Context Sufficiency` and `Response Groundedness` scores are low (a score of `0.002`) because the context doesn't contain information about the query, and the response isn't grounded in the context.


================================================================================
filename: Components/bundles-clickhouse.mdx
================================================================================

---
title: ClickHouse
slug: /bundles-clickhouse
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';
import PartialConditionalParams from '@site/docs/_partial-conditional-params.mdx';
import PartialVectorSearchResults from '@site/docs/_partial-vector-search-results.mdx';
import PartialVectorStoreInstance from '@site/docs/_partial-vector-store-instance.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **ClickHouse** bundle.

## ClickHouse vector store

The **ClickHouse** component reads and writes to a ClickHouse vector store using an instance of `ClickHouse` vector store.

<details>
<summary>About vector store instances</summary>

<PartialVectorStoreInstance />

</details>

<PartialVectorSearchResults />

:::tip
For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).
:::

### ClickHouse parameters

You can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.

<PartialParams />

<PartialConditionalParams />

For information about accepted values and functionality, see the [ClickHouse Documentation](https://clickhouse.com/docs/en/intro) or inspect [component code](/concepts-components#component-code).

| Name | Display Name | Info |
|------|--------------|------|
| host | hostname | Input parameter. The ClickHouse server hostname. Required. Default: `localhost`. |
| port | port | Input parameter. The ClickHouse server port. Required. Default: `8123`. |
| database | database | Input parameter. The ClickHouse database name. Required. |
| table | Table name | Input parameter. The ClickHouse table name. Required. |
| username | Username | Input parameter. ClickHouse username for authentication. Required. |
| password | Password | Input parameter. ClickHouse password for authentication. Required. |
| index_type | index_type | Input parameter. Type of the index, either `annoy` (default) or `vector_similarity`. |
| metric | metric | Input parameter. Metric to compute distance for similarity search. The options are `angular` (default), `euclidean`, `manhattan`, `hamming`, `dot`. |
| secure | Use HTTPS/TLS | Input parameter. If `true`, enables HTTPS/TLS for the ClickHouse server and overrides inferred values for interface or port arguments. Default: `false`. |
| index_param | Param of the index | Input parameter. Index parameters. Default: `100,'L2Distance'`. |
| index_query_params | index query params | Input parameter. Additional index query parameters. |
| search_query | Search Query | Input parameter. The query string for similarity search. Only relevant for reads. |
| ingest_data | Ingest Data | Input parameter. The records to load into the vector store. |
| cache_vector_store | Cache Vector Store | Input parameter. If `true`, the component caches the vector store in memory for faster reads. Default: Enabled (`true`). |
| embedding | Embedding | Input parameter. The embedding model to use. |
| number_of_results | Number of Results | Input parameter. The number of search results to return. Default: `4`. Only relevant for reads. |
| score_threshold | Score threshold | Input parameter. The threshold for similarity score comparison. Default: Unset (no threshold). Only relevant for reads. |


================================================================================
filename: Components/bundles-cloudflare.mdx
================================================================================

---
title: Cloudflare
slug: /bundles-cloudflare
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Cloudflare** bundle.

## Cloudflare Workers AI Embeddings

The **Cloudflare Workers AI Embeddings** component generates embeddings using [Cloudflare Workers AI models](https://developers.cloudflare.com/workers-ai/).

For more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).

### Cloudflare Workers AI Embeddings parameters

<PartialParams />

| Name | Display Name | Info |
|------|--------------|------|
| account_id | Cloudflare account ID | Input parameter. Your [Cloudflare account ID](https://developers.cloudflare.com/fundamentals/setup/find-account-and-zone-ids/#find-account-id-workers-and-pages). |
| api_token | Cloudflare API token | Input parameter. Your [Cloudflare API token](https://developers.cloudflare.com/fundamentals/api/get-started/create-token/). |
| model_name | Model Name | Input parameter. A [supported model](https://developers.cloudflare.com/workers-ai/models/#text-embeddings) for embedding generation. |
| strip_new_lines | Strip New Lines | Input parameter. Whether to strip new lines from the input text. |
| batch_size | Batch Size | Input parameter. The number of texts to embed in each batch. |
| api_base_url | Cloudflare API base URL | Input parameter. The base URL for the Cloudflare API. |
| headers | Headers | Input parameter. Additional headers for the embedding generation API request. |


================================================================================
filename: Components/bundles-cohere.mdx
================================================================================

---
title: Cohere
slug: /bundles-cohere
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Cohere** bundle.

For more information about Cohere features and functionality used by Cohere components, see the [Cohere documentation](https://cohere.ai/).

## Cohere text generation

This component generates text using Cohere's language models.

It can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).

Use the **Language Model** output when you want to use a Cohere model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.

For more information, see [Language model components](/components-models).

### Cohere text generation parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| Input | String | Input parameter. Specifies the input text for text generation. |
| System Message | String | Input parameter. A [system message](https://docs.cohere.com/docs/system-instructions) to pass to the model. |
| Stream | Boolean | Input parameter. Whether to stream the response. Only works in chat. Default: `false`. |
| Cohere API Key | SecretString | Input parameter. Your Cohere API key. |
| Temperature | Float | Input parameter. Specifies the randomness of sampling. Lower values (near 0) are more deterministic, and higher values (near 1) are more creative. Defaults to `0.75`. |

## Cohere Embeddings

The **Cohere Embeddings** component is used to load embedding models from Cohere.

For more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).

### Cohere Embeddings parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| cohere_api_key | SecretString | Input parameter. The API key required to authenticate with the Cohere service. |
| model | String | Input parameter. The language model used for embedding text documents and performing queries. Default: `embed-english-v2.0` |
| truncate | Boolean | Input parameter. How to handle input that exceeds the model's token limit. One of `NONE`, `START`, or `END` (default). For more information, see the [Cohere `truncate` API reference](https://docs.cohere.com/reference/embed#request.body.truncate). |
| max_retries | Integer | Input parameter. The maximum number of retry attempts for failed requests. Default: `3` |
| user_agent | String | Input parameter. A user agent string to include in requests. Default: `langchain`|
| request_timeout | Float | Input parameter. The timeout duration for requests in seconds. Default: None |

## Cohere Rerank

This component finds and reranks documents using the Cohere API.

Outputs `Data` containing the reranked documents, limited by the **Top N** parameter.

### Cohere Rerank parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| **Search Query** | String | Input parameter. The search query for reranking documents. |
| **Search Results** | Data | Input parameter. Connect search results output from a vector store component. Use this parameter to apply reranking after running a similarity search on your vector database. |
| **Top N** | Integer | Input parameter. The number of documents to return after reranking. Default: `3`. |
| **Cohere API Key** | SecretString | Input parameter. Your Cohere API key. |
| **Model** | String | Input parameter. The re-ranker model to use. Default: `rerank-english-v3.0` |


================================================================================
filename: Components/bundles-composio.mdx
================================================================================

---
title: Composio
slug: /bundles-composio
---

import Icon from "@site/src/components/icon";

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Composio** bundle.

For information about specific Composio functionality, see the [Composio documentation](https://docs.composio.dev/introduction/intro/overview).

## Use Composio components in a flow

Composio components are primarily used as [tools for agents](/agents-tools).

The **Composio** bundle includes an aggregate **Composio Tools** component and the following single-service components:

- **Dropbox**
- **GitHub**
- **Gmail**
- **Google Calendar**
- **Google Meet**
- **Google Tasks**
- **Linear**
- **Outlook**
- **Reddit**
- **Slack** (your Slack account)
- **Slackbot** (bot integration)
- **Supabase**
- **Todoist**
- **Youtube**

The **Composio Tools** component is an access point for multiple Composio services (tools).
However, most of these services are also available as single-service components, which are recommended over the **Composio Tools** component.

Although you can use single-service components for non-agentic actions in your flows, they are primarily used in **Tool Mode** with **Agent** components.
In contrast, the **Composio Tools** component can _only_ be a tool for agents; it doesn't support non-agentic use.

The following example demonstrates how to use the Composio **Gmail** component as a tool for an **Agent** component.
This allows the agent to use Composio Gmail functionality, if necessary, when generating responses.
You can connect other Composio components in the same way.

1. Create a flow based on the **Simple Agent** template.

2. In <Icon name="Blocks" aria-hidden="true" /> **Bundles**, find the **Composio** bundle, and then add the **Gmail** component to the flow.

3. In the **Composio API Key** field, enter your Composio API key or use the `COMPOSIO_API_KEY` global variable.
For more information, see [Composio authentication](#composio-authentication).

   If the key is valid, the <Icon name="TriangleAlert" aria-hidden="true" /> **Alert** is replaced by a <Icon name="Check" aria-hidden="true" /> **Success** indicator, and the **Actions** list populates with actions available to your API key.

4. In the [component's header menu](/concepts-components#component-menus), enable **Tool Mode**.

    If you are using the **Composio Tools** component, skip this step because the component is already configured as a tool.

5. In the **Actions** list, configure the Gmail actions that you want to provide to the agent.
You can select the actions you want to allow, and you can edit each action's slug (agentic label) and description, which help the agent decide which tools to use.

6. Connect the **Gmail** component's **Toolset** output to the **Agent** component's **Tools** input.

7. In the **Agent** component, enter your OpenAI API key or configure the **Agent** component to use a different LLM.
For more information, see [Language model components](/components-models).

    At this point, your flow has four connected components:

    * The **Chat Input** component is connected to the **Agent** component's **Input** port.
    This allows the flow to be triggered by an incoming prompt from a user or application.
    * The **Gmail** component is connected to the **Agent** component as a tool.
    The agent may not use this tool for every request; the agent only uses this connection if it decides the Gmail tool can help respond to the prompt.
    * The **Agent** component's **Output** port is connected to the **Chat Output** component, which returns the final response to the user or application.

    ![Simple Agent flow with a Composio Tools component](/img/composio-create-flow.png)

8. To test the flow, click **Playground**, and then ask the LLM about the tools available to it.
The agent should provide a list of connected tools, including the **Gmail** tool and the built-in tools in the **Agent** component.
For example:

    ```text
    User:
    What tools are available to you?

    I have access to a variety of tools that allow me to help you with different tasks. Here are the main categories of tools available to me:

    1. Gmail Tools:
       - Fetch emails, search, and filter messages.
       - Fetch specific email details by message ID.
       - Create email drafts (with attachments, HTML, CC/BCC, etc.).
       - Delete email drafts or specific messages.

    2. Date & Time Tools:
       - Get the current date and time in any timezone.

    3. Multi-Tool Use:
       - Run multiple tools in parallel for efficiency.

    If you have a specific task in mind, let me know and I can tell you which tool I would use or demonstrate how I can help!
    ```

9. To test a specific tool or function, tell the agent to perform an action that uses that tool.
For example, ask the agent to write a draft email:

    ```text
    Create a draft email with the subject line "Greetings from Composio"
    recipient: "your.email@address.com"
    Body content: "Hello from composio!"
    ```

    The **Playground** prints the logic as the agent chooses the `GMAIL_CREATE_EMAIL_DRAFT` tool to create the email draft.
    The following example response is abbreviated:

    ```text
    The draft email with the subject "Greetings from Composio" and body "Hello from composio!" has been successfully created.
    ```

    ```json
    {
      "recipient_email": "your.email@address.com",
      "subject": "Greetings from Composio",
      "body": "Hello from composio!",
      "is_html": false
    }

    {
      "data": {
        "response_data": {
          "id": "r-237981011463568567",
          "message": {
            "id": "195dd80528171132",
            "threadId": "195dd80528171132",
            "labelIds": [
              "DRAFT"
            ]
          }
        }
      },
      "error": null,
      "successfull": true,
      "successful": true
    }
    ```

    For further confirmation, you can go to your Gmail account and find the message in your drafts folder.

10. Optional: To add more Composio services, repeat these steps to add more Composio components to your flow.
For each component, provide the necessary credentials, enable **Tool Mode**, configure the actions, and then connect it to the **Agent** component's **Tools** port.

## Composio parameters

All single-service Composio components have the same parameters, and the **Composio Tools** component has one additional parameter:

| Name | Type | Description |
|------|------|-------------|
| entity_id | String | Input parameter. The entity ID for the Composio account. Default: `default`. This parameter is hidden by default in the visual editor. If you need to set this parameter, you can access it through the <Icon name="SlidersHorizontal" aria-hidden="true"/> **Controls** in the [component's header menu](/concepts-components#component-menus). |
| api_key | SecretString | Input parameter. The Composio API key for authentication with the Composio platform. Make sure the key authorizes the specific service that you want to use. For more information, see [Composio authentication](#composio-authentication). |
| tool_name | Connection | Input parameter for the **Composio Tools** component only. Select the Composio service (tool) to connect to. |
| action | List | Input parameter. Select actions to use. Available actions vary by service. Some actions might require premium access to a particular service. |

## Composio authentication

Composio components require authentication to the Composio platform with a Composio API key.

You can provide this key directly in your components, or you can use the `COMPOSIO_API_KEY` [global variable](/configuration-global-variables), which Langflow can automatically load from your `.env` file.

:::info
The Composio API key _only_ handles the connection to Composio.
Service provider authentication is managed through the Composio platform for each service that you want to use.
:::

Make sure that your Composio API key provides access to the required services for the components in your flow.
For example, if you are using the Composio **Gmail** component, your Composio API key must have access to the Gmail service.

## Composio output

When used as tools for an agent, Composio components output [`Tools`](/data-types#tool), which is a list of tools for use by an agent.
When called by the agent, the response from the Composio service is ingested by the agent, not passed directly as output to the user or application.

In non-agentic use cases, the output is a [`DataFrame`](/data-types#dataframe) containing the response from the specified Composio service, depending on the component and action used in the flow.

Because the **Composio Tools** component supports _only_ agentic use, it cannot output `DataFrame`.
All single-service Composio components can output either `DataFrame` or `Tools`.


================================================================================
filename: Components/bundles-couchbase.mdx
================================================================================

---
title: Couchbase
slug: /bundles-couchbase
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';
import PartialConditionalParams from '@site/docs/_partial-conditional-params.mdx';
import PartialVectorSearchResults from '@site/docs/_partial-vector-search-results.mdx';
import PartialVectorStoreInstance from '@site/docs/_partial-vector-store-instance.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Couchbase** bundle.

## Couchbase vector store

The **Couchbase** component reads and writes to a Couchbase vector store using an instance of `CouchbaseSearchVectorStore`.

<details>
<summary>About vector store instances</summary>

<PartialVectorStoreInstance />

</details>

<PartialVectorSearchResults />

:::tip
For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).
:::

### Couchbase parameters

You can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.

<PartialParams />

<PartialConditionalParams />

For information about accepted values and functionality, see the [Couchbase documentation](https://docs.couchbase.com/home/index.html) or inspect [component code](/concepts-components#component-code).

| Name | Type | Description |
|------|------|-------------|
| couchbase_connection_string | SecretString | Input parameter. Couchbase Cluster connection string. Required. |
| couchbase_username | String | Input parameter. Couchbase username for authentication. Required. |
| couchbase_password | SecretString | Input parameter. Couchbase password for authentication. Required. |
| bucket_name | String | Input parameter. Name of the Couchbase bucket. Required. |
| scope_name | String | Input parameter. Name of the Couchbase scope. Required. |
| collection_name | String | Input parameter. Name of the Couchbase collection. Required. |
| index_name | String | Input parameter. Name of the Couchbase index. Required. |
| ingest_data | Data | Input parameter. The records to load into the vector store. Only relevant for writes. |
| search_query | String | Input parameter. The query string for vector search. Only relevant for reads. |
| cache_vector_store | Boolean | Input parameter. If `true`, the component caches the vector store in memory for faster reads. Default: Enabled (`true`). |
| embedding | Embeddings | Input parameter. The embedding function to use for the vector store. |
| number_of_results | Integer | Input parameter. Maximum number of search results to return. Default: 4. Only relevant for reads. |


================================================================================
filename: Components/bundles-datastax.mdx
================================================================================

---
title: DataStax
slug: /bundles-datastax
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';
import PartialConditionalParams from '@site/docs/_partial-conditional-params.mdx';
import PartialVectorSearchResults from '@site/docs/_partial-vector-search-results.mdx';
import PartialVectorStoreInstance from '@site/docs/_partial-vector-store-instance.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **DataStax** bundle, including components that read and write to Astra DB databases.

## Astra DB

:::warning
It is recommended that you create any databases, keyspaces, and collections you need before configuring the **Astra DB** component.

You can create new databases and collections through this component, but this is only possible in the Langflow visual editor (not at runtime), and you must wait while the database or collection initializes before proceeding with flow configuration.
Additionally, not all database and collection configuration options are available through the **Astra DB** component, such as hybrid search options, PCU groups, vectorize integration management, and multi-region deployments.
:::

The **Astra DB** component reads and writes to Astra DB Serverless databases, using an instance of `AstraDBVectorStore` to call the Data API and DevOps API.

<details>
<summary>About vector store instances</summary>

<PartialVectorStoreInstance />

</details>

### Astra DB parameters

You can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.

<PartialParams />

<PartialConditionalParams />

For information about accepted values and functionality, see the [Astra DB Serverless documentation](https://docs.datastax.com/en/astra-db-serverless/index.html) or inspect [component code](/concepts-components#component-code).

| Name | Display Name | Info |
|------|--------------|------|
| token | Astra DB Application Token | Input parameter. An Astra application token with permission to access your vector database. Once the connection is verified, additional fields are populated with your existing databases and collections. If you want to create a database through this component, the application token must have Organization Administrator permissions. |
| environment | Environment | Input parameter. The environment for the Astra DB API endpoint. Typically always `prod`. |
| database_name | Database | Input parameter. The name of the database that you want this component to connect to. Or, you can select **New Database** to create a new database, and then wait for the database to initialize before setting the remaining parameters. |
| endpoint | Astra DB API Endpoint | Input parameter. For multi-region databases, select the API endpoint for your nearest datacenter. To get the list of regions for a multi-region database, see [List database regions](https://docs.datastax.com/en/astra-db-serverless/databases/manage-regions.html#list-db-regions). This field is automatically populated when you select a database, and it defaults to the primary region's endpoint. |
| keyspace | Keyspace | Input parameter. The keyspace in your database that contains the collection specified in `collection_name`. Default: `default_keyspace`. |
| collection_name | Collection | Input parameter. The name of the collection that you want to use with this flow. Or, select **New Collection** to create a new collection with limited configuration options. To ensure your collection is configured with the correct embedding provider and search capabilities, it is recommended to create the collection in the Astra Portal or with the Data API *before* configuring this component. For more information, see [Manage collections in Astra DB Serverless](https://docs.datastax.com/en/astra-db-serverless/databases/manage-collections.html). |
| embedding_model | Embedding Model | Input parameter. Attach an [embedding model component](/components-embedding-models) to generate embeddings. Only available if the specified collection doesn't have a [vectorize integration](https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html). If a vectorize integration exists, the component automatically uses the collection's integrated model. |
| ingest_data | Ingest Data | Input parameter. The documents to load into the specified collection. Accepts `Data` or `DataFrame` input. |
| search_query | Search Query | Input parameter. The query string for vector search. |
| cache_vector_store | Cache Vector Store | Input parameter. Whether to cache the vector store in Langflow memory for faster reads. Default: Enabled (`true`). |
| search_method | Search Method | Input parameter. The search methods to use, either `Hybrid Search` or `Vector Search`. Your collection must be configured to support the chosen option, and the default depends on what your collection supports. All vector-enabled collections in Astra DB Serverless (Vector) databases support vector search, but hybrid search requires that you set specific collection settings when creating the collection. These options are only available when creating a collection programmatically. For more information, see [Ways to find data in Astra DB Serverless](https://docs.datastax.com/en/astra-db-serverless/databases/about-search.html) and [Create a collection that supports hybrid search](https://docs.datastax.com/en/astra-db-serverless/api-reference/collection-methods/create-collection.html#example-hybrid). |
| reranker | Reranker | Input parameter. The re-ranker model to use for hybrid search, depending on the collection configuration. This parameter is only available for collections that support hybrid search. To determine if a collection supports hybrid search, [get collection metadata](https://docs.datastax.com/en/astra-db-serverless/api-reference/collection-methods/list-collection-metadata.html), and then check that `lexical` and `rerank` both have `"enabled": true`. |
| lexical_terms | Lexical Terms | Input parameter. A space-separated string of keywords for hybrid search, like `features, data, attributes, characteristics`. This parameter is only available if the collection supports hybrid search. For more information, see the [Hybrid search example](#astra-db-examples). |
| number_of_results | Number of Search Results | Input parameter. The number of search results to return. Default: 4. |
| search_type | Search Type | Input parameter. The search type to use, either `Similarity` (default), `Similarity with score threshold`, and `MMR (Max Marginal Relevance)`. |
| search_score_threshold | Search Score Threshold | Input parameter. The minimum similarity score threshold for vector search results with the `Similarity with score threshold` search type. Default: 0. |
| advanced_search_filter | Search Metadata Filter | Input parameter. An optional dictionary of metadata filters to apply in addition to vector or hybrid search. |
| autodetect_collection | Autodetect Collection | Input parameter. Whether to automatically fetch a list of available collections after providing an application token and API endpoint. |
| content_field | Content Field | Input parameter. For writes, this parameter specifies the name of the field in the documents that contains text strings for which you want to generate embeddings. |
| deletion_field | Deletion Based On Field | Input parameter. When provided, documents in the target collection with metadata field values matching the input metadata field value are deleted before new records are loaded. Use this setting for writes with upserts (overwrites). |
| ignore_invalid_documents | Ignore Invalid Documents | Input parameter. Whether to ignore invalid documents during writes. If disabled (`false`), then an error is raised for invalid documents. Default: Enabled (`true`). |
| astradb_vectorstore_kwargs | AstraDBVectorStore Parameters | Input parameter. An optional dictionary of additional parameters for the `AstraDBVectorStore` instance. |

### Astra DB examples

<details>
<summary>Example: Vector RAG</summary>

import PartialVectorRagFlow from '@site/docs/_partial-vector-rag-flow.mdx';

<PartialVectorRagFlow />

</details>

<details>
<summary>Example: Hybrid search</summary>

The **Astra DB** component supports the Data API's [hybrid search](https://docs.datastax.com/en/astra-db-serverless/databases/hybrid-search.html) feature.
Hybrid search performs a vector similarity search and a lexical search, compares the results of both searches, and then returns the most relevant results overall.

To use hybrid search through the **Astra DB** component, do the following:

1. Use the Data API to [create a collection that supports hybrid search](https://docs.datastax.com/en/astra-db-serverless/api-reference/collection-methods/create-collection.html#example-hybrid) if you don't already have one.

    Although you can create a collection through the **Astra DB** component, you have more control and insight into the collection settings when using the Data API for this operation.

2. Create a flow based on the **Hybrid Search RAG** template, which includes an **Astra DB** component that is pre-configured for hybrid search.

    After loading the template, check for **Upgrade available** alerts on the components.
    If any components have an upgrade pending, upgrade and reconnect them before continuing.

3. In the **Language Model** components, add your OpenAI API key.
If you want to use a different provider or model, see [Language model components](/components-models).

4. Delete the **Language Model** component that is connected to the **Structured Output** component's **Input Message** port, and then connect the **Chat Input** component to that port.

5. Configure the **Astra DB** vector store component:

    1. Enter your Astra DB application token.
    2. In the **Database** field, select your database.
    3. In the **Collection** field, select your collection with hybrid search enabled.

    Once you select a collection that supports hybrid search, the other parameters automatically update to allow hybrid search options.

6. Connect the first **Parser** component's **Parsed Text** output to the **Astra DB** component's **Lexical Terms** input.
This input only appears after connecting a collection that support hybrid search with reranking.

7. Update the **Structured Output** template:

    1. Click the **Structured Output** component to expose the [component's header menu](/concepts-components#component-menus), and then click <Icon name="SlidersHorizontal" aria-hidden="true"/> **Controls**.
    2. Find the **Format Instructions** row, click <Icon name="Expand" aria-hidden="true"/> **Expand**, and then replace the prompt with the following text:

        ```text
        You are a database query planner that takes a user's requests, and then converts to a search against the subject matter in question.
        You should convert the query into:
        1. A list of keywords to use against a Lucene text analyzer index, no more than 4. Strictly unigrams.
        2. A question to use as the basis for a QA embedding engine.
        Avoid common keywords associated with the user's subject matter.
        ```

    3. Click **Finish Editing**, and then click **Close** to save your changes to the component.

8. Open the **Playground**, and then enter a natural language question that you would ask about your database.

    In this example, your input is sent to both the **Astra DB** and **Structured Output** components:

    * The input sent directly to the **Astra DB** component's **Search Query** port is used as a string for similarity search.
    An embedding is generated from the query string using the collection's Astra DB vectorize integration.

    * The input sent to the **Structured Output** component is processed by the **Structured Output**, **Language Model**, and **Parser** components to extract space-separated `keywords` used for the lexical search portion of the hybrid search.

    The complete hybrid search query is executed against your database using the Data API's `find_and_rerank` command.
    The API's response is output as a `DataFrame` that is transformed into a text string `Message` by another **Parser** component.
    Finally, the **Chat Output** component prints the `Message` response to the **Playground**.

9. Optional: Exit the **Playground**, and then click <Icon name="TextSearch" aria-hidden="true"/> **Inspect Output** on each individual component to understand how lexical keywords were constructed and view the raw response from the Data API.
This is helpful for debugging flows where a certain component isn't receiving input as expected from another component.

    * **Structured Output component**: The output is the `Data` object produced by applying the output schema to the LLM's response to the input message and format instructions.
    The following example is based on the aforementioned instructions for keyword extraction:

        ```
        1. Keywords: features, data, attributes, characteristics
        2. Question: What characteristics can be identified in my data?
        ```

    * **Parser component**: The output is the string of keywords extracted from the structured output `Data`, and then used as lexical terms for the hybrid search.

    * **Astra DB component**: The output is the `DataFrame` containing the results of the hybrid search as returned by the Data API.

</details>

### Astra DB output

<PartialVectorSearchResults />

<details>
<summary>Vector Store Connection port</summary>

The **Astra DB** component has an additional **Vector Store Connection** output.
This output can only connect to a `VectorStore` input port, and it was intended for use with dedicated Graph RAG components.

The only non-legacy component that supports this input is the [**Graph RAG** component](#graph-rag), which can be a Graph RAG extension to the **Astra DB** component.
Instead, use the **Astra DB Graph** component that includes both the vector store connection and Graph RAG functionality.

</details>

## Astra DB CQL

The **Astra DB CQL** component allows agents to query data from CQL tables in Astra DB.

The output is a list of [`Data`](/data-types#data) objects containing the query results from the Astra DB CQL table. Each Data object contains the document fields specified by the projection fields. Limited by the `number_of_results` parameter.

### Astra DB CQL parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| Tool Name | String | Input parameter. The name used to reference the tool in the agent's prompt. |
| Tool Description | String | Input parameter. A brief description of the tool to guide the model in using it. |
| Keyspace | String | Input parameter. The name of the keyspace. |
| Table Name | String | Input parameter. The name of the Astra DB CQL table to query. |
| Token | SecretString | Input parameter. The authentication token for Astra DB. |
| API Endpoint | String | Input parameter. The Astra DB API endpoint. |
| Projection Fields | String | Input parameter. The attributes to return, separated by commas. Default: "*". |
| Partition Keys | Dict | Input parameter. Required parameters that the model must fill to query the tool. |
| Clustering Keys | Dict | Input parameter. Optional parameters the model can fill to refine the query. Required parameters should be marked with an exclamation mark, for example, `!customer_id`. |
| Static Filters | Dict | Input parameter. Attribute-value pairs used to filter query results. |
| Limit | String | Input parameter. The number of records to return. |

## Astra DB Tool

The **Astra DB Tool** component enables searching data in Astra DB collections, including hybrid search, vector search, and regular filter-based search.
Specialized searches require that the collection is pre-configured with the required parameters.

Outputs a list of [`Data`](/data-types#data) objects containing the query results from Astra DB. Each `Data` object contains the document fields specified by the projection attributes. Limited by the `number_of_results` parameter and the upper limit of the Astra DB Data API, depending on the type of search.

You can use the component to execute queries directly as isolated steps in a flow, or you can connect it as a [tool for an agent](/agents-tools) to allow the agent to query data from Astra DB collections as needed to respond to user queries.
For more information, see [Use Langflow agents](/agents).

![Astra DB Tool component connected as a tool to an Agent component](/img/component-astra-db-json-tool.png)

### Astra DB Tool parameters

The following parameters are for the **Astra DB Tool** component overall.

The values for **Collection Name**, **Astra DB Application Token**, and **Astra DB API Endpoint** are found in your Astra DB deployment. For more information, see the [Astra DB Serverless documentation](https://docs.datastax.com/en/astra-db-serverless/databases/create-database.html).

| Name              | Type   | Description   |
|-------------------|--------|--------|
| Tool Name         | String | Input parameter. The name used to reference the tool in the agent's prompt.  |
| Tool Description  | String | Input parameter. A brief description of the tool. This helps the model decide when to use it. |
| Keyspace Name     | String | Input parameter. The name of the keyspace in Astra DB. Default: `default_keyspace` |
| Collection Name   | String | Input parameter. The name of the Astra DB collection to query.    |
| Token             | SecretString | Input parameter. The authentication token for accessing Astra DB.  |
| API Endpoint      | String | Input parameter. The Astra DB API endpoint.  |
| Projection Fields | String | Input parameter. Comma-separated list of attributes to return from matching documents. The default is the default projection, `*`, which returns all attributes except reserved fields like `$vector`.  |
| Tool Parameters  | Dict   | Input parameter. [Astra DB Data API `find` filters](https://docs.datastax.com/en/astra-db-serverless/api-reference/document-methods/find-many.html#parameters) that become tools for an agent. These Filters _may_ be used in a search, if the agent selects them. See [Define tool-specific parameters](#define-tool-specific-parameters). |
| Static Filters    | Dict   | Input parameter. Attribute-value pairs used to filter query results. Equivalent to [Astra DB Data API `find` filters](https://docs.datastax.com/en/astra-db-serverless/api-reference/document-methods/find-many.html#parameters). **Static Filters** are included with _every_ query. Use **Static Filters** without semantic search to perform a regular filter search. |
| Number of Results  | Int | Input parameter. The maximum number of documents to return.   |
| Semantic Search | Boolean | Input parameter. Whether to run a similarity search by generating a vector embedding from the chat input and following the **Semantic Search Instruction**. Default: `false`. If `true`, you must attach an [embedding model component](/components-embedding-models) or have vectorize pre-enabled on your collection. |
| Use Astra DB Vectorize | Boolean | Input parameter. Whether to use the Astra DB vectorize feature for embedding generation when running a semantic search. Default: `false`. If `true`, you must have vectorize pre-enabled on your collection. |
| Embedding Model | Embedding | Input parameter. A port to attach an embedding model component to generate a vector from input text for semantic search. This can be used when **Semantic Search** is `true`, with or without vectorize. Be sure to use a model that aligns with the dimensions of the embeddings already present in the collection. |
| Semantic Search Instruction | String | Input parameter. The query to use for similarity search. Default: `"Find documents similar to the query."`. This instruction is used to guide the model in performing semantic search. |

### Define tool-specific parameters

:::tip
**Tool Parameters** are small functions that you create within the **Astra DB Tool** component.
They give the LLM pre-defined ways to interact with the data in your collection.

Without these filters, the LLM has no concept of the data in your collection or which attributes are important.

At runtime, the LLM can decide which filters are relevant to the current query.

Filters in **Tool Parameters** aren't always applied.
If you want to enforce filters for _every_ query, use the **Static Filters** parameter.
You can use both **Tool Parameters** and **Static Filters** to set some required filters and some optional filters.
:::

In the **Astra DB Tool** component's **Tool Parameters** field, you can create filters to query documents in your collection.

When used in **Tool Mode** with an agent, these filters tell the agent which document attributes are most important, which are required in searches, and which operators to use on certain attributes.
The filters become available as parameters that the LLM can use when calling the tool, with a better understanding of each parameter provided by the **Description** field.

In the **Tool Parameters** pane, click <Icon name="Plus" aria-hidden="true"/> **Add a new row**, and then edit each cell in the row.
For example, the following filter allows an LLM to filter by unique `customer_id` values:

    * Name: `customer_id`
    * Attribute Name: Leave empty if the attribute matches the field name in the database.
    * Description: `"The unique identifier of the customer to filter by"`.
    * Is Metadata: Select **False** unless the value is stored in the metadata field.
    * Is Mandatory: Set to **True** to make the filter required.
    * Is Timestamp: For this example, select **False** because the value is an ID, not a timestamp.
    * Operator: `$eq` to look for an exact match.

The following fields are available for each row in the **Tool Parameters** pane:

| Parameter | Description |
|-----------|-------------|
| Name | The name of the parameter that is exposed to the LLM. It can be the same as the underlying field name or a more descriptive label. The LLM uses this name, along with the description, to infer what value to provide during execution. |
| Attribute Name | When the parameter name shown to the LLM differs from the actual field or property in the database, use this setting to map the user-facing name to the correct attribute. For example, to apply a range filter to the timestamp field, define two separate parameters, such as `start_date` and `end_date`, that both reference the same timestamp attribute. |
| Description | Provides instructions to the LLM on how the parameter should be used. Clear and specific guidance helps the LLM provide valid input. For example, if a field such as `specialty` is stored in lowercase, the description should indicate that the input must be lowercase. |
| Is Metadata | When loading data using LangChain or Langflow, additional attributes may be stored under a metadata object. If the target attribute is stored this way, enable this option. It adjusts the query by generating a filter in the format: `{"metadata.<attribute_name>": "<value>"}` |
| Is Timestamp | For date or time-based filters, enable this option to automatically convert values to the timestamp format that the Astrapy client expects. This ensures compatibility with the underlying API without requiring manual formatting. |
| Operator | Defines the filtering logic applied to the attribute. You can use any valid [Data API filter operator](https://docs.datastax.com/en/astra-db-serverless/api-reference/filter-operator-collections.html). For example, to filter a time range on the timestamp attribute, use two parameters: one with the `$gt` operator for "greater than", and another with the `$lt` operator for "less than". |

## Astra DB Graph

The **Astra DB Graph** component uses `AstraDBGraphVectorStore`, an instance of [LangChain graph vector store](https://python.langchain.com/api_reference/community/graph_vectorstores.html), for graph traversal and graph-based document retrieval in an Astra DB collection. It also supports writing to the vector store.
For more information, see [Build a Graph RAG system with LangChain and GraphRetriever](https://docs.datastax.com/en/astra-db-serverless/tutorials/graph-rag.html).

<PartialVectorSearchResults />

### Astra DB Graph parameters

You can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.

<PartialParams />

<PartialConditionalParams />

For information about accepted values and functionality, see the [Astra DB Serverless documentation](https://docs.datastax.com/en/astra-db-serverless/index.html) or inspect [component code](/concepts-components#component-code).

| Name | Display Name | Info |
|------|--------------|------|
| token | Astra DB Application Token | Input parameter. An Astra application token with permission to access your vector database. Once the connection is verified, additional fields are populated with your existing databases and collections. If you want to create a database through this component, the application token must have Organization Administrator permissions. |
| api_endpoint | API Endpoint | Input parameter. Your database's API endpoint. |
| keyspace | Keyspace | Input parameter. The keyspace in your database that contains the collection specified in `collection_name`. Default: `default_keyspace`. |
| collection_name | Collection | Input parameter. The name of the collection that you want to use with this flow. For write operations, if a matching collection doesn't exist, a new one is created. |
| metadata_incoming_links_key | Metadata Incoming Links Key | Input parameter. The metadata key for the incoming links in the vector store. |
| ingest_data | Ingest Data | Input parameter. Records to load into the vector store. Only relevant for writes. |
| search_input | Search Query | Input parameter. Query string for similarity search. Only relevant for reads. |
| cache_vector_store | Cache Vector Store | Input parameter. Whether to cache the vector store in Langflow memory for faster reads. Default: Enabled (`true`). |
| embedding_model | Embedding Model | Input parameter. Attach an [embedding model component](/components-embedding-models) to generate embeddings. If the collection has a [vectorize integration](https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html), don't attach an embedding model component. |
| metric | Metric | Input parameter. The metrics to use for similarity search calculations, either `cosine` (default), `dot_product`, or `euclidean`. This is a collection setting. |
| batch_size | Batch Size | Input parameter. Optional number of records to process in a single batch. |
| bulk_insert_batch_concurrency | Bulk Insert Batch Concurrency | Input parameter. Optional concurrency level for bulk write operations. |
| bulk_insert_overwrite_concurrency | Bulk Insert Overwrite Concurrency | Input parameter. Optional concurrency level for bulk write operations that allow upserts (overwriting existing records). |
| bulk_delete_concurrency | Bulk Delete Concurrency | Input parameter. Optional concurrency level for bulk delete operations. |
| setup_mode | Setup Mode | Input parameter. Configuration mode for setting up the vector store, either `Sync` (default) or `Off`. |
| pre_delete_collection | Pre Delete Collection | Input parameter. Whether to delete the collection before creating a new one. Default: Disabled (`false`). |
| metadata_indexing_include | Metadata Indexing Include | Input parameter. A list of metadata fields to index if you want to enable [selective indexing](https://docs.datastax.com/en/astra-db-serverless/api-reference/collection-indexes.html) *only* when creating a collection. Doesn't apply to existing collections. Only one `*_indexing_*` parameter can be set per collection. If all `*_indexing_*` parameters are unset, then all fields are indexed (default indexing). |
| metadata_indexing_exclude | Metadata Indexing Exclude | Input parameter. A list of metadata fields to exclude from indexing if you want to enable selective indexing *only* when creating a collection. Doesn't apply to existing collections. Only one `*_indexing_*` parameter can be set per collection. If all `*_indexing_*` parameters are unset, then all fields are indexed (default indexing). |
| collection_indexing_policy | Collection Indexing Policy | Input parameter. A dictionary to define the indexing policy if you want to enable selective indexing *only* when creating a collection. Doesn't apply to existing collections. Only one `*_indexing_*` parameter can be set per collection. If all `*_indexing_*` parameters are unset, then all fields are indexed (default indexing). The `collection_indexing_policy` dictionary is used when you need to set indexing on subfields or a complex indexing definition that isn't compatible as a list. |
| number_of_results | Number of Results | Input parameter. Number of search results to return. Default: 4. Only relevant to reads. |
| search_type | Search Type | Input parameter. Search type to use, either `Similarity`, `Similarity with score threshold`, or `MMR (Max Marginal Relevance)`, `Graph Traversal`, or `MMR (Max Marginal Relevance) Graph Traversal` (default). Only relevant to reads. |
| search_score_threshold | Search Score Threshold | Input parameter. Minimum similarity score threshold for search results if the `search_type` is `Similarity with score threshold`. Default: 0. |
| search_filter | Search Metadata Filter | Input parameter. Optional dictionary of metadata filters to apply in addition to vector search. |

## Graph RAG

The **Graph RAG** component uses an instance of [`GraphRetriever`](https://datastax.github.io/graph-rag/reference/langchain_graph_retriever/) for Graph RAG traversal enabling graph-based document retrieval in an Astra DB vector store.
For more information, see the [DataStax Graph RAG documentation](https://datastax.github.io/graph-rag/).

:::info
This component can be a Graph RAG extension for the [**Astra DB** vector store component](#astra-db).
However, the [**Astra DB Graph** component](#astra-db-graph) includes both the vector store connection and Graph RAG functionality.
:::

### Graph RAG parameters

You can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.

<PartialParams />

<PartialConditionalParams />

| Name | Display Name | Info |
|------|--------------|------|
| embedding_model | Embedding Model | Input parameter. Specify the embedding model to use. Not required if the connected vector store has a [vectorize integration](https://docs.datastax.com/en/astra-db-serverless/databases/embedding-generation.html). |
| vector_store | Vector Store Connection | Input parameter. An instance of `AstraDbVectorStore` inherited from the [**Astra DB** component](#astra-db)'s **Vector Store Connection** output. |
| edge_definition | Edge Definition | Input parameter. [Edge definition](https://datastax.github.io/graph-rag/reference/graph_retriever/edges/) for the graph traversal. |
| strategy | Traversal Strategies | Input parameter. The strategy to use for graph traversal. Strategy options are dynamically loaded from available strategies. |
| search_query | Search Query | Input parameter. The query to search for in the vector store. |
| graphrag_strategy_kwargs | Strategy Parameters | Input parameter. Optional dictionary of additional parameters for the [retrieval strategy](https://datastax.github.io/graph-rag/reference/graph_retriever/strategies/). |
| search_results | **Search Results** or **DataFrame** | Output parameter. The results of the graph-based document retrieval as a list of [`Data`](/data-types#data) objects or as a tabular [`DataFrame`](/data-types#dataframe). You can set the desired output type near the component's output port. |

## Hyper-Converged Database (HCD)

The **Hyper-Converged Database (HCD)** component uses your cluster's Data API server to read and write to your HCD vector store.
Because the underlying functions call the Data API, which originated from Astra DB, the component uses an instance of `AstraDBVectorStore`.

![A flow using the HCD component to load vector data.](/img/component-hcd-example-flow.png)

<details>
<summary>About vector store instances</summary>

<PartialVectorStoreInstance />

</details>

<PartialVectorSearchResults />

For more information about HCD, see [Get started with HCD 1.2](https://docs.datastax.com/en/hyper-converged-database/1.2/get-started/get-started-hcd.html) and [Get started with the Data API in HCD 1.2](https://docs.datastax.com/en/hyper-converged-database/1.2/api-reference/dataapiclient.html).

### HCD parameters

You can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.

<PartialParams />

<PartialConditionalParams />

| Name | Display Name | Info |
|------|--------------|------|
| collection_name | Collection Name | Input parameter. The name of a vector store collection in HCD. For write operations, if the collection doesn't exist, then a new one is created. Required. |
| username | HCD Username | Input parameter. Username for authenticating to your HCD deployment. Default: `hcd-superuser`. Required. |
| password | HCD Password | Input parameter. Password for authenticating to your HCD deployment. Required. |
| api_endpoint | HCD API Endpoint | Input parameter. Your deployment's HCD Data API endpoint, formatted as `http[s]://CLUSTER_HOST:GATEWAY_PORT` where `CLUSTER_HOST` is the IP address of any node in your cluster and `GATEWAY_PORT` is the port number for your API gateway service. For example, `http://192.0.2.250:8181`. Required. |
| ingest_data | Ingest Data | Input parameter. Records to load into the vector store. Only relevant for writes. |
| search_input | Search Input | Input parameter. Query string for similarity search. Only relevant for reads. |
| namespace | Namespace | Input parameter. The namespace in HCD that contains or will contain the collection specified in `collection_name`. Default: `default_namespace`. |
| ca_certificate | CA Certificate | Input parameter. Optional CA certificate for TLS connections to HCD. |
| metric | Metric | Input parameter. The metrics to use for similarity search calculations, either `cosine`, `dot_product`, or `euclidean`. This is a collection setting. If calling an existing collection, leave unset to use the collection's metric. If a write operation creates a new collection, specify the desired similarity metric setting. |
| batch_size | Batch Size | Input parameter. Optional number of records to process in a single batch. |
| bulk_insert_batch_concurrency | Bulk Insert Batch Concurrency | Input parameter. Optional concurrency level for bulk write operations. |
| bulk_insert_overwrite_concurrency | Bulk Insert Overwrite Concurrency | Input parameter. Optional concurrency level for bulk write operations that allow upserts (overwriting existing records). |
| bulk_delete_concurrency | Bulk Delete Concurrency | Input parameter. Optional concurrency level for bulk delete operations. |
| setup_mode | Setup Mode | Input parameter. Configuration mode for setting up the vector store, either `Sync` (default), `Async`, or `Off`. |
| pre_delete_collection | Pre Delete Collection | Input parameter. Whether to delete the collection before creating a new one. |
| metadata_indexing_include | Metadata Indexing Include | Input parameter. A list of metadata fields to index if you want to enable [selective indexing](https://docs.datastax.com/en/hyper-converged-database/1.2/api-reference/collection-indexes.html) *only* when creating a collection. Doesn't apply to existing collections. Only one `*_indexing_*` parameter can be set per collection. If all `*_indexing_*` parameters are unset, then all fields are indexed (default indexing). |
| metadata_indexing_exclude | Metadata Indexing Exclude | Input parameter. A list of metadata fields to exclude from indexing if you want to enable selective indexing *only* when creating a collection. Doesn't apply to existing collections. Only one `*_indexing_*` parameter can be set per collection. If all `*_indexing_*` parameters are unset, then all fields are indexed (default indexing). |
| collection_indexing_policy | Collection Indexing Policy | Input parameter. A dictionary to define the indexing policy if you want to enable selective indexing *only* when creating a collection. Doesn't apply to existing collections. Only one `*_indexing_*` parameter can be set per collection. If all `*_indexing_*` parameters are unset, then all fields are indexed (default indexing). The `collection_indexing_policy` dictionary is used when you need to set indexing on subfields or a complex indexing definition that isn't compatible as a list. |
| embedding | Embedding or Astra Vectorize | Input parameter. The embedding model to use by attaching an **Embedding Model** component. This component doesn't support additional vectorize authentication headers, so it isn't possible to use a vectorize integration with this component, even if you have enabled one on an existing HCD collection. |
| number_of_results | Number of Results | Input parameter. Number of search results to return. Default: 4. Only relevant to reads. |
| search_type | Search Type | Input parameter. Search type to use, either `Similarity` (default), `Similarity with score threshold`, or `MMR (Max Marginal Relevance)`. Only relevant to reads. |
| search_score_threshold | Search Score Threshold | Input parameter. Minimum similarity score threshold for search results if the `search_type` is `Similarity with score threshold`. Default: 0. |
| search_filter | Search Metadata Filter | Input parameter. Optional dictionary of metadata filters to apply in addition to vector search. |

## Other DataStax components

The following components are also included in the **DataStax** bundle.

### Astra DB Chat Memory

The **Astra DB Chat Memory** component retrieves and stores chat messages using an Astra DB database.

Chat memories are passed between memory storage components as the [`Memory`](/data-types#memory) data type.
Specifically, the component creates an instance of `AstraDBChatMessageHistory`, which is a LangChain chat message history class that uses Astra DB for storage.

:::warning
The **Astra DB Chat Memory** component isn't recommended for most memory storage because memories tend to be long JSON objects or strings, often exceeding the maximum size of a document or object supported by Astra DB.

However, Langflow's **Agent** component includes built-in chat memory that is enabled by default.
Your agentic flows don't need an external database to store chat memory.
For more information, see [Memory management options](/memory).
:::

For more information about using external chat memory in flows, see the [**Message History** component](/components-helpers#message-history).

#### Astra DB Chat Memory parameters

<PartialParams />

| Name             | Type          | Description                                                           |
|------------------|---------------|-----------------------------------------------------------------------|
| collection_name  | String        | Input parameter. The name of the Astra DB collection for storing messages. Required. |
| token            | SecretString  | Input parameter. The authentication token for Astra DB access. Required. |
| api_endpoint     | SecretString  | Input parameter. The API endpoint URL for the Astra DB service. Required. |
| namespace        | String        | Input parameter. The optional namespace within Astra DB for the collection. |
| session_id       | MessageText   | Input parameter. The unique identifier for the chat session. Uses the current session ID if not provided. |

### Assistants API

The following DataStax components are used to create and manage Assistants API functions in a flow:

* **Astra Assistant Agent**
* **Create Assistant**
* **Create Assistant Thread**
* **Get Assistant Name**
* **List Assistants**
* **Run Assistant**

## Environment variables

The following DataStax components are used to load and retrieve environment variables in a flow:

* **Dotenv**
* **Get Environment Variable**

## Legacy DataStax components

import PartialLegacy from '@site/docs/_partial-legacy.mdx';

<PartialLegacy />

The following DataStax components are in legacy status:

<details>
<summary>Astra Vectorize</summary>

This component was deprecated in Langflow version 1.1.2.
Replace it with the [**Astra DB** component](#astra-db).

The **Astra DB Vectorize** component was used to generate embeddings with Astra DB's vectorize feature in conjunction with an **Astra DB** component.

The vectorize functionality is now built into the **Astra DB** component.
You no longer need a separate component for vectorize embedding generation.

</details>

## See also

* [**Cassandra** bundle](/bundles-cassandra)
* [Create a vector RAG chatbot](/chat-with-rag)


================================================================================
filename: Components/bundles-deepseek.mdx
================================================================================

---
title: DeepSeek
slug: /bundles-deepseek
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **DeepSeek** bundle.

For more information about DeepSeek features and functionality used by DeepSeek components, see the [DeepSeek documentation](https://api-docs.deepseek.com/).

## DeepSeek text generation

The **DeepSeek** component generates text using DeepSeek's language models.

It can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).

Use the **Language Model** output when you want to use a DeepSeek model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.

For more information, see [Language model components](/components-models).

### DeepSeek text generation parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| max_tokens | Integer | Input parameter. Maximum number of tokens to generate. Set to `0` for unlimited. Range: `0-128000`. |
| model_kwargs | Dictionary | Input parameter. Additional keyword arguments for the model. |
| json_mode | Boolean | Input parameter. If `true`, outputs JSON regardless of passing a schema. |
| model_name | String | Input parameter. The DeepSeek model to use. Default: `deepseek-chat`. |
| api_base | String | Input parameter. Base URL for API requests. Default: `https://api.deepseek.com`. |
| api_key | SecretString | Input parameter. Your DeepSeek API key for authentication. |
| temperature | Float | Input parameter. Controls randomness in responses. Range: `[0.0, 2.0]`. Default: `1.0`. |
| seed | Integer | Input parameter. Number initialized for random number generation. Use the same seed integer for more reproducible results, and use a different seed number for more random results. |


================================================================================
filename: Components/bundles-docling.mdx
================================================================================

---
title: Docling
slug: /bundles-docling
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import Icon from "@site/src/components/icon";
import PartialDevModeWindows from '@site/docs/_partial-dev-mode-windows.mdx';

Langflow integrates with [Docling](https://docling-project.github.io/docling/) through a bundle of components for parsing documents.

## Prerequisites

* **Enable Developer Mode for Windows**:
<PartialDevModeWindows />

* **Install Docling dependency**:
The Docling dependency is required to use the Docling components in Langflow.

    * **Langflow version 1.6 and later**: The Docling dependency is included by default for all operating systems except macOS Intel (x86_64).

        For macOS Intel (x86_64), use the [Docling installation guide](https://docling-project.github.io/docling/installation/) to install the Docling dependency.

    * **Earlier versions**: Langflow versions earlier than 1.6 don't include the Docling dependency.
    For Langflow OSS, install the Docling extra with `uv pip install 'langflow[docling]'`.
    For Langflow Desktop, add the Docling dependency to Langflow Desktop's `requirements.txt`.
    For more information, see [Install custom dependencies](/install-custom-dependencies).

## Use Docling components in a flow

:::tip
To learn more about content extraction with Docling, see the video tutorial [Docling + Langflow: Document Processing for AI Workflows](https://www.youtube.com/watch?v=5DuS6uRI5OM).
:::

This example demonstrates how to use Docling components to split a PDF in a flow:

1. Connect a **Docling** and an **Export DoclingDocument** component to a [**Split Text** component](/components-processing#split-text).

    The **Docling** component loads the document, and the **Export DoclingDocument** component converts the `DoclingDocument` into the format you select. This example converts the document to Markdown, with images represented as placeholders.
    The **Split Text** component will split the Markdown into chunks for the vector database to store in the next part of the flow.

2. Connect a [**Chroma DB** vector store component](/bundles-chroma#chroma-db) to the **Split Text** component's **Chunks** output.
3. Connect an [embedding model component](/components-embedding-models) to the **Chroma DB** component's **Embedding** port and a **Chat Output** component to view the extracted [`DataFrame`](/data-types#dataframe).
4. In the embedding model component, select your preferred model, provide credentials, and configure other settings as needed.

    ![Docling and ExportDoclingDocument extracting and splitting text to vector database](/img/integrations-docling-split-text.png)

5. Add a file to the **Docling** component.
6. To run the flow, click <Icon name="Play" aria-hidden="true"/> **Playground**.

    The chunked document is loaded as vectors into your vector database.

## Docling components

The following sections describe the purpose and configuration options for each component in the **Docling** bundle.

### Docling language model

The **Docling** language model component ingest documents, and then uses Docling to process them by running the Docling models locally.

It outputs `files`, which is the processed files with `DoclingDocument` data.

For more information, see the [Docling IBM models project repository](https://github.com/docling-project/docling-ibm-models).

#### Docling parameters

| Name | Type | Description |
|------|------|-------------|
| files | File | The files to process. |
| pipeline | String | Docling pipeline to use (standard, vlm). |
| ocr_engine | String | OCR engine to use (easyocr, tesserocr, rapidocr, ocrmac). |

### Docling Serve

The **Docling Serve** component runs Docling as an API service.

It outputs `files`, which is the processed files with `DoclingDocument` data.

For more information, see the [Docling serve project repository](https://github.com/docling-project/docling-serve).

#### Docling Serve parameters

| Name | Type | Description |
|------|------|-------------|
| files | File | The files to process. |
| api_url | String | URL of the Docling Serve instance. |
| max_concurrency | Integer | Maximum number of concurrent requests for the server. |
| max_poll_timeout | Float | Maximum waiting time for the document conversion to complete. |
| api_headers | Dict | Optional dictionary of additional headers required for connecting to Docling Serve. |
| docling_serve_opts | Dict | Optional dictionary of additional options for Docling Serve. |

### Chunk DoclingDocument

The **Chunk DoclingDocument** component uses the `DoclingDocument` chunkers to split a document into chunks.

It outputs the chunked documents as a [`DataFrame`](/data-types#dataframe).

For more information, see the [Docling core project repository](https://github.com/docling-project/docling-core).

#### Chunk DoclingDocument parameters

| Name | Type | Description |
|------|------|-------------|
| data_inputs | Data/DataFrame | The data with documents to split in chunks. |
| chunker | String | Which chunker to use (HybridChunker, HierarchicalChunker). |
| provider | String | Which tokenizer provider (Hugging Face, OpenAI). |
| hf_model_name | String | Model name of the tokenizer to use with the HybridChunker when Hugging Face is chosen. |
| openai_model_name | String | Model name of the tokenizer to use with the HybridChunker when OpenAI is chosen. |
| max_tokens | Integer | Maximum number of tokens for the HybridChunker. |
| doc_key | String | The key to use for the `DoclingDocument` column. |

### Export DoclingDocument

The **Export DoclingDocument** component exports `DoclingDocument` to Markdown, HTML, and other formats.

It can output the exported data as either [`Data`](/data-types#data) or [`DataFrame`](/data-types#dataframe).

For more information, see the [Docling core project repository](https://github.com/docling-project/docling-core).

#### Export DoclingDocument parameters

| Name | Type | Description |
|------|------|-------------|
| data_inputs | Data/DataFrame | The data with documents to export. |
| export_format | String | Select the export format to convert the input (Markdown, HTML, Plaintext, DocTags). |
| image_mode | String | Specify how images are exported in the output (placeholder, embedded). |
| md_image_placeholder | String | Specify the image placeholder for markdown exports. |
| md_page_break_placeholder | String | Add this placeholder between pages in the markdown output. |
| doc_key | String | The key to use for the `DoclingDocument` column. |

## See also

* [**File** component](/components-data#file)


================================================================================
filename: Components/bundles-duckduckgo.mdx
================================================================================

---
title: DuckDuckGo
slug: /bundles-duckduckgo
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **DuckDuckGo** bundle.

## DuckDuckGo Search

This component performs web searches using the [DuckDuckGo](https://www.duckduckgo.com) search engine with result-limiting capabilities.

It outputs a list of search results as a [`DataFrame`](/data-types#dataframe) with a `text` key containing the search results as a single string.

### DuckDuckGo Search parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| input_value | String | Input parameter. The search query to execute with DuckDuckGo. |
| max_results | Integer | Input parameter. The maximum number of search results to return. Default: 5. |
| max_snippet_length | Integer | Input parameter. The maximum length of each result snippet. Default: 100. |

## See also

* [**Web Search** component](/components-data#web-search)
* [**SearchApi** bundle](/bundles-searchapi)


================================================================================
filename: Components/bundles-elastic.mdx
================================================================================

---
title: Elastic
slug: /bundles-elastic
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';
import PartialConditionalParams from '@site/docs/_partial-conditional-params.mdx';
import PartialVectorSearchResults from '@site/docs/_partial-vector-search-results.mdx';
import PartialVectorStoreInstance from '@site/docs/_partial-vector-store-instance.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Elastic** bundle.

## Elasticsearch

The **Elasticsearch** component reads and writes to an Elasticsearch instance using [`ElasticsearchStore`](https://docs.langchain.com/oss/python/integrations/vectorstores/elasticsearch).

<details>
<summary>About vector store instances</summary>

<PartialVectorStoreInstance />

</details>

<PartialVectorSearchResults />

### Elasticsearch parameters

You can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.

<PartialParams />

<PartialConditionalParams />

For information about accepted values and functionality, see the [Elasticsearch documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/dense-vector.html) or inspect [component code](/concepts-components#component-code).

| Name | Type | Description |
|------|------|-------------|
| elasticsearch_url | String | Input parameter. Elasticsearch server URL. |
| cloud_id | String | Input parameter. Elasticsearch Cloud ID. |
| index_name | String | Input parameter. Name of the Elasticsearch index. |
| ingest_data | Data | Input parameter. Records to load into the vector store. |
| search_query | String | Input parameter. Query string for similarity search. |
| cache_vector_store | Boolean | Input parameter. If `true`, the component caches the vector store in memory for faster reads. Default: Enabled (`true`). |
| username | String | Input parameter. Username for Elasticsearch authentication. Required for all local deployments. Required for cloud deployments if `api_key` is empty. |
| password | SecretString | Input parameter. Password for Elasticsearch authentication. Required for all local deployments. Required for cloud deployments if `api_key` is empty |
| embedding | Embeddings | Input parameter. The embedding model to use. |
| search_type | String | Input parameter. The type of search to perform. Options are `similarity` (default) or `mmr`. |
| number_of_results | Integer | Input parameter. Number of search results to return. Default: 4. |
| search_score_threshold | Float | Input parameter. The minimum similarity score threshold for search results. Default: 0. |
| api_key | SecretString | Input parameter. API key for Elastic Cloud authentication. If provided, `username` and `password` aren't required. |
| verify_certs | Boolean | Input parameter. Whether to verify SSL certificates when connecting to Elasticsearch. Default: Enabled (`true`). |

## OpenSearch

The **OpenSearch** component reads and writes to OpenSearch instances using [`OpenSearchVectorSearch`](https://docs.langchain.com/oss/python/integrations/vectorstores/opensearch).

<details>
<summary>About vector store instances</summary>

<PartialVectorStoreInstance />

</details>

### OpenSearch parameters

You can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.;

<PartialParams />

<PartialConditionalParams />

For information about accepted values and functionality, see the [OpenSearch documentation](https://opensearch.org/platform/search/vector-database.html) or inspect [component code](/concepts-components#component-code).

| Name | Type | Description |
|------|------|-------------|
| opensearch_url | String | Input parameter. URL for OpenSearch cluster, such as `https://192.168.1.1:9200`. |
| index_name | String | Input parameter. The index name where the vectors are stored in OpenSearch cluster. Default: `langflow`. |
| ingest_data | Data | Input parameter. The data to be ingested into the vector store. |
| search_input | String | Input parameter. Enter a search query. Leave empty to retrieve all documents or if hybrid search is being used. |
| cache_vector_store | Boolean | Input parameter. If `true`, the component caches the vector store in memory for faster reads. Default: Enabled (`true`). |
| embedding | Embeddings | Input parameter. Attach an [embedding model component](/components-embedding-models) to use to generate an embedding from the search query. |
| search_type | String | Input parameter. The type of search to perform. Options are `similarity` (default), `similarity_score_threshold`, `mmr`. |
| number_of_results | Integer | Input parameter. The number of results to return in search. Default: 4. |
| search_score_threshold | Float | Input parameter. The minimum similarity score threshold for search results. Default: 0. |
| username | String | Input parameter. The username for the OpenSearch cluster. Default: `admin`.|
| password | SecretString | Input parameter. The password for the OpenSearch cluster. |
| use_ssl | Boolean | Input parameter. Whether to use SSL. Default: Enabled (`true`). |
| verify_certs | Boolean | Input parameter. Whether to verify SSL certificates. Default: Disabled (`false`). |
| hybrid_search_query | String | Input parameter. Provide a custom hybrid search query in JSON format. This allows you to combine vector similarity and keyword matching. |

### OpenSearch output

<PartialVectorSearchResults />

<details>
<summary>Vector Store Connection port</summary>

The **OpenSearch** component has an additional deprecated **Vector Store Connection** output.
This output can only connect to a `VectorStore` input port, and it was intended for use with dedicated Graph RAG components.

The **OpenSearch** component doesn't require a separate Graph RAG component because OpenSearch instances support Graph traversal through built-in RAG functionality and plugins.

</details>


================================================================================
filename: Components/bundles-exa.mdx
================================================================================

---
title: Exa
slug: /bundles-exa
---

import Icon from "@site/src/components/icon";

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Exa** bundle.

## Exa Search

This component provides an [Exa Search](https://exa.ai/) toolkit for search and content retrieval by a Langflow [**Agent** component](/agents) or [MCP client](/mcp-client).

The output is exclusively [`Tools`](/data-types#tool).

### Exa Search parameters

| Name | Type | Description |
|------|------|-------------|
| Exa Search API Key (`metaphor_api_key`) | SecretString | Input parameter. An API key for Exa Search. |
| Use Autoprompt (`use_autoprompt`) | Boolean | Input parameter. Whether to use the autoprompt feature. Default: `true`. |
| Search Number of Results (`search_num_results`) | Integer | Input parameter. The number of results to return for search. Default: 5. |
| Similar Number of Results (`similar_num_results`) | Integer | Input parameter. The number of similar results to return. Default: 5. |


================================================================================
filename: Components/bundles-faiss.mdx
================================================================================

---
title: FAISS
slug: /bundles-faiss
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';
import PartialConditionalParams from '@site/docs/_partial-conditional-params.mdx';
import PartialVectorSearchResults from '@site/docs/_partial-vector-search-results.mdx';
import PartialVectorStoreInstance from '@site/docs/_partial-vector-store-instance.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **FAISS** bundle.

## FAISS vector store

The **FAISS** component provides access to the Facebook AI Similarity Search (FAISS) library through an instance of `FAISS` vector store.

<details>
<summary>About vector store instances</summary>

<PartialVectorStoreInstance />

</details>

<PartialVectorSearchResults />

### FAISS vector store parameters

You can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.

<PartialParams />

<PartialConditionalParams />

For information about accepted values and functionality, see the [FAISS documentation](https://faiss.ai/index.html) or inspect [component code](/concepts-components#component-code).

| Name                      | Type          | Description                                      |
|---------------------------|---------------|--------------------------------------------------|
| index_name                 | String        | Input parameter. The name of the FAISS index. Default: "langflow_index". |
| persist_directory          | String        | Input parameter. Path to save the FAISS index. It is relative to where Langflow is running. |
| search_query               | String        | Input parameter. The query to search for in the vector store.     |
| ingest_data                | Data          | Input parameter. The list of data to ingest into the vector store. |
| allow_dangerous_deserialization | Boolean  | Input parameter. Set to `True` to allow loading pickle files from untrusted sources. Default: `True`. |
| embedding                  | Embeddings    | Input parameter. The embedding function to use for the vector store. |
| number_of_results          | Integer       | Input parameter. Number of results to return from the search. Default: 4. |


================================================================================
filename: Components/bundles-glean.mdx
================================================================================

---
title: Glean
slug: /bundles-glean
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Glean** bundle.

## Glean Search API

This component allows you to call the Glean Search API.

It returns a list of search results as a [`DataFrame`](/data-types#dataframe).

### Glean Search API parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| glean_api_url | String | Input parameter. The URL of the Glean API. |
| glean_access_token | SecretString | Input parameter. An access token for Glean API authentication. |
| query | String | Input parameter. The search query input. |
| page_size | Integer | Input parameter. The number of results per page. Default: 10. |
| request_options | Dict | Input parameter. Additional options for the API request. |

## See also

* [**Web Search** component](/components-data#web-search)


================================================================================
filename: Components/bundles-google.mdx
================================================================================

---
title: Google components
slug: /bundles-google
---

import Icon from "@site/src/components/icon";

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Google** bundle.

## BigQuery

Langflow integrates with [Google BigQuery](https://cloud.google.com/bigquery) through the **BigQuery** component in the [**Google** bundle](/bundles-google), allowing you to execute SQL queries and retrieve data from your BigQuery datasets.

### Use the BigQuery component in a flow

To use the **BigQuery** component in a flow, you need the following:

* A [Google Cloud project](https://developers.google.com/workspace/guides/create-project) with the BigQuery API enabled
* A [service account](https://developers.google.com/workspace/guides/create-credentials#service-account) with the **BigQuery Job User** role
* A [BigQuery dataset and table](https://cloud.google.com/bigquery/docs/datasets-intro)
* A [running Langflow server](/get-started-installation)

#### Create a service account with BigQuery access

1. Select and enable your Google Cloud project.
For more information, see [Create a Google Cloud project](https://developers.google.com/workspace/guides/create-project).
2. Create a service account in your Google Cloud project.
For more information, see [Create a service account](https://developers.google.com/workspace/guides/create-credentials#service-account).
3. Assign the **BigQuery Job User** role to your new account.
This role allows Langflow to access BigQuery resources with the service account.
You may also need to allow access to your BigQuery dataset.
For more information, see [BigQuery access control with IAM](https://cloud.google.com/bigquery/docs/access-control).
4. To generate a new JSON key for the service account, navigate to your service account.
5. Click **Add Key**, and then click **Create new key**.
6. Under **Key type**, select **JSON**, and then click **Create**.
A JSON private key file is downloaded to your machine.
Now that you have a service account and a JSON private key, you need to configure the credentials in the Langflow **BigQuery** component.

#### Configure credentials in the Langflow component

With your service account configured and your credentials JSON file created, follow these steps to authenticate the Langflow application.

1. Create a new flow in Langflow.
2. In <Icon name="Blocks" aria-hidden="true" /> **Bundles**, find the Google **BigQuery** component, and then add it to your flow.
3. In the **BigQuery** component's **Upload Service Account JSON** field, click **Select file**.
4. In the **My Files** pane, select **Click or drag files here**.
Your file browser opens.
5. In your file browser, select the service account JSON file, and then click **Open**.
6. In the **My Files** pane, select your service account JSON file, and then click **Select files**.
The **BigQuery** component can now query your datasets and tables using your service account JSON file.

#### Query a BigQuery dataset

With your component credentials configured, query your BigQuery datasets and tables to confirm connectivity.

1. Connect **Chat Input** and **Chat Output** components to the **BigQuery** component.

    ![BigQuery component connected to chat input and output](/img/integrations-bigquery.png)

2. Open the **Playground**, and then submit a valid SQL query.

    This example queries a table of Oscar winners stored within a BigQuery dataset called `the_oscar_award`:

    ```sql
    SELECT film, category, year_film
    FROM `big-query-langflow-project.the_oscar_award.oscar_winners`
    WHERE winner = TRUE
    LIMIT 10
    ```

    <details>
    <summary>Result</summary>

    ```text
    film	category	year_film
    The Last Command	ACTOR	1927
    7th Heaven	ACTRESS	1927
    The Dove;	ART DIRECTION	1927
    Sunrise	CINEMATOGRAPHY	1927
    Sunrise	CINEMATOGRAPHY	1927
    Two Arabian Knights	DIRECTING (Comedy Picture)	1927
    7th Heaven	DIRECTING (Dramatic Picture)	1927
    Wings	ENGINEERING EFFECTS	1927
    Wings	OUTSTANDING PICTURE	1927
    Sunrise	UNIQUE AND ARTISTIC PICTURE	1927
    ```

    </details>

    A successful chat confirms the component can access the BigQuery table.

## Google Generative AI

This component generates text using [Google Generative AI models](https://cloud.google.com/vertex-ai/docs/).

### Google Generative AI parameters

| Name | Type | Description |
|------|------|-------------|
| Google API Key | SecretString | Input parameter. Your Google API key to use for the Google Generative AI. |
| Model | String | Input parameter. The name of the model to use, such as `"gemini-pro"`. |
| Max Output Tokens | Integer | Input parameter. The maximum number of tokens to generate. |
| Temperature | Float | Input parameter. Run inference with this temperature. |
| Top K | Integer | Input parameter. Consider the set of top K most probable tokens. |
| Top P | Float | Input parameter. The maximum cumulative probability of tokens to consider when sampling. |
| N | Integer | Input parameter. Number of chat completions to generate for each prompt. |
| model | LanguageModel | Output parameter. An instance of ChatGoogleGenerativeAI configured with the specified parameters. |

## Google Generative AI Embeddings

The **Google Generative AI Embeddings** component connects to Google's generative AI embedding service using the GoogleGenerativeAIEmbeddings class from the `langchain-google-genai` package.

For more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).

### Google Generative AI Embeddings parameters

| Name | Display Name | Info |
|------|--------------|------|
| api_key | API Key | Input parameter. The secret API key for accessing Google's generative AI service. Required. |
| model_name | Model Name | Input parameter. The name of the embedding model to use. Default: "models/text-embedding-004". |
| embeddings | Embeddings | Output parameter. The built GoogleGenerativeAIEmbeddings object. |

## Google Search API

This component allows you to call the Google Search API.

### Google Search API parameters

| Name | Type | Description |
|------|------|-------------|
| google_api_key | SecretString | Input parameter. A Google API key for authentication. |
| google_cse_id | SecretString | Input parameter. A Google Custom Search Engine ID. |
| input_value | String | Input parameter. The search query input. |
| k | Integer | Input parameter. The number of search results to return. |
| results | List[Data] | Output parameter. A list of search results. |
| tool | Tool | Output parameter. A Google Search tool for use in LangChain. |

### Other Google Search components

Langflow includes multiple components that support Google Search, such as the following:

* [**Apify Actors** component](/bundles-apify)
* [**SearchApi** component](/bundles-searchapi)
* [**Serper Google Search API** component](/bundles-serper)
* [**News Search** component](/components-data#news-search)
* [**Web Search** component](/components-data#web-search)

## Google Vertex AI

For information about Vertex AI components, see the [**Vertex AI** bundle](/bundles-vertexai).

## Legacy Google components

import PartialLegacy from '@site/docs/_partial-legacy.mdx';

<PartialLegacy />

The following Google components are in legacy status:

<details>
<summary>Google OAuth Token</summary>

The **Google OAuth Token** component was deprecated in Langflow 1.4.0.

To connect your flows to Google OAuth services, use [Composio components](/bundles-composio).

</details>

<details>
<summary>Gmail Loader</summary>

This component loads emails from Gmail using [Service Account JSON](https://developers.google.com/identity/protocols/oauth2/service-account) credentials and label ID filters.

As an alternative, you can use [Composio components](/bundles-composio) to connect your flows to Google services.

</details>

<details>
<summary>Google Drive Loader</summary>

This component loads documents from Google Drive using [Service Account JSON](https://developers.google.com/identity/protocols/oauth2/service-account) credentials and document ID filters.

While there is no direct replacement, consider using the [**API Request** component](/components-data#api-request) to call the Google Drive API.

</details>

<details>
<summary>Google Drive Search</summary>

This component searches Google Drive using [Service Account JSON](https://developers.google.com/identity/protocols/oauth2/service-account) credentials and various query strings and filters.

While there is no direct replacement, consider using the [**API Request** component](/components-data#api-request) to call the Google Drive API.

</details>

## See also

- [**Composio** bundle](/bundles-composio)
- [**Vertex AI** bundle](/bundles-vertexai)


================================================================================
filename: Components/bundles-groq.mdx
================================================================================

---
title: Groq
slug: /bundles-groq
---

import Icon from "@site/src/components/icon";

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Groq** bundle.

For more information, see the [Groq documentation](https://groq.com/).

## Groq text generation

This component generates text using Groq's language models.

It can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).
Specifically, the **Language Model** output is an instance of [`ChatGroq`](https://docs.langchain.com/oss/python/integrations/chat/groq) configured according to the component's parameters.

Use the **Language Model** output when you want to use a Groq model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.

For more information, see [Language model components](/components-models).

![Groq component in a basic prompting flow](/img/component-groq.png)

### Groq text generation parameters

| Name | Type | Description |
|------|------|-------------|
| groq_api_key | SecretString | Your Groq API key. |
| groq_api_base | String | Base URL path for API requests. Default: `https://api.groq.com`. |
| max_tokens | Integer | The maximum number of tokens to generate. |
| temperature | Float | Controls randomness in the output. Range: `[0.0, 1.0]`. Default: `0.1`. |
| n | Integer | Number of chat completions to generate for each prompt. |
| model_name | String | The name of the Groq model to use. Options are dynamically fetched from the Groq API after entering your API key and URL. To refresh your list of models, click <Icon name="RefreshCw" aria-hidden="true"/> **Refresh**. |
| tool_mode_enabled | Boolean | If enabled, the component only displays models that work with tools. |


================================================================================
filename: Components/bundles-huggingface.mdx
================================================================================

---
title: Hugging Face
slug: /bundles-huggingface
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

The components in the **Hugging Face** bundle require access to Hugging Face APIs.

For more information about Hugging Face features and functionality used by Hugging Face components, see the [Hugging Face documentation](https://huggingface.co/docs).

## Hugging Face text generation

The **Hugging Face** component generates text using a specified model by sending requests to the Hugging Face API, which is a hosted inference API for models hosted on Hugging Face.
Authentication is required.

This component can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).
Specifically, the **Language Model** output is an instance of [`ChatHuggingFace`](https://docs.langchain.com/oss/python/integrations/chat/huggingface) configured according to the component's parameters.

Use the **Language Model** output when you want to use a Hugging Face model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.

For more information, see [Language model components](/components-models).

### Hugging Face text generation parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| model_id | String | Input parameter. The model ID from Hugging Face Hub. For example, "gpt2", "facebook/bart-large". |
| huggingfacehub_api_token | SecretString | Input parameter. Your [Hugging Face API token](https://huggingface.co/docs/hub/security-tokens) for authentication. |
| temperature | Float | Input parameter. Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.7. |
| max_new_tokens | Integer | Input parameter. Maximum number of tokens to generate. Default: 512. |
| top_p | Float | Input parameter. Nucleus sampling parameter. Range: [0.0, 1.0]. Default: 0.95. |
| top_k | Integer | Input parameter. Top-k sampling parameter. Default: 50. |
| model_kwargs | Dictionary | Input parameter. Additional keyword arguments to pass to the model. |

## Hugging Face Embeddings Inference

Use the **Hugging Face Embeddings Inference** component to create embeddings with Hugging Face's hosted models or with your own locally hosted models.

The component generates embeddings using [Hugging Face Inference API models](https://huggingface.co/models).
Authentication is required when not using a local model.

For more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models) and [Use a local Hugging Face embeddings model](#local-hugging-face-model).

### Hugging Face Embeddings Inference parameters

| Name | Display Name | Info |
|------|--------------|------|
| api_key | API Key | Input parameter. Your [Hugging Face API token](https://huggingface.co/docs/hub/security-tokens) for accessing the Hugging Face Inference API, if required. Local inference models don't require an API key. |
| api_url | API URL | Input parameter. The URL of the Hugging Face Inference API. |
| model_name | Model Name | Input parameter. The name of the model to use for embeddings. |

### Use a local Hugging Face embeddings model {#local-hugging-face-model}

To connect the local Hugging Face model to the **Hugging Face Embeddings Inference** component and use it in a flow, follow these steps:

1. Run a [local Hugging Face embeddings inference](https://huggingface.co/docs/text-embeddings-inference/local_cpu).

2. For this example, create a flow from the **Vector Store RAG** template.

3. Replace the two **OpenAI Embeddings** components with **Hugging Face Embeddings Inference** components.

    Make sure to reconnect the **Embedding Model** ports from each **Embeddings Inference** component to its corresponding **Astra DB** component.

4. Configure the **Astra DB** components to connect to your Astra organization, or replace both **Astra DB** components with other vector store components.

5. Connect each **Hugging Face Embeddings Inference** component to your local inference model:

    * **Inference Endpoint**: Enter the URL of your local inference model.
    * **API Key**: Can be empty for local inference.
    * **Model Name**: Enter the name of your local inference model if it isn't automatically detected.

6. To test the flow, click **Playground**, and then enter some text to generate an embedding.


================================================================================
filename: Components/bundles-ibm.mdx
================================================================================

---
title: IBM
slug: /bundles-ibm
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

The **IBM** bundle provides access to IBM watsonx.ai models for text and embedding generation.
These components require an IBM watsonx.ai deployment and watsonx API credentials.

## IBM watsonx.ai

The **IBM watsonx.ai** component generates text using [supported foundation models](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models.html?context=wx) in [IBM watsonx.ai](https://www.ibm.com/products/watsonx-ai).
To use gateway models, use the [**OpenAI** text generation component](/bundles-openai) with the gateway model's OpenAI-compatible endpoint.

You can use the **IBM watsonx.ai** component anywhere you need a language model in a flow.

![A basic prompting flow using the IBM watsonx.ai component as the central Language Model component.](/img/component-watsonx-model.png)

### IBM watsonx.ai parameters {#ibm-watsonxai-parameters}

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| url | String | Input parameter. The [watsonx API base URL](https://cloud.ibm.com/apidocs/watsonx-ai#endpoint-url) for your deployment and region. |
| project_id | String | Input parameter. Your [watsonx Project ID](https://www.ibm.com/docs/en/watsonx/saas?topic=projects). |
| api_key | SecretString | Input parameter. A [watsonx API key](https://www.ibm.com/docs/en/watsonx/saas?topic=administration-managing-user-api-key) to authenticate watsonx API access to the specified watsonx.ai deployment and model. |
| model_name | String | Input parameter. The name of the watsonx model to use. Options are dynamically fetched from the API. |
| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Default: `1000`. |
| stop_sequence | String | Input parameter. The sequence where generation should stop. |
| temperature | Float | Input parameter. Controls randomness in the output. Default: `0.1`. |
| top_p | Float | Input parameter. Controls nucleus sampling, which limits the model to tokens whose probability is below the `top_p` value. Range: Default: `0.9`. |
| frequency_penalty | Float | Input parameter. Controls frequency penalty. A positive value decreases the probability of repeating tokens, and a negative value increases the probability. Range: Default: `0.5`. |
| presence_penalty | Float | Input parameter. Controls presence penalty. A positive value increases the likelihood of new topics being introduced. Default: `0.3`. |
| seed | Integer | Input parameter. A random seed for the model. Default: `8`. |
| logprobs | Boolean | Input parameter. Whether to return log probabilities of output tokens or not. Default: `true`. |
| top_logprobs | Integer | Input parameter. The number of most likely tokens to return at each position. Default: `3`. |
| logit_bias | String | Input parameter. A JSON string of token IDs to bias or suppress. |

### IBM watsonx.ai output

The **IBM watsonx.ai** component can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).

Use the **Language Model** output when you want to use an IBM watsonx.ai model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.
For more information, see [Language model components](/components-models).

The `LanguageModel` output from the **IBM watsonx.ai** component is an instance of `[ChatWatsonx](https://docs.langchain.com/oss/python/integrations/chat/ibm_watsonx)` configured according to the [component's parameters](#ibm-watsonxai-parameters).

## IBM watsonx.ai Embeddings

The **IBM watsonx.ai Embeddings** component uses the [supported foundation models](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models.html?context=wx) in [IBM watsonx.ai](https://www.ibm.com/products/watsonx-ai) for embedding generation.

The output is [`Embeddings`](/data-types#embeddings) generated with [`WatsonxEmbeddings`](https://python.langchain.com/docs/integrations/text_embedding/ibm_watsonx/).

For more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).

![A basic embedding generation flow using the IBM watsonx.ai Embeddings component](/img/component-watsonx-embeddings-chroma.png)

### IBM watsonx.ai Embeddings parameters

<PartialParams />

| Name | Display Name | Info |
|------|--------------|------|
| url | watsonx API Endpoint | Input parameter. The [watsonx API base URL](https://cloud.ibm.com/apidocs/watsonx-ai#endpoint-url) for your deployment and region. |
| project_id | watsonx project id | Input parameter. Your [watsonx Project ID](https://www.ibm.com/docs/en/watsonx/saas?topic=projects). |
| api_key | API Key | Input parameter. A [watsonx API key](https://www.ibm.com/docs/en/watsonx/saas?topic=administration-managing-user-api-key) to authenticate watsonx API access to the specified watsonx.ai deployment and model. |
| model_name | Model Name | Input parameter. The name of the embedding model to use. Supports [default embedding models](#default-embedding-models) and automatically updates after connecting to your watsonx.ai deployment. |
| truncate_input_tokens | Truncate Input Tokens | Input parameter. The maximum number of tokens to process. Default: `200`. |
| input_text | Include the original text in the output | Input parameter. Determines if the original text is included in the output. Default: `true`. |

### Default embedding models

By default, the **IBM watsonx.ai Embeddings** component supports the following default models:

- `sentence-transformers/all-minilm-l12-v2`: 384-dimensional embeddings
- `ibm/slate-125m-english-rtrvr-v2`: 768-dimensional embeddings
- `ibm/slate-30m-english-rtrvr-v2`: 768-dimensional embeddings
- `intfloat/multilingual-e5-large`: 1024-dimensional embeddings

After entering your API endpoint and credentials, the component automatically fetches the list of available models from your watsonx.ai deployment.

## See also

* [IBM documentation](https://cloud.ibm.com/docs)


================================================================================
filename: Components/bundles-icosacomputing.mdx
================================================================================

---
title: Icosa Computing
slug: /bundles-icosacomputing
---

import Icon from "@site/src/components/icon";

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

The **Icosa Computing** components require access to Icosa Computing services.
For more information and to request access, see the [Icosa Computing site](https://www.icosacomputing.com/).

## Combinatorial Reasoner

The **Combinatorial Reasoner** component runs Icosa's Combinatorial Reasoning (CR) pipeline on an input to create an optimized prompt with embedded reasons.

### Combinatorial Reasoner parameters

| Name | Type | Description |
|------|------|-------------|
| prompt | String | Input parameter. The input to run CR on. |
| openai_api_key | SecretString | Input parameter. An OpenAI API key for authentication to OpenAI, which is used for reason generation. |
| model_name | String | Input parameter. The OpenAI LLM to use for reason generation. |
| username | String | Input parameter. A username for authentication to the Icosa API. |
| password | SecretString | Input parameter. A password for authentication to the Icosa API. |

### Combinatorial Reasoner outputs

The **Combinatorial Reasoner** component outputs one of the following:

* **Optimized Prompt (`optimized_prompt`)**: A [`Message`](/data-types#message) object containing the optimized prompt with embedded reasons.
* **Selected Reasons (`reasons`)**: A [`Data`](/data-types#data) object containing a list of strings where each string is a reason that was selected and embedded in the optimized prompt.

You can toggle the output type near the component's output port.

## See also

* [**Prompt Template** component](/components-prompts)


================================================================================
filename: Components/bundles-langchain.mdx
================================================================================

---
title: LangChain
slug: /bundles-langchain
---

import Icon from "@site/src/components/icon";

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **LangChain** bundle.

## CSV Agent

This component is based on the [**Agent** core component](/agents).

This component creates a CSV agent from a CSV file and LLM.
For more information, see the [LangChain CSV agent documentation](https://python.langchain.com/api_reference/experimental/agents/langchain_experimental.agents.agent_toolkits.csv.base.create_csv_agent.html).

### CSV Agent parameters

| Name | Type | Description |
|------|------|-------------|
| llm | LanguageModel | Input parameter. The language model to use for the agent. |
| path | File | Input parameter. The path to the CSV file. |
| agent_type | String | Input parameter. The type of agent to create. |
| agent | AgentExecutor | Output parameter. The CSV agent instance. |

## OpenAI Tools Agent

This component is based on the [**Agent** core component](/agents).

This component creates an OpenAI Tools Agent.
For more information, see the [LangChain OpenAI agent documentation](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.openai_functions_agent.base.create_openai_functions_agent.html).

### OpenAI Tools Agent parameters

| Name | Type | Description |
|------|------|-------------|
| llm | LanguageModel | Input parameter. The language model to use. |
| tools | List of Tools | Input parameter. The tools to give the agent access to. |
| system_prompt | String | Input parameter. The system prompt to provide context to the agent. |
| input_value | String | Input parameter. The user's input to the agent. |
| memory | Memory | Input parameter. The memory for the agent to use for context persistence. |
| max_iterations | Integer | Input parameter. The maximum number of iterations to allow the agent to execute. |
| verbose | Boolean | Input parameter. This determines whether to print out the agent's intermediate steps. |
| handle_parsing_errors | Boolean | Input parameter. This determines whether to handle parsing errors in the agent. |
| agent | AgentExecutor | Output parameter. The OpenAI Tools agent instance. |
| output | String | Output parameter. The output from executing the agent on the input. |

## OpenAPI Agent

This component is based on the [**Agent** core component](/agents).

This component creates an agent for interacting with OpenAPI services.
For more information, see the [LangChain OpenAPI toolkit documentation](https://docs.langchain.com/oss/python/integrations/tools/openapi).

### OpenAPI Agent parameters

| Name | Type | Description |
|------|------|-------------|
| llm | LanguageModel | Input parameter. The language model to use. |
| openapi_spec | String | Input parameter. The OpenAPI specification for the service. |
| base_url | String | Input parameter. The base URL for the API. |
| headers | Dict | Input parameter. The optional headers for API requests. |
| agent_executor_kwargs | Dict | Input parameter. The optional parameters for the agent executor. |
| agent | AgentExecutor | Output parameter.The OpenAPI agent instance. |

## Prompt Hub

This component fetches prompts from the [LangChain Hub](https://docs.langchain.com/langsmith/manage-prompts#public-prompt-hub).

Like the [**Prompt Template** core component](/components-prompts), additional fields are added to the component for each variable in the prompt.
For example, the default prompt `efriis/my-first-prompt` adds fields for `profession` and `question`.

### Prompt Hub parameters

| Name                | Display Name              | Description                              |
|---------------------|---------------------------|------------------------------------------|
| langchain_api_key   | Your LangChain API Key    | Input parameter. The LangChain API Key to use. |
| langchain_hub_prompt| LangChain Hub Prompt      | Input parameter. The LangChain Hub prompt to use.  |
| prompt              | Build Prompt              | Output parameter. The built prompt message returned by the `build_prompt` method.   |

## SQL Agent

This component is based on the [**Agent** core component](/agents).

This component creates an agent for interacting with SQL databases.
For more information, see the [LangChain SQL agent documentation](https://docs.langchain.com/oss/python/langchain/sql-agent).

### SQL Agent parameters

| Name | Type | Description |
|------|------|-------------|
| llm | LanguageModel | Input parameter. The language model to use. |
| database | Database | Input parameter. The SQL database connection. |
| top_k | Integer | Input parameter. The number of results to return from a SELECT query. |
| use_tools | Boolean | Input parameter. This determines whether to use tools for query execution. |
| return_intermediate_steps | Boolean | Input parameter. This determines whether to return the agent's intermediate steps. |
| max_iterations | Integer | Input parameter. The maximum number of iterations to run the agent. |
| max_execution_time | Integer | Input parameter. The maximum execution time in seconds. |
| early_stopping_method | String | Input parameter. The method to use for early stopping. |
| verbose | Boolean | Input parameter. This determines whether to print the agent's thoughts. |
| agent | AgentExecutor | Output parameter. The SQL agent instance. |

## SQL Database

The LangChain **SQL Database** component establishes a connection to an SQL database.

This component is different from the [**SQL Database** core component](/components-data#sql-database), which executes SQL queries on SQLAlchemy-compatible databases.

## Text Splitters

The **LangChain** bundle includes the following text splitter components:

- **Character Text Splitter**
- **Language Recursive Text Splitter**
- **Natural Language Text Splitter**
- **Recursive Character Text Splitter**
- **Semantic Text Splitter**

## Tool Calling Agent

This component is based on the [**Agent** core component](/agents).

This component creates an agent for structured tool calling with various language models.
For more information, see the [LangChain tool calling documentation](https://docs.langchain.com/oss/python/langchain/agents#tools).

### Tool Calling Agent parameters

| Name | Type | Description |
|------|------|-------------|
| llm | LanguageModel | Input parameter. The language model to use. |
| tools | List[Tool] | Input parameter. The list of tools available to the agent. |
| system_message | String | Input parameter. The system message to use for the agent. |
| return_intermediate_steps | Boolean | Input parameter. This determines whether to return the agent's intermediate steps. |
| max_iterations | Integer | Input parameter. The maximum number of iterations to run the agent. |
| max_execution_time | Integer | Input parameter. The maximum execution time in seconds. |
| early_stopping_method | String | Input parameter. The method to use for early stopping. |
| verbose | Boolean | Input parameter. This determines whether to print the agent's thoughts. |
| agent | AgentExecutor | Output parameter. The tool calling agent instance. |

## XML Agent

This component is based on the [**Agent** core component](/agents).

This component creates an XML Agent using LangChain.
The agent uses XML formatting for tool instructions to the LLM.
For more information, see the [LangChain XML Agent documentation](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.xml.base.create_xml_agent.html).

### XML Agent parameters

| Name | Type | Description |
|------|------|-------------|
| llm | LanguageModel | Input parameter. The language model to use for the agent. |
| user_prompt | String | Input parameter. The custom prompt template for the agent with XML formatting instructions. |
| tools | List[Tool] | Input parameter. The list of tools available to the agent. |
| agent | AgentExecutor | Output parameter. The XML Agent instance. |

## Other LangChain components

Other components in the **LangChain** bundle include the following:

- **Fake Embeddings**
- **HTML Link Extractor**
- **Runnable Executor**
- **Spider Web Crawler & Scraper**

## Legacy LangChain components

import PartialLegacy from '@site/docs/_partial-legacy.mdx';

<PartialLegacy />

The following LangChain components are in legacy status:

* **Conversation Chain**
* **LLM Checker Chain**
* **LLM Math Chain**
* **Natural Language to SQL**
* **Retrieval QA**
* **Self Query Retriever**
* **JSON Agent**
* **Vector Store Info/Agent**
* **VectorStoreRouterAgent**

To replace these components, consider other components in the **LangChain** bundle or general Langflow components, such as the [**Agent** component](/components-agents) or the [**SQL Database** component](/components-data#sql-database).


================================================================================
filename: Components/bundles-lmstudio.mdx
================================================================================

---
title: LM Studio
slug: /bundles-lmstudio
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

The components in the **LM Studio** bundle let you use models from a local or hosted instance of LM Studio.
Components can require authentication with an LM Studio API key. For information about LM Studio models, connections, and credentials, see the [LM Studio documentation](https://lmstudio.ai/docs).

## LM Studio text generation

The **LM Studio** component generates text using LM Studio's local language models.

It can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).

Use the **Language Model** output when you want to use an LM Studio model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.

For more information, see [Language model components](/components-models).

### LM Studio text generation parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| base_url | String | Input parameter. The URL where LM Studio is running. Default: `"http://localhost:1234"`. |
| api_key | LM Studio API Key | Input parameter. The API key for authentication with LM Studio, if required. |
| max_tokens | Integer | Input parameter. Maximum number of tokens to generate in the response. Default: `512`. |
| temperature | Float | Input parameter. Controls randomness in the output. Range: `[0.0, 2.0]`. Default: `0.7`. |
| top_p | Float | Input parameter. Controls diversity with nucleus sampling. Range: `[0.0, 1.0]`. Default: `1.0`. |
| stop | List[String] | Input parameter. List of strings that stop generation when encountered. |
| stream | Boolean | Input parameter. Whether to stream the response. Default: `false`. |
| presence_penalty | Float | Input parameter. Penalizes repeated tokens. Range: `[-2.0, 2.0]`. Default: `0.0`. |
| frequency_penalty | Float | Input parameter. Penalizes frequent tokens. Range: `[-2.0, 2.0]`. Default: `0.0`. |

## LM Studio Embeddings

The **LM Studio Embeddings** component generates embeddings using LM Studio models.

For more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).

### LM Studio Embeddings parameters

<PartialParams />

| Name | Display Name | Info |
|------|--------------|------|
| model | Model | Input parameter. The LM Studio model to use for generating embeddings. |
| base_url | LM Studio Base URL | Input parameter. The base URL for the LM Studio API. |
| api_key | LM Studio API Key | Input parameter. The API key for authentication with LM Studio. |
| temperature | Model Temperature | Input parameter. The temperature setting for the model. |


================================================================================
filename: Components/bundles-maritalk.mdx
================================================================================

---
title: MariTalk
slug: /bundles-maritalk
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **MariTalk** bundle.

For more information, see the [MariTalk documentation](https://www.maritalk.com/).

## MariTalk text generation

The **MariTalk** component generates text using MariTalk LLMs.

It can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).

Use the **Language Model** output when you want to use a MariTalk model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.

For more information, see [Language model components](/components-models).

### MariTalk text generation parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Set to `0` for unlimited tokens. Default: `512`. |
| model_name | String | Input parameter. The name of the MariTalk model to use. Options: `sabia-2-small`, `sabia-2-medium`. Default: `sabia-2-small`. |
| api_key | SecretString | Input parameter. The MariTalk API Key to use for authentication. |
| temperature | Float | Input parameter. Controls randomness in the output. Range: `[0.0, 1.0]`. Default: `0.5`. |
| endpoint_url | String | Input parameter. The MariTalk API endpoint. Default: `https://api.maritalk.com`. |


================================================================================
filename: Components/bundles-mem0.mdx
================================================================================

---
title: Mem0
slug: /bundles-mem0
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Mem0** bundle.

## Mem0 Chat Memory

The **Mem0 Chat Memory** component retrieves and stores chat messages using Mem0 memory storage.

### Mem0 Chat Memory parameters

<PartialParams />

| Name | Display Name | Info |
|------|--------------|------|
| mem0_config | Mem0 Configuration | Input parameter. The configuration dictionary for initializing the Mem0 memory instance. |
| ingest_message | Message to Ingest | Input parameter. The message content to be ingested into Mem0 memory. |
| existing_memory | Existing Memory Instance | Input parameter. An optional existing Mem0 memory instance. |
| user_id | User ID | Input parameter. The identifier for the user associated with the messages. |
| search_query | Search Query | Input parameter. The input text for searching related memories in Mem0. |
| mem0_api_key | Mem0 API Key | Input parameter. The API key for the Mem0 platform. Leave empty to use the local version. |
| metadata | Metadata | Input parameter. The additional metadata to associate with the ingested message. |
| openai_api_key | OpenAI API Key | Input parameter. The API key for OpenAI. Required when using OpenAI embeddings without a provided configuration. |

### Mem0 Chat Memory output

The **Mem0 Chat Memory** component can output either **Mem0 Memory** ([`Memory`](/data-types#memory)) or **Search Results** ([`Data`](/data-types#data)).
You can select the output type near the component's output port.

Use **Mem0 Chat Memory** for memory storage and retrieval operations with the [**Message History** component](/components-helpers#message-history).

Use the **Search Results** output to retrieve specific memories based on a search query.


================================================================================
filename: Components/bundles-milvus.mdx
================================================================================

---
title: Milvus
slug: /bundles-milvus
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';
import PartialConditionalParams from '@site/docs/_partial-conditional-params.mdx';
import PartialVectorSearchResults from '@site/docs/_partial-vector-search-results.mdx';
import PartialVectorStoreInstance from '@site/docs/_partial-vector-store-instance.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Milvus** bundle.

## Milvus vector store

The **Milvus** component reads and writes to Milvus vector stores using an instance of `Milvus` vector store.

<details>
<summary>About vector store instances</summary>

<PartialVectorStoreInstance />

</details>

<PartialVectorSearchResults />

:::tip
For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).
:::

### Milvus vector store parameters

You can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.

<PartialParams />

<PartialConditionalParams />

For information about accepted values and functionality, see the [Milvus documentation](https://milvus.io/docs) or inspect [component code](/concepts-components#component-code).

| Name                    | Type          | Description                                      |
|-------------------------|---------------|--------------------------------------------------|
| collection_name          | String        | Input parameter. Name of the Milvus collection.                   |
| collection_description   | String        | Input parameter. Description of the Milvus collection.            |
| uri                      | String        | Input parameter. Connection URI for Milvus.                       |
| password                 | SecretString  | Input parameter. Password for Milvus.                             |
| username                 | SecretString  | Input parameter. Username for Milvus.                             |
| batch_size               | Integer       | Input parameter. Number of data to process in a single batch.     |
| search_query             | String        | Input parameter. Query for similarity search.                     |
| ingest_data              | Data          | Input parameter. Data to be ingested into the vector store.       |
| embedding                | Embeddings    | Input parameter. Embedding function to use.                       |
| number_of_results        | Integer       | Input parameter. Number of results to return in search.           |
| search_type              | String        | Input parameter. Type of search to perform.                       |
| search_score_threshold   | Float         | Input parameter. Minimum similarity score for search results.     |
| search_filter            | Dict          | Input parameter. Metadata filters for search query.               |
| setup_mode               | String        | Input parameter. Configuration mode for setting up the vector store. |
| vector_dimensions        | Integer       | Input parameter. Number of dimensions of the vectors.             |
| pre_delete_collection    | Boolean       | Input parameter. Whether to delete the collection before creating a new one. |


================================================================================
filename: Components/bundles-mistralai.mdx
================================================================================

---
title: MistralAI
slug: /bundles-mistralai
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **MistralAI** bundle.

For more information about MistralAI features and functionality used by MistralAI components, see the [MistralAI documentation](https://docs.mistral.ai).

## MistralAI text generation

The **MistralAI** component generates text using MistralAI LLMs.

It can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).

Use the **Language Model** output when you want to use a MistralAI model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.

For more information, see [Language model components](/components-models).

### MistralAI text generation parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Set to 0 for unlimited tokens. |
| model_name | String | Input parameter. The name of the Mistral AI model to use. Options include `open-mixtral-8x7b`, `open-mixtral-8x22b`, `mistral-small-latest`, `mistral-medium-latest`, `mistral-large-latest`, and `codestral-latest`. Default: `codestral-latest`. |
| mistral_api_base | String | Input parameter. The base URL of the Mistral API. Defaults to `https://api.mistral.ai/v1`. |
| api_key | SecretString | Input parameter. The Mistral API Key to use for authentication. |
| temperature | Float | Input parameter. Controls randomness in the output. Default: 0.5. |
| max_retries | Integer | Input parameter. Maximum number of retries for API calls. Default: 5. |
| timeout | Integer | Input parameter. Timeout for API calls in seconds. Default: 60. |
| max_concurrent_requests | Integer | Input parameter. Maximum number of concurrent API requests. Default: 3. |
| top_p | Float | Input parameter. Nucleus sampling parameter. Default: 1. |
| random_seed | Integer | Input parameter. Seed for random number generation. Default: 1. |
| safe_mode | Boolean | Input parameter. Enables safe mode for content generation. |

## MistralAI Embeddings

The **MistralAI Embeddings** component generates embeddings using MistralAI models.

For more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).

### MistralAI Embeddings parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| model | String | Input parameter. The MistralAI model to use. Default: "mistral-embed". |
| mistral_api_key | SecretString | Input parameter. The API key for authenticating with MistralAI. |
| max_concurrent_requests | Integer | Input parameter. The maximum number of concurrent API requests. Default: 64. |
| max_retries | Integer | Input parameter. The maximum number of retry attempts for failed requests. Default: 5. |
| timeout | Integer | Input parameter. The request timeout in seconds. Default: 120. |
| endpoint | String | Input parameter. The custom API endpoint URL. Default: `https://api.mistral.ai/v1/`. |


================================================================================
filename: Components/bundles-mongodb.mdx
================================================================================

---
title: MongoDB
slug: /bundles-mongodb
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';
import PartialConditionalParams from '@site/docs/_partial-conditional-params.mdx';
import PartialVectorSearchResults from '@site/docs/_partial-vector-search-results.mdx';
import PartialVectorStoreInstance from '@site/docs/_partial-vector-store-instance.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **MongoDB** bundle.

## MongoDB Atlas

The **MongoDB Atlas** component reads and writes to MongoDB Atlas vector stores using an instance of [`MongoDBAtlasVectorSearch`](https://docs.langchain.com/oss/python/integrations/vectorstores/mongodb_atlas).

<details>
<summary>About vector store instances</summary>

<PartialVectorStoreInstance />

</details>

<PartialVectorSearchResults />

### MongoDB Atlas parameters

You can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.

<PartialParams />

<PartialConditionalParams />

For information about accepted values and functionality, see the [MongoDB Atlas documentation](https://www.mongodb.com/docs/atlas/atlas-vector-search/tutorials/vector-search-quick-start/) or inspect [component code](/concepts-components#component-code).

| Name                      | Type         | Description                               |
| ------------------------- | ------------ | ----------------------------------------- |
| mongodb_atlas_cluster_uri | SecretString | Input parameter. The connection URI for your MongoDB Atlas cluster. Required. |
| enable_mtls               | Boolean      | Input parameter. Enable mutual TLS authentication. Default: `false`. |
| mongodb_atlas_client_cert | SecretString | Input parameter. Client certificate combined with private key for mTLS authentication. Required if mTLS is enabled. |
| db_name                   | String       | Input parameter. The name of the database to use. Required. |
| collection_name           | String       | Input parameter. The name of the collection to use. Required. |
| index_name                | String       | Input parameter. The name of the Atlas Search index, it should be a Vector Search. Required. |
| insert_mode               | String       | Input parameter. How to insert new documents into the collection. The options are "append" or "overwrite". Default: "append". |
| embedding                 | Embeddings   | Input parameter. The embedding model to use. |
| number_of_results         | Integer      | Input parameter. Number of results to return in similarity search. Default: 4. |
| index_field               | String       | Input parameter. The field to index. Default: "embedding". |
| filter_field              | String       | Input parameter. The field to filter the index. |
| number_dimensions         | Integer      | Input parameter. Embedding vector dimension count. Default: 1536. |
| similarity                | String       | Input parameter. The method used to measure similarity between vectors. The options are "cosine", "euclidean", or "dotProduct". Default: "cosine". |
| quantization              | String       | Input parameter. Quantization reduces memory costs by converting 32-bit floats to smaller data types. The options are "scalar" or "binary". |


================================================================================
filename: Components/bundles-notion.mdx
================================================================================

---
title: Notion
slug: /bundles-notion
---

import Icon from "@site/src/components/icon";

To use Notion components in Langflow, you first need to create a Notion integration and configure it with the necessary capabilities. This guide will walk you through the process of setting up a Notion integration and granting it access to your Notion databases, and then provide two examples of flows with Notion components.

## Set up a Notion app

To create a Notion app, you need the following:

- A Notion account with access to the workspace where you want to use the integration.
- Admin permissions in the Notion workspace to create and manage integrations.

### Create a Notion Integration

1. Go to the [Notion Integrations](https://www.notion.com/my-integrations) page.
2. Click **New Integration**.
3. Enter an integration name, and then select the workspace where you want to use it.
4. Optional: Enable the [necessary Notion integration capabilities](https://developers.notion.com/reference/capabilities) based on your requirements.
5. Click **Submit** to create the integration.

### Configure Integration Capabilities

After creating the integration, you need to configure its capabilities to define what actions it can perform and what data it can access.

1. In the integration settings page, go to the **Capabilities** tab.
2. Enable the required capabilities for your integration. For example:
   - If your integration needs to read data from Notion, enable the "Read content" capability.
   - If your integration needs to create new content in Notion, enable the "Insert content" capability.
   - If your integration needs to update existing content in Notion, enable the "Update content" capability.
3. Configure the user information access level based on your integration's requirements.
4. Save the changes.

### Obtain Integration Token

:::warning
Your integration token is a sensitive piece of information. Make sure to keep it secure and never share it publicly. Store it safely in your Langflow configuration or environment variables.
:::

To authenticate your integration with Notion, you need to obtain an integration token.

1. In the integration settings page, go to the "Secrets" tab.
2. Copy the "Internal Integration Token" value. This token will be used to authenticate your integration with Notion.

### Grant Integration Access to Notion Databases

For your integration to interact with Notion databases, you need to grant it access to the specific databases it must work with:

1. Open the Notion database that you want your integration to access.
2. Click **Share**.
3. In the **Invite** section, select your integration from the list.
4. Click **Invite** to grant the integration access to the database.

   If your database contains references to other databases, you need to grant the integration access to those referenced databases as well.
   Repeat this step for each referenced database that your integration must access.

## Build with Notion components in Langflow

Once you have set up your Notion integration and granted it access to the required databases, you can start using the Notion components in Langflow:

- **Add Content to Page**: Converts markdown text to Notion blocks and appends them to a specified Notion page.
- **Create Page**: Creates a new page in a specified Notion database with the provided properties.
- **List Database Properties**: Retrieves the properties of a specified Notion database.
- **List Pages**: Queries a Notion database with filtering and sorting options.
- **List Users**: Retrieves a list of users from the Notion workspace.
- **Page Content Viewer**: Retrieves the content of a Notion page as plain text.
- **Search**: Searches all pages and databases that have been shared with the integration. You can filter results to either pages or databases and specify the sort direction.
- **Update Page Property**: Updates the properties of an existing Notion page.

Each of these components can output `Data` and `Tool` [data types](/data-types).

## Notion Meeting Notes Agent

The Notion Agent for Meeting Notes is an AI-powered tool that automatically processes meeting transcripts and updates your Notion workspace. It identifies tasks, action items, and key points from your meetings, then creates new tasks or updates existing ones in Notion without manual input.

To build this flow, you need the following:

- [A Notion App](#set-up-a-notion-app)
- [A Notion API key](https://www.notion.so/my-integrations)
- [An OpenAI API key](https://platform.openai.com/account/api-keys)
- Recommended: [Download the Meeting Agent Flow JSON](/files/Meeting_Notes_Agent.json), and then [import the flow](/concepts-flows-import) into Langflow.

:::warning
Treat all keys and other credentials as sensitive information. Use secure references, and don't share them publicly.
:::

### Components used in this flow

![Notion Meeting Agent Part 1](/img/notion_meeting_agent_part_1.png)

* Meeting Transcript (text input): This component allows users to input the meeting transcript directly into the flow.
* List Users (Notion component): Retrieves a list of users from the Notion workspace. Requires a Notion Secret (API key).
* List Databases (Notion component): Searches and lists all databases in the Notion workspace. Requires the following inputs:
    - Notion Secret (API key)
    - Query (optional)
    - Filter Type (default: database)
    - Sort Direction
* Prompt Template: This component creates a dynamic prompt template using the following inputs:
    - Meeting Transcript
    - List of Users
    - List of Databases
    - Current Date
* Meeting Summarizer (Tool Calling Agent): Analyzes the meeting transcript and identifies tasks and action items. Requires the following inputs:
    - System Prompt (from the **Prompt Template** component)
    - Language Model (OpenAI)
    - Tools:
        - Notion Search
        - List Database Properties: Retrieves the properties of a specified Notion database. Requires a Database ID and a Notion Secret (API key) as input.
        - Create Page: Creates a new page in a Notion database. Requires a Database ID, Properties (JSON), and a Notion Secret (API key) as input.
        - Update Page Property: Updates the properties of an existing Notion page. Requires a Page ID, Notion Secret (API key), and properties to update as input.
        - Add Content to Page: Converts markdown text to Notion blocks and appends them to a specified Notion page. Requires a Page/Block ID, Notion Secret (API key), and Markdown text as input.

![Notion Meeting Agent Part 2](/img/notion_meeting_agent_part_2.png)

* Notion Agent (Tool Calling Agent): Executes actions in Notion based on the meeting summary/ Requires the following inputs:
    - System Prompt (from the second **Prompt Template** component)
    - Language Model (OpenAI)
    - Tools:
        - List Database Properties: Retrieves the properties of a specified Notion database. Requires a Database ID and a Notion Secret (API key) as input.
        - Create Page: Creates a new page in a Notion database. Requires a Database ID, Properties (JSON), and a Notion Secret (API key) as input.
        - Update Page Property: Updates the properties of an existing Notion page. Requires a Page ID, Notion Secret (API key), and properties to update as input.
        - Add Content to Page: Converts markdown text to Notion blocks and appends them to a specified Notion page. Requires a Page/Block ID, Notion Secret (API key), and Markdown text as input.
* Chat Output: Displays the final output of the Notion Agent in the Playground.

### Flow Process

1. The user inputs a meeting transcript.
2. The flow retrieves the list of Notion users and databases.
3. A prompt is generated using the transcript, user list, database list, and current date.
4. The Meeting Summarizer analyzes the transcript and identifies tasks and action items.
5. The Notion Agent uses the meeting summary to:
   - Create new pages for new tasks
   - Update existing pages for existing tasks
   - Add content to pages with meeting notes
6. The Chat Output displays a summary of actions taken in Notion.

### Run the Notion Meeting Notes flow

1. Create a flow manually or import a pre-built flow JSON file:

    * Recommended: [Download the Meeting Agent flow JSON](/files/Meeting_Notes_Agent.json) and then [import the flow](/concepts-flows-import) into Langflow.
    * Create a blank flow, and then add the previously described components to your flow, connecting them as shown in the flow diagram.

2. Input the Notion and OpenAI API keys in their respective components.
3. Paste your meeting transcript into the **Meeting Transcript** component.

    For optimal results, use detailed meeting transcripts. The quality of the output depends on the comprehensiveness of the input provided.

4. Run the flow by clicking <Icon name="Play" aria-hidden="true" /> **Run component** on the **Chat Output** component or open the **Playground**.
5. Review the output summarizing the actions taken in your Notion workspace.
6. Optional: The flow can be customized to meet your team's specific needs.
Customize this flow by:
    * Adjusting the system prompt to change the agent's behavior or context.
    * Adding or removing Notion tools based on your specific needs.
    * Modifying the OpenAI model parameters (e.g., temperature) to adjust the agent's response style.

## Notion Conversational Agent

The Notion Conversational Agent is an AI-powered assistant that interacts with your Notion workspace through natural language conversations. This flow performs Notion-related tasks like creating pages, searching for information, and managing content, all through a chat interface.

![Notion Components Toolkit](/img/notion_conversational_agent_tools.png)

To build this flow, you need the following:

- [A Notion App](#set-up-a-notion-app)
- [A Notion account and API key](https://www.notion.so/my-integrations)
- [An OpenAI API key](https://platform.openai.com/account/api-keys)
- Recommended: [Download the Conversation Agent Flow JSON](/files/Conversational_Notion_Agent.json), and then [import the flow](/concepts-flows-import) into Langflow.

### Components used in this flow

- **Chat Input**: Accepts user queries and commands
- **Chat Output**: Displays the agent's responses
- **Language Model**: Processes user input and generates responses with an OpenAI model
- **Tool Calling Agent**: Coordinates the use of various Notion tools based on user input
- **Toolkit**: Combines multiple Notion-specific tools into a single toolkit
- **Notion Tools**: Various components for interacting with Notion, including:
  - List Users
  - List Database Properties
  - List Pages
  - Page Content Viewer
  - Create Page
  - Update Page Property
  - Add Content to Page
  - Search
- **Message History**: Stores conversation history
- **Prompt Template**: Provides system instructions and context for the agent
- **Current Date**: Supplies the current date and time for context

### Run the Conversational Notion Agent

1. Open Langflow and create a new flow.
2. Add the components listed above to your flow canvas, or Download the [Conversation Agent Flow](/files/Conversational_Notion_Agent.json)(Download link) and **Import** the JSON file into Langflow.
3. Connect the components as shown in the flow diagram.
4. Input the Notion and OpenAI API keys in their respective components.
5. To start the flow, click the "Play" button in Langflow.
6. In the chat interface, enter a Notion-related request or question. For example:
   - "Create a new page in my task database"
   - "Search for pages about project management"
   - "List all users in my Notion workspace"
7. The agent will process your request, use the appropriate Notion tools, and provide a response.
8. Continue the conversation, asking follow-up questions or giving new commands as needed.

   <details>
   <summary>Example Conversation</summary>

   ```
   User: List all the users in my Notion workspace.

   Agent: Certainly! I'll use the Notion User List tool to retrieve that information for you. One moment please.

   [Agent uses the Notion User List tool]

   Here's a list of users in your Notion workspace:

   1. John Doe (john@example.com)
   2. Jane Smith (jane@example.com)
   3. Alex Johnson (alex@example.com)

   Is there anything specific you'd like to know about these users or any other Notion-related task I can help you with?

   User: Create a new page in my Projects database with the title "Website Redesign".

   Agent: Certainly! I'd be happy to create a new page in your Projects database with the title "Website Redesign". I'll use the Notion Page Creator tool to do this. First, let me check the properties of your Projects database to ensure we include all necessary information.

   [Agent uses the Notion Database Properties tool]

   I've retrieved the properties for your Projects database. Now, I'll create the new page with the title "Website Redesign". One moment please.

   [Agent uses the Notion Page Creator tool]

   Great news! I've successfully created a new page in your Projects database with the title "Website Redesign". Here are the details:

   - Title: Website Redesign
   - Page ID: [page_id_here]
   - URL: [page_url_here]

   Is there any additional information you'd like me to add to this new project page, such as a description, due date, or assigned team members?

   User: Yes, please add a description: "Redesign company website to improve user experience and modernize the look."

   Agent: Certainly! I'd be happy to add that description to the "Website Redesign" project page. I'll use the Add Content to Page tool to update the page with this information.

   [Agent uses the Add Content to Page tool]

   I've successfully added the description to your "Website Redesign" project page. Here's a summary of what I've done:

   1. Located the "Website Redesign" page using the page ID from our previous interaction.
   2. Added the following content to the page:

      Description: Redesign company website to improve user experience and modernize the look.

   The description has been added as a new text block on the page. Is there anything else you'd like me to add or modify on this project page?
   ```

   </details>

9. Optional: The flow can be customized to meet your team's specific needs.
For example:
   * Adjust the system prompt to change the agent's behavior or context.
   * Add or remove Notion tools based on your specific needs.
   * Modify the OpenAI model parameters (e.g., temperature) to adjust the agent's response style.

## Troubleshooting

If you encounter issues:

1. Ensure all API keys are correctly set and have the necessary permissions.
2. Check that your Notion integration has access to the relevant pages and databases.
3. Verify that all components are properly connected in the flow.
4. Review the Langflow logs for any error messages.

For more advanced usage and integration options, refer to the [Notion API documentation](https://developers.notion.com/) and [Langflow documentation](/).


================================================================================
filename: Components/bundles-novita.mdx
================================================================================

---
title: Novita
slug: /bundles-novita
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Novita** bundle.

## Novita AI

This component generates text using [Novita's language models](https://novita.ai/docs/guides/llm-api).

It can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).

Use the **Language Model** output when you want to use a Novita model as the LLM for another LLM-driven component, such as a **Language Model** or **Smart Function** component.

For more information, see [Language model components](/components-models).

### Novita AI parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| api_key | SecretString | Input parameter. Your Novita AI API Key. |
| model | String | Input parameter. The id of the Novita AI model to use. |
| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Set to 0 for unlimited tokens. |
| temperature | Float | Input parameter. Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.7. |
| top_p | Float | Input parameter. Controls the nucleus sampling. Range: [0.0, 1.0]. Default: 1.0. |
| frequency_penalty | Float | Input parameter. Controls the frequency penalty. Range: [0.0, 2.0]. Default: 0.0. |
| presence_penalty | Float | Input parameter. Controls the presence penalty. Range: [0.0, 2.0]. Default: 0.0. |


================================================================================
filename: Components/bundles-nvidia.mdx
================================================================================

---
title: NVIDIA
slug: /bundles-nvidia
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **NVIDIA** bundle.

## NVIDIA

This component generates text using NVIDIA LLMs.
For more information about NVIDIA LLMs, see the [NVIDIA AI documentation](https://developer.nvidia.com/generative-ai).

### NVIDIA parameters

| Name | Type | Description |
|------|------|-------------|
| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Set to `0` for unlimited tokens. |
| model_name | String | Input parameter. The name of the NVIDIA model to use. Default: `mistralai/mixtral-8x7b-instruct-v0.1`. |
| base_url | String | Input parameter. The base URL of the NVIDIA API. Default: `https://integrate.api.nvidia.com/v1`. |
| nvidia_api_key | SecretString | Input parameter. The NVIDIA API Key for authentication. |
| temperature | Float | Input parameter. Controls randomness in the output. Default: `0.1`. |
| seed | Integer | Input parameter. The seed controls the reproducibility of the job. Default: `1`. |
| model | LanguageModel | Output parameter. An instance of ChatNVIDIA configured with the specified parameters. |

### NVIDIA NIM on WSL2

[NVIDIA NIM (NVIDIA Inference Microservices)](https://docs.nvidia.com/nim/index.html) provides containers to self-host GPU-accelerated inferencing microservices.

You can use the **NVIDIA** component to connect Langflow with NVIDIA NIM on an RTX Windows system with [Windows Subsystem for Linux 2 (WSL2)](https://learn.microsoft.com/en-us/windows/wsl/install) installed.
The following example connects an **NVIDIA** language model component in Langflow to a deployed `mistral-nemo-12b-instruct` NIM on an **RTX Windows system** with **WSL2**.

1. Prepare your system:

    * [NVIDIA NIM WSL2 installed](https://docs.nvidia.com/nim/wsl2/latest/getting-started.html)
    * A NIM container deployed according to the model's instructions

        Prerequisites vary between models.
        For example, to deploy the `mistral-nemo-12b-instruct` NIM, follow the instructions for **Windows on RTX AI PCs (Beta)** on your [model's deployment overview](https://build.nvidia.com/nv-mistralai/mistral-nemo-12b-instruct/deploy?environment=wsl2.md).

    * Windows 11 build 23H2 or later
    * At least 12 GB of RAM

2. Create a flow based on the **Basic Prompting** template.
3. Replace the **OpenAI** model component with the **NVIDIA** component.
4. In the **NVIDIA** component's **Base URL** field, add the URL where your NIM is accessible. If you followed your model's [deployment instructions](https://build.nvidia.com/nv-mistralai/mistral-nemo-12b-instruct/deploy?environment=wsl2.md), the value is `http://localhost:8000/v1`.
5. In the **NVIDIA** component's **NVIDIA API Key** field, add your NVIDIA API Key.
6. Select your model from the **Model Name** field.
7. Open the **Playground** and chat with your **NIM** model.

## NVIDIA Embeddings

The **NVIDIA Embeddings** component generates embeddings using [NVIDIA models](https://docs.nvidia.com).

For more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).

### NVIDIA Embeddings parameters

| Name | Type | Description |
|------|------|-------------|
| model | String | Input parameter. The NVIDIA model to use for embeddings, such as `nvidia/nv-embed-v1`. |
| base_url | String | Input parameter. The base URL for the NVIDIA API. Default: `https://integrate.api.nvidia.com/v1`. |
| nvidia_api_key | SecretString | Input parameter. The API key for authenticating with NVIDIA's service. |
| temperature | Float | Input parameter. The model temperature for embedding generation. Default: `0.1`. |
| embeddings | Embeddings | Output parameter. An `NVIDIAEmbeddings` instance for generating embeddings. |

:::tip Tokenization considerations
Be aware of your embedding model's chunk size limit.
Tokenization errors can occur if your text chunks are too large.
For more information, see [Tokenization errors due to chunk size](/components-processing#chunk-size).
:::

## NVIDIA Rerank

This component finds and reranks documents using the NVIDIA API.

## NVIDIA Retriever Extraction

The **NVIDIA Retriever Extraction** component integrates with the [NVIDIA nv-ingest](https://github.com/NVIDIA/nv-ingest) microservice for data ingestion, processing, and extraction of text files.

The `nv-ingest` service supports multiple extraction methods for PDF, DOCX, and PPTX file types, and includes pre-  and post-processing services like splitting, chunking, and embedding generation. The extractor service's High Resolution mode uses the `nemoretriever-parse` extraction method for better quality extraction from scanned PDF documents. This feature is only available for PDF files.

The **NVIDIA Retriever Extraction** component imports the NVIDIA `Ingestor` client, ingests files with requests to the NVIDIA ingest endpoint, and outputs the processed content as a list of [`Data`](/data-types#data) objects. `Ingestor` accepts additional configuration options for data extraction from other text formats. To configure these options, see the [parameters](#parameters).

:::tip
NVIDIA Retriever Extraction is also known as NV-Ingest and NeMo Retriever Extraction.
:::

### Use the NVIDIA Retriever Extraction component in a flow

The **NVIDIA Retriever Extraction** component accepts `Message` inputs, and then outputs `Data`. The component calls an NVIDIA Ingest microservice's endpoint to ingest a local file and extract the text.

To use the **NVIDIA Retriever Extraction** component in your flow, follow these steps:

1. Prepare your system:

    * An NVIDIA Ingest endpoint. For more information on setting up an NVIDIA Ingest endpoint, see the [NVIDIA Ingest quickstart](https://github.com/NVIDIA/nv-ingest?tab=readme-ov-file#quickstart).

    * The **NVIDIA Retriever Extraction** component requires the installation of additional dependencies to your Langflow environment. To install the dependencies in a virtual environment, run the following commands.

        * If you have the Langflow repository cloned and installed from source:
        ```bash
        source **YOUR_LANGFLOW_VENV**/bin/activate
        uv sync --extra nv-ingest
        uv run langflow run
        ```

        * If you are installing Langflow from the Python Package Index:
        ```bash
        source **YOUR_LANGFLOW_VENV**/bin/activate
        uv pip install --prerelease=allow 'langflow[nv-ingest]'
        uv run langflow run
        ```

2. Add the **NVIDIA Retriever Extraction** component to your flow.
3. In the **Base URL** field, enter the URL of the NVIDIA Ingest endpoint.
You can also store the URL as a [global variable](/configuration-global-variables) to reuse it in multiple components and flows.
4. Click **Select Files** to select a file to ingest.
5. Select which text type to extract from the file: text, charts, tables, images, or infographics.
6. Optional: For PDF files, enable **High Resolution Mode** for better quality extraction from scanned documents.
7. Select whether to split the text into chunks.

    <PartialParams />

8. Click <Icon name="Play" aria-hidden="true" /> **Run component** to ingest the file, and then click **Logs** or <Icon name="TextSearch" aria-hidden="true" /> **Inspect output** to confirm the component ingested the file.
9. To store the processed data in a vector database, add a vector store component to your flow, and then connect the **NVIDIA Retriever Extraction** component's `Data` output to the vector store component's input.

    When you run the flow with a vector store component, the processed data is stored in the vector database.
    You can query your database to retrieve the uploaded data.

    ![NVIDIA Retriever Extraction component flow](/img/nvidia-component-ingest-astra.png)

### NVIDIA Retriever Extraction parameters {#parameters}

The **NVIDIA Retriever Extraction** component has the following parameters.

For more information, see the [NV-Ingest documentation](https://nvidia.github.io/nv-ingest/user-guide/).

#### Inputs

| Name | Display Name | Info |
|------|--------------|------|
| base_url | NVIDIA Ingestion URL | The URL of the NVIDIA Ingestion API. |
| path | Path | File path to process. |
| extract_text | Extract Text | Extract text from documents. Default: `true`. |
| extract_charts | Extract Charts | Extract text from charts. Default: `false`. |
| extract_tables | Extract Tables | Extract text from tables. Default: `true`. |
| extract_images | Extract Images | Extract images from document. Default: `true`. |
| extract_infographics | Extract Infographics | Extract infographics from document. Default: `false`. |
| text_depth | Text Depth | The level at which text is extracted. Options: 'document', 'page', 'block', 'line', 'span'. Default: `page`. |
| split_text | Split Text | Split text into smaller chunks. Default: `true`. |
| chunk_size | Chunk Size | The number of tokens per chunk. Default: `500`. Make sure the chunk size is compatible with your embedding model. For more information, see [Tokenization errors due to chunk size](/components-processing#chunk-size). |
| chunk_overlap | Chunk Overlap | Number of tokens to overlap from previous chunk. Default: `150`. |
| filter_images | Filter Images | Filter images (see advanced options for filtering criteria). Default: `false`. |
| min_image_size | Minimum Image Size Filter | Minimum image width/length in pixels. Default: `128`. |
| min_aspect_ratio | Minimum Aspect Ratio Filter | Minimum allowed aspect ratio (width / height). Default: `0.2`. |
| max_aspect_ratio | Maximum Aspect Ratio Filter | Maximum allowed aspect ratio (width / height). Default: `5.0`. |
| dedup_images | Deduplicate Images | Filter duplicated images. Default: `true`. |
| caption_images | Caption Images | Generate captions for images using the NVIDIA captioning model. Default: `true`. |
| high_resolution | High Resolution (PDF only) | Process PDF in high-resolution mode for better quality extraction from scanned PDF. Default: `false`. |

#### Outputs

The **NVIDIA Retriever Extraction** component outputs a list of [`Data`](/data-types#data) objects where each object contains:
- `text`: The extracted content.
  - For text documents: The extracted text content.
  - For tables and charts: The extracted table/chart content.
  - For images: The image caption.
  - For infographics: The extracted infographic content.
- `file_path`: The source file name and path.
- `document_type`: The type of the document, which can be `text`, `structured`, or `image`.
- `description`: Additional description of the content.

The output varies based on the `document_type`:

- Documents with `document_type: "text"` contain:
  - Raw text content extracted from documents, for example, paragraphs from PDFs or DOCX files.
  - Content stored directly in the `text` field.
  - Content extracted using the `extract_text` parameter.

- Documents with `document_type: "structured"` contain:
  - Text extracted from tables, charts, and infographics and processed to preserve structural information.
  - Content extracted using the `extract_tables`, `extract_charts`, and `extract_infographics` parameters.
  - Content stored in the `text` field after being processed from the `table_content` metadata.

- Documents with `document_type: "image"` contain:
  - Image content extracted from documents.
  - Caption text stored in the `text` field when `caption_images` is enabled.
  - Content extracted using the `extract_images` parameter.

## NVIDIA System-Assist

The **NVIDIA System-Assist** component integrates your flows with NVIDIA G-Assist, enabling interaction with NVIDIA GPU drivers through natural language prompts.
For example, prompt G-Assist with `"What is my current GPU temperature?"` or `"Show me the available GPU memory"` to get information, and then tell G-Assist to modify your GPU settings.
For more information, see the [NVIDIA G-Assist repository](https://github.com/NVIDIA/g-assist).

1. Prepare your system:

    * The **NVIDIA System-Assist** component requires an NVIDIA GPU on a Windows operating system.
    * It uses the `gassist.rise` package, which is installed with all Langflow versions that include this component.

2. Create a flow with a **Chat Input** component, **NVIDIA System-Assist** component, and **Chat Output** components.

    This is a simplified example that uses only three components.
    Depending on your use case, your flow might use more components or different inputs and outputs.

3. Connect the **Chat Input** component to the **NVIDIA System-Assist** component's **Prompt** input.

    The **Prompt** parameter accepts a natural language prompt that is processed by the NVIDIA G-Assist AI Assistant.
    In this example, you'll provide the prompt as chat input.
    You could also enter a prompt directly in the **Prompt** input or connect another input component.

4. Connect the **NVIDIA System-Assist** component's output to the **Chat Output** component.

5. To test the flow, open the **Playground**, and then ask a question about your GPU.
For example, `"What is my current GPU temperature?"`.

    Through the **NVIDIA System-Assist** component, NVIDIA G-Assist queries your GPU based on the prompt, and then prints the response to the **Playground**.

    The component's output is a [`Message`](/data-types#message) containing the NVIDIA G-Assist response.
    The string response with the completed operation result is available in the `text` key in the `Message` object.


================================================================================
filename: Components/bundles-ollama.mdx
================================================================================

---
title: Ollama
slug: /bundles-ollama
---

import Icon from "@site/src/components/icon";

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Ollama** bundle.

For more information about Ollama features and functionality used by Ollama components, see the [Ollama documentation](https://ollama.com/).

## Ollama text generation

This component generates text using [Ollama's language models](https://ollama.com/library).

To use the **Ollama** component in a flow, connect Langflow to your locally running Ollama server and select a model:

1. Add the **Ollama** component to your flow.

2. In the **Base URL** field, enter the address for your locally running Ollama server.

    This value is set as the `OLLAMA_HOST` environment variable in Ollama.
    The default base URL is `http://127.0.0.1:11434`.

3. Once the connection is established, select a model in the **Model Name** field, such as `llama3.2:latest`.

    To refresh the server's list of models, click <Icon name="RefreshCw" aria-hidden="true"/> **Refresh**.

4. Optional: To configure additional parameters, such as temperature or max tokens, click <Icon name="SlidersHorizontal" aria-hidden="true"/> **Controls** in the [component's header menu](/concepts-components#component-menus).

5. Connect the **Ollama** component to other components in the flow, depending on how you want to use the model.

    Language model components can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)). Use the **Language Model** output when you want to use an Ollama model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component. For more information, see [Language model components](/components-models).

    In the following example, the flow uses `LanguageModel` output to use an Ollama model as the LLM for an [**Agent** component](/components-agents).

    ![Ollama component used as the LLM in an agent flow](/img/component-ollama-model.png)

## Ollama Embeddings

The **Ollama Embeddings** component generates embeddings using [Ollama embedding models](https://ollama.com/search?c=embedding).

To use this component in a flow, connect Langflow to your locally running Ollama server and select an embeddings model:

1. Add the **Ollama Embeddings** component to your flow.

2. In the **Ollama Base URL** field, enter the address for your locally running Ollama server.

    This value is set as the `OLLAMA_HOST` environment variable in Ollama.
    The default base URL is `http://127.0.0.1:11434`.

3. Once the connection is established, select a model in the **Ollama Model** field, such as `all-minilm:latest`.

    To refresh the server's list of models, click <Icon name="RefreshCw" aria-hidden="true"/> **Refresh**.

4. Optional: To configure additional parameters, such as temperature or max tokens, click <Icon name="SlidersHorizontal" aria-hidden="true"/> **Controls** in the [component's header menu](/concepts-components#component-menus).
Available parameters depend on the selected model.

5. Connect the **Ollama Embeddings** component to other components in the flow.
For more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).

    This example connects the **Ollama Embeddings** component to generate embeddings for text chunks extracted from a PDF file, and then stores the embeddings and chunks in a Chroma DB vector store.

    ![Ollama Embeddings component in an embedding generation flow](/img/component-ollama-embeddings-chromadb.png)


================================================================================
filename: Components/bundles-openai.mdx
================================================================================

---
title: OpenAI
slug: /bundles-openai
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **OpenAI** bundle.

For more information about OpenAI features and functionality used by OpenAI components, see the [OpenAI documentation](https://platform.openai.com/docs/overview).

## OpenAI text generation

The **OpenAI** component generates text using [OpenAI's language models](https://platform.openai.com/docs/models).

It provides access to the same OpenAI models that are available in the core **Language Model** component, but the **OpenAI** component provides additional parameters for customizing the request to the OpenAI API.

It can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).

Use the **Language Model** output when you want to use a specific OpenAI model configuration as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.

For more information, see [Language model components](/components-models).

### OpenAI text generation parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| api_key | SecretString | Input parameter. Your OpenAI API Key. |
| model | String | Input parameter. The name of the OpenAI model to use. Options include "gpt-3.5-turbo" and "gpt-4". |
| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Set to 0 for unlimited tokens. |
| temperature | Float | Input parameter. Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.7. |
| top_p | Float | Input parameter. Controls the nucleus sampling. Range: [0.0, 1.0]. Default: 1.0. |
| frequency_penalty | Float | Input parameter. Controls the frequency penalty. Range: [0.0, 2.0]. Default: 0.0. |
| presence_penalty | Float | Input parameter. Controls the presence penalty. Range: [0.0, 2.0]. Default: 0.0. |

## OpenAI Embeddings

The **OpenAI Embeddings** component uses [OpenAI embedding models](https://platform.openai.com/docs/guides/embeddings) for embedding generation.

It provides access to the same OpenAI models that are available in the core **Embedding Model** component, but the **OpenAI Embeddings** component provides additional parameters for customizing the request to the OpenAI embedding API.

For more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).

### OpenAI Embeddings parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| OpenAI API Key | String | Input parameter. The API key to use for accessing the OpenAI API. |
| Default Headers | Dict | Input parameter. The default headers for the HTTP requests. |
| Default Query | NestedDict | Input parameter. The default query parameters for the HTTP requests. |
| Allowed Special | List | Input parameter. The special tokens allowed for processing. Default: `[]`. |
| Disallowed Special | List | Input parameter. The special tokens disallowed for processing. Default: `["all"]`. |
| Chunk Size | Integer | Input parameter. The chunk size for processing. Default: `1000`. |
| Client | Any | Input parameter. The HTTP client for making requests. |
| Deployment | String | Input parameter. The deployment name for the model. Default: `text-embedding-3-small`. |
| Embedding Context Length | Integer | Input parameter. The length of embedding context. Default: `8191`. |
| Max Retries | Integer | Input parameter. The maximum number of retries for failed requests. Default: `6`. |
| Model | String | Input parameter. The name of the model to use. Default: `text-embedding-3-small`. |
| Model Kwargs | NestedDict | Input parameter. Additional keyword arguments for the model. |
| OpenAI API Base | String | Input parameter. The base URL of the OpenAI API. |
| OpenAI API Type | String | Input parameter. The type of the OpenAI API. |
| OpenAI API Version | String | Input parameter. The version of the OpenAI API. |
| OpenAI Organization | String | Input parameter. The organization associated with the API key. |
| OpenAI Proxy | String | Input parameter. The proxy server for the requests. |
| Request Timeout | Float | Input parameter. The timeout for the HTTP requests. |
| Show Progress Bar | Boolean | Input parameter. Whether to show a progress bar for processing. Default: `false`. |
| Skip Empty | Boolean | Input parameter. Whether to skip empty inputs. Default: `false`. |
| TikToken Enable | Boolean | Input parameter. Whether to enable TikToken. Default: `true`. |
| TikToken Model Name | String | Input parameter. The name of the TikToken model. |

## See also

* [**Agent** component](/components-agents)
* [LangChain **OpenAI Tools Agent** component](/bundles-langchain#openai-tools-agent)


================================================================================
filename: Components/bundles-openrouter.mdx
================================================================================

---
title: OpenRouter
slug: /bundles-openrouter
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **OpenRouter** bundle.

For more information about OpenRouter features and functionality used by OpenRouter components, see the [OpenRouter documentation](https://openrouter.ai/docs).

## OpenRouter text generation

This component generates text using OpenRouter's unified API for multiple AI models from different providers.

It can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).

Use the **Language Model** output when you want to use an OpenRouter model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.

For more information, see [Language model components](/components-models).

### OpenRouter text generation parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| api_key | SecretString | Input parameter. Your OpenRouter API key for authentication. |
| site_url | String | Input parameter. Your site URL for OpenRouter rankings. |
| app_name | String | Input parameter. Your app name for OpenRouter rankings. |
| provider | String | Input parameter. The AI model provider to use. |
| model_name | String | Input parameter. The specific model to use for chat completion. |
| temperature | Float | Input parameter. Controls randomness in the output. Range: [0.0, 2.0]. Default: 0.7. |
| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. |


================================================================================
filename: Components/bundles-perplexity.mdx
================================================================================

---
title: Perplexity
slug: /bundles-perplexity
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Perplexity** bundle.

For more information about Perplexity features and functionality used by Perplexity components, see the [Perplexity documentation](https://perplexity.ai/).

## Perplexity text generation

This component generates text using Perplexity's language models.

It can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).

Use the **Language Model** output when you want to use a Perplexity model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.

For more information, see [Language model components](/components-models).

### Perplexity text generation parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| model_name | String | Input parameter. The name of the Perplexity model to use. Options include various Llama 3.1 models. |
| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. |
| api_key | SecretString | Input parameter. The Perplexity API Key for authentication. |
| temperature | Float | Input parameter. Controls randomness in the output. Default: 0.75. |
| top_p | Float | Input parameter. The maximum cumulative probability of tokens to consider when sampling. |
| n | Integer | Input parameter. Number of chat completions to generate for each prompt. |


================================================================================
filename: Components/bundles-pgvector.mdx
================================================================================

---
title: pgvector
slug: /bundles-pgvector
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';
import PartialConditionalParams from '@site/docs/_partial-conditional-params.mdx';
import PartialVectorSearchResults from '@site/docs/_partial-vector-search-results.mdx';
import PartialVectorStoreInstance from '@site/docs/_partial-vector-store-instance.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **pgvector** bundle.

## pgvector vector store

The **PGVector** component reads and writes to PostgreSQL vector stores using an instance of [`PGVector`](https://docs.langchain.com/oss/python/integrations/vectorstores/pgvector).

<details>
<summary>About vector store instances</summary>

<PartialVectorStoreInstance />

</details>

<PartialVectorSearchResults />

:::tip
For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).
:::

### pgvector vector store parameters

You can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.

<PartialParams />

<PartialConditionalParams />

For information about accepted values and functionality, see the [PGVector documentation](https://github.com/pgvector/pgvector) or inspect [component code](/concepts-components#component-code).

| Name            | Type         | Description                               |
| --------------- | ------------ | ----------------------------------------- |
| pg_server_url   | SecretString | Input parameter. The PostgreSQL server connection string.       |
| collection_name | String       | Input parameter. The table name for the vector store.           |
| search_query    | String       | Input parameter. The query for similarity search.               |
| ingest_data     | Data         | Input parameter. The data to be ingested into the vector store. |
| embedding       | Embeddings   | Input parameter. The embedding function to use.                 |
| number_of_results | Integer    | Input parameter. The number of results to return in search.     |


================================================================================
filename: Components/bundles-pinecone.mdx
================================================================================

---
title: Pinecone
slug: /bundles-pinecone
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';
import PartialConditionalParams from '@site/docs/_partial-conditional-params.mdx';
import PartialVectorSearchResults from '@site/docs/_partial-vector-search-results.mdx';
import PartialVectorStoreInstance from '@site/docs/_partial-vector-store-instance.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Pinecone** bundle.

## Pinecone vector store

The **Pinecone** component reads and writes to Pinecone vector stores using an instance of `PineconeVectorStore`.

<details>
<summary>About vector store instances</summary>

<PartialVectorStoreInstance />

</details>

<PartialVectorSearchResults />

:::tip
For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).
:::

### Pinecone vector store parameters

You can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.

<PartialParams />

<PartialConditionalParams />

For information about accepted values and functionality, see the [Pinecone documentation](https://docs.pinecone.io/home) or inspect [component code](/concepts-components#component-code).

| Name              | Type         | Description                               |
| ----------------- | ------------ | ----------------------------------------- |
| index_name        | String       | Input parameter. The name of the Pinecone index.                |
| namespace         | String       | Input parameter. The namespace for the index.                   |
| distance_strategy | String       | Input parameter. The strategy for calculating distance between vectors. |
| pinecone_api_key  | SecretString | Input parameter. The API key for Pinecone.                      |
| text_key          | String       | Input parameter. The key in the record to use as text.          |
| search_query      | String       | Input parameter. The query for similarity search.               |
| ingest_data       | Data         | Input parameter. The data to be ingested into the vector store. |
| embedding         | Embeddings   | Input parameter. The embedding function to use.                 |
| number_of_results | Integer      | Input parameter. The number of results to return in search.     |


================================================================================
filename: Components/bundles-qdrant.mdx
================================================================================

---
title: Qdrant
slug: /bundles-qdrant
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';
import PartialConditionalParams from '@site/docs/_partial-conditional-params.mdx';
import PartialVectorSearchResults from '@site/docs/_partial-vector-search-results.mdx';
import PartialVectorStoreInstance from '@site/docs/_partial-vector-store-instance.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Qdrant** bundle.

## Qdrant vector store

The **Qdrant** component reads and writes to Qdrant vector stores using an instance of `QdrantVectorStore`.

<details>
<summary>About vector store instances</summary>

<PartialVectorStoreInstance />

</details>

<PartialVectorSearchResults />

:::tip
For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).
:::

### Qdrant vector store parameters

You can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.

<PartialParams />

<PartialConditionalParams />

For information about accepted values and functionality, see the [Qdrant documentation](https://qdrant.tech/documentation/) or inspect [component code](/concepts-components#component-code).

| Name                 | Type         | Description                               |
| -------------------- | ------------ | ----------------------------------------- |
| collection_name      | String       | Input parameter. The name of the Qdrant collection.             |
| host                 | String       | Input parameter. The Qdrant server host.                        |
| port                 | Integer      | Input parameter. The Qdrant server port.                        |
| grpc_port            | Integer      | Input parameter. The Qdrant gRPC port.                          |
| api_key              | SecretString | Input parameter. The API key for Qdrant.                        |
| prefix               | String       | Input parameter. The prefix for Qdrant.                         |
| timeout              | Integer      | Input parameter. The timeout for Qdrant operations.             |
| path                 | String       | Input parameter. The path for Qdrant.                           |
| url                  | String       | Input parameter. The URL for Qdrant.                            |
| distance_func        | String       | Input parameter. The distance function for vector similarity.   |
| content_payload_key  | String       | Input parameter. The content payload key.                       |
| metadata_payload_key | String       | Input parameter. The metadata payload key.                      |
| search_query         | String       | Input parameter. The query for similarity search.               |
| ingest_data          | Data         | Input parameter. The data to be ingested into the vector store. |
| embedding            | Embeddings   | Input parameter. The embedding function to use.                 |
| number_of_results    | Integer      | Input parameter. The number of results to return in search.     |


================================================================================
filename: Components/bundles-redis.mdx
================================================================================

---
title: Redis
slug: /bundles-redis
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Redis** bundle.

## Redis Chat Memory

The **Redis Chat Memory** component retrieves and stores chat messages using Redis memory storage.

Chat memories are passed between memory storage components as the [`Memory`](/data-types#memory) data type.

For more information about using external chat memory in flows, see the [**Message History** component](/components-helpers#message-history).

### Redis Chat Memory parameters

<PartialParams />

| Name | Display Name | Info |
|------|--------------|------|
| host | hostname | Input parameter. The IP address or hostname. |
| port | port | Input parameter. The Redis Port Number. |
| database | database | Input parameter. The Redis database. |
| username | Username | Input parameter. The Redis username. |
| password | Password | Input parameter. The password for the username. |
| key_prefix | Key prefix | Input parameter. The key prefix. |
| session_id | Session ID | Input parameter. The unique session identifier for the message. |

## Redis vector store

The **Redis** vector store component reads and writes to Redis vector stores using an instance of [`RedisVectorStore`](https://python.langchain.com/docs/integrations/vectorstores/redis/).

<details>
<summary>About vector store instances</summary>

import PartialVectorStoreInstance from '@site/docs/_partial-vector-store-instance.mdx';

<PartialVectorStoreInstance />

</details>

import PartialVectorSearchResults from '@site/docs/_partial-vector-search-results.mdx';

<PartialVectorSearchResults />

:::tip
For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).
:::

### Redis vector store parameters

You can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.

<PartialParams />

import PartialConditionalParams from '@site/docs/_partial-conditional-params.mdx';

<PartialConditionalParams />

For information about accepted values and functionality, see the [Redis documentation](https://redis.io/docs/latest/develop/interact/search-and-query/advanced-concepts/vectors/) or inspect [component code](/concepts-components#component-code).

| Name              | Type         | Description                               |
| ----------------- | ------------ | ----------------------------------------- |
| redis_server_url  | SecretString | Input parameter. The Redis server connection string. |
| redis_index_name  | String       | Input parameter. The name of the Redis index. |
| code              | String       | Input parameter. Additional custom code for Redis, if supported. |
| schema            | String       | Input parameter. The schema for Redis index. |
| ingest_data       | Data         | Input parameter. The data to be ingested into the vector store. |
| search_query      | String       | Input parameter. The query for similarity search. |
| embedding         | Embeddings   | Input parameter. The embedding function to use. |
| number_of_results | Integer      | Input parameter. The number of results to return in search. |


================================================================================
filename: Components/bundles-sambanova.mdx
================================================================================

---
title: SambaNova
slug: /bundles-sambanova
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **SambaNova** bundle.

For more information about SambaNova features and functionality used by SambaNova components, see the [SambaNova Cloud documentation](https://cloud.sambanova.ai/).

## SambaNova text generation

This component generates text using SambaNova LLMs.

It can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).

Use the **Language Model** output when you want to use a SambaNova model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.

For more information, see [Language model components](/components-models).

### SambaNova text generation parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| sambanova_url | String | Input parameter. Base URL path for API requests. Default: `https://api.sambanova.ai/v1/chat/completions`. |
| sambanova_api_key | SecretString | Input parameter. Your SambaNova API Key. |
| model_name | String | Input parameter. The name of the SambaNova model to use. Options include various Llama models. |
| max_tokens | Integer | Input parameter. The maximum number of tokens to generate. Set to 0 for unlimited tokens. |
| temperature | Float | Input parameter. Controls randomness in the output. Range: [0.0, 1.0]. Default: 0.07. |


================================================================================
filename: Components/bundles-searchapi.mdx
================================================================================

---
title: SearchApi
slug: /bundles-searchapi
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **SearchApi** bundle.

For more information, see the [SearchApi documentation](https://www.searchapi.io/docs/google).

## SearchApi web search

This component calls the SearchApi API to run Google, Bing, and DuckDuckGo web searches.

It returns a list of search results as a [`DataFrame`](/data-types#dataframe).

### SearchApi web search parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| engine | String | Input parameter. The search engine to use. Default: `google`. |
| api_key | SecretString | Input parameter. The API key for authenticating with SearchApi. |
| input_value | String | Input parameter. The search query or input for the API call. |
| max_results | Integer | Input parameter. The maximum number of search results to return. Default: `5`. |
| max_snippet_length | Integer | Input parameter. The maximum length of the snippet to return. Default: `100`. |
| search_params | Dict | Input parameter. Additional key-value pairs to customize the request. |

## See also

* [**Web Search** component](/components-data#web-search)
* [**Google** bundle](/bundles-google)
* [**Bing** bundle](/bundles-bing)
* [**DuckDuckGo** bundle](/bundles-duckduckgo)


================================================================================
filename: Components/bundles-serper.mdx
================================================================================

---
title: Serper
slug: /bundles-serper
---

import Icon from "@site/src/components/icon";

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Serper** bundle.

For more information, see the [Serper documentation](https://serper.dev/).

## Serper Google Search API

This component allows you to call the Serper.dev Google Search API.

It returns a list of search results as a [`DataFrame`](/data-types#dataframe).

### Google Serper API parameters

| Name | Type | Description |
|------|------|-------------|
| **Serper API Key** (`serper_api_key`) | SecretString | Input parameter. An API key for Serper.dev API authentication. |
| **Input Value** (`input_value`) | String | Input parameter. The search query input. |
| **Number of Results** (`k`) | Integer | Input parameter. The number of search results to return. |

## See also

* [**Web Search** component](/components-data#web-search)
* [**Google** bundle](/bundles-google)
* [**Bing** bundle](/bundles-bing)
* [**DuckDuckGo** bundle](/bundles-duckduckgo)


================================================================================
filename: Components/bundles-supabase.mdx
================================================================================

---
title: Supabase
slug: /bundles-supabase
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';
import PartialConditionalParams from '@site/docs/_partial-conditional-params.mdx';
import PartialVectorSearchResults from '@site/docs/_partial-vector-search-results.mdx';
import PartialVectorStoreInstance from '@site/docs/_partial-vector-store-instance.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Supabase** bundle.

## Supabase vector store

The **Supabase** component reads and writes to Supabase vector stores using an instance of `SupabaseVectorStore`.

<details>
<summary>About vector store instances</summary>

<PartialVectorStoreInstance />

</details>

<PartialVectorSearchResults />

:::tip
For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).
:::

### Supabase vector store parameters

You can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.

<PartialParams />

<PartialConditionalParams />

For information about accepted values and functionality, see the [Supabase documentation](https://supabase.com/docs/guides/ai) or inspect [component code](/concepts-components#component-code).

| Name                | Type         | Description                               |
| ------------------- | ------------ | ----------------------------------------- |
| supabase_url        | String       | Input parameter. The URL of the Supabase instance.              |
| supabase_service_key| SecretString | Input parameter. The service key for Supabase authentication.   |
| table_name          | String       | Input parameter. The name of the table in Supabase.             |
| query_name          | String       | Input parameter. The name of the query to use.                  |
| search_query        | String       | Input parameter. The query for similarity search.               |
| ingest_data         | Data         | Input parameter. The data to be ingested into the vector store. |
| embedding           | Embeddings   | Input parameter. The embedding function to use.                 |
| number_of_results   | Integer      | Input parameter. The number of results to return in search.     |


================================================================================
filename: Components/bundles-upstash.mdx
================================================================================

---
title: Upstash
slug: /bundles-upstash
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';
import PartialConditionalParams from '@site/docs/_partial-conditional-params.mdx';
import PartialVectorSearchResults from '@site/docs/_partial-vector-search-results.mdx';
import PartialVectorStoreInstance from '@site/docs/_partial-vector-store-instance.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Upstash** bundle.

## Upstash vector store

The **Upstash** component reads and writes to Upstash vector stores using an instance of `UpstashVectorStore`.

<details>
<summary>About vector store instances</summary>

<PartialVectorStoreInstance />

</details>

<PartialVectorSearchResults />

:::tip
For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).
:::

### Upstash vector store parameters

You can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.

<PartialParams />

<PartialConditionalParams />

For information about accepted values and functionality, see the [Upstash documentation](https://upstash.com/docs/introduction) or inspect [component code](/concepts-components#component-code).

| Name            | Type         | Description                               |
| --------------- | ------------ | ----------------------------------------- |
| index_url       | String       | Input parameter. The URL of the Upstash index.              |
| index_token     | SecretString | Input parameter. The token for the Upstash index.           |
| text_key        | String       | Input parameter. The key in the record to use as text.      |
| namespace       | String       | Input parameter. The namespace for the index.                   |
| search_query    | String       | Input parameter. The query for similarity search.               |
| metadata_filter | String       | Input parameter. Filter documents by metadata.             |
| ingest_data     | Data         | Input parameter. The data to be ingested into the vector store. |
| embedding       | Embeddings   | Input parameter. The embedding function to use.      |
| number_of_results | Integer    | Input parameter. The number of results to return in search.     |


================================================================================
filename: Components/bundles-vectara.mdx
================================================================================

---
title: Vectara
slug: /bundles-vectara
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';
import PartialConditionalParams from '@site/docs/_partial-conditional-params.mdx';
import PartialVectorSearchResults from '@site/docs/_partial-vector-search-results.mdx';
import PartialVectorStoreInstance from '@site/docs/_partial-vector-store-instance.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Vectara** bundle.

## Vectara vector store

The **Vectara** component reads and writes to Vectara vector stores using an instance of `Vectara` vector store.

<details>
<summary>About vector store instances</summary>

<PartialVectorStoreInstance />

</details>

<PartialVectorSearchResults />

### Vectara vector store parameters

You can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.

<PartialParams />

<PartialConditionalParams />

For information about accepted values and functionality, see the [Vectara documentation](https://docs.vectara.com/docs/) or inspect [component code](/concepts-components#component-code).

| Name             | Type         | Description                               |
| ---------------- | ------------ | ----------------------------------------- |
| vectara_customer_id | String     | Input parameter. The Vectara customer ID.                       |
| vectara_corpus_id   | String     | Input parameter. The Vectara corpus ID.                         |
| vectara_api_key   | SecretString | Input parameter. The Vectara API key.                           |
| embedding         | Embeddings   | Input parameter. The embedding function to use (optional).      |
| ingest_data       | List[Document/Data] | Input parameter. The data to be ingested into the vector store. |
| search_query      | String       | Input parameter. The query for similarity search.               |
| number_of_results | Integer      | Input parameter. The number of results to return in search.     |

## Vectara RAG

This component enables Vectara's full end-to-end RAG capabilities with reranking options.

This component uses a `Vectara` vector store to execute the vector search and reranking functions, and then outputs an **Answer** string in [`Message`](/data-types#message) format.

<details>
<summary>About vector store instances</summary>

<PartialVectorStoreInstance />

</details>


================================================================================
filename: Components/bundles-vertexai.mdx
================================================================================

---
title: Vertex AI
slug: /bundles-vertexai
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Vertex AI** bundle.

For more information about Vertex AI features and functionality used by Vertex AI components, see the [Vertex AI documentation](https://cloud.google.com/vertex-ai).

For other Google components, see the [**Google** bundle](/bundles-google).

## Vertex AI text generation

The **Vertex AI** component generates text using Google Vertex AI models.

It can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).

Use the **Language Model** output when you want to use a Vertex AI model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.

For more information, see [Language model components](/components-models).

### Vertex AI text generation parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| credentials | File | Input parameter. JSON credentials file. Leave empty to fall back to environment variables. File type: JSON. For more information, see [Create a service account for Vertex AI authentication](#vertexai-credentials). |
| model_name | String | Input parameter. The name of the Vertex AI model to use. Default: "gemini-1.5-pro". |
| project | String | Input parameter. The project ID. |
| location | String | Input parameter. The location for the Vertex AI API. Default: "us-central1". |
| max_output_tokens | Integer | Input parameter. The maximum number of tokens to generate. |
| max_retries | Integer | Input parameter. Maximum number of retries for API calls. Default: 1. |
| temperature | Float | Input parameter. Controls randomness in the output. Default: 0.0. |
| top_k | Integer | Input parameter. The number of highest-probability vocabulary tokens to keep for top-k-filtering. |
| top_p | Float | Input parameter. The cumulative probability of all highest-probability vocabulary tokens that are kept for nucleus sampling. Default: 0.95. |
| verbose | Boolean | Input parameter. Whether to print verbose output. Default: `false`. |

For more information about Vertex AI text generation parameters, see the [Vertex AI content generation parameters documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/content-generation-parameters#token-sampling-parameters).

## Vertex AI Embeddings

The **Vertex AI Embeddings** component is a wrapper around the [Google Vertex AI Embeddings API](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings) for embedding generation.

For more information about using embedding model components in flows, see [Embedding model components](/components-embedding-models).

### Vertex AI Embeddings parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| credentials | Credentials | Input parameter. JSON credentials file. Leave empty to fall back to environment variables. File type: JSON. For more information, see [Create a service account for Vertex AI authentication](#vertexai-credentials). |
| location | String | Input parameter. The default location to use when making API calls. Default: `us-central1`. |
| max_output_tokens | Integer | Input parameter. The token limit determines the maximum amount of text output from one prompt. Default: `128`. |
| model_name | String | Input parameter. The name of the Vertex AI large language model. Default: `text-bison`. |
| project | String | Input parameter. The default GCP project to use when making Vertex API calls. |
| request_parallelism | Integer | Input parameter. The amount of parallelism allowed for requests issued to Vertex AI models. Default: `5`. |
| temperature | Float | Input parameter. Tunes the degree of randomness in text generations. Should be a non-negative value. Default: `0`. |
| top_k | Integer | Input parameter. How the model selects tokens for output. The next token is selected from the top `k` tokens. Default: `40`. |
| top_p | Float | Input parameter. Tokens are selected from the most probable to least until the sum of their probabilities exceeds the top `p` value. Default: `0.95`. |
| tuned_model_name | String | Input parameter. The name of a tuned model. If provided, `model_name` is ignored. |
| verbose | Boolean | Input parameter. This parameter controls the level of detail in the output. When set to `true`, it prints internal states of the chain to help debug. Default: `false`. |
| embeddings | Embeddings | Output parameter. An instance for generating embeddings using Vertex AI. |

## Create a service account for Vertex AI authentication {#vertexai-credentials}

The **Vertex AI Embeddings** and **Vertex AI** components authenticate with the [Google Vertex AI API](https://console.cloud.google.com/marketplace/product/google/aiplatform.googleapis.com) using a service account JSON file.

These components require that you provide a service account JSON file in the **Credentials** parameter (`credentials`).
You can either provide the file directly in your component's settings or you can use a [`GOOGLE_APPLICATION_CREDENTIALS` environment variable](https://cloud.google.com/docs/authentication/application-default-credentials) to reference the path to your JSON file.

The following steps explain how to create a service account for Langflow, create a service account JSON key file, and then use the JSON key file in Langflow:

1. If you haven't done so already, [enable the Vertex AI APIs in your Google Cloud project](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).

2. Create a [service account](https://cloud.google.com/iam/docs/service-account-overview) in your Google Cloud project.

    It is recommended that you [create a custom service account for Vertex AI](https://cloud.google.com/vertex-ai/docs/general/custom-service-account) because Langflow uses this account to access the Vertex AI APIs.

3. Assign the **Vertex AI Service Agent** role to the service account.

    This role allows Langflow to access Vertex AI resources.
    For more information, see [Vertex AI access control with IAM](https://cloud.google.com/vertex-ai/docs/general/access-control).

4. [Create a service account key](https://cloud.google.com/iam/docs/keys-create-delete#console) in JSON format for your Langflow Vertex AI service account.

    When you click **Create**, the JSON key file is downloaded.

5. In Langflow, create or open a flow with the **Vertex AI** or **Vertex AI Embeddings** component.

6. In the component's **Credentials** field, do one of the following:

    * Select your service account JSON file directly.

    * Leave the **Credentials** field empty if you want to pull the key file from an environment variable.
    Then, you must provide the path to your JSON file in a `GOOGLE_APPLICATION_CREDENTIALS` environment variable that is set somewhere relative to your Langflow instance.
    For example, you can set the environment variable in your terminal, your Langflow `.env` file, or otherwise in the environment where your Langflow server or application runs.

    ![Select a local JSON key file for the Vertex AI component](/img/configure-vertex-ai-credentials-in-langflow.gif)

7. Run your flow to verify that Langflow can use the service account credentials successfully.

    For example, in a flow with **Chat Input**, **Chat Output**, and **Vertex AI** components, you can open the **Playground** and try chatting with the LLM.
    If the LLM responds, then the credentials are configured correctly.


================================================================================
filename: Components/bundles-weaviate.mdx
================================================================================

---
title: Weaviate
slug: /bundles-weaviate
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';
import PartialConditionalParams from '@site/docs/_partial-conditional-params.mdx';
import PartialVectorSearchResults from '@site/docs/_partial-vector-search-results.mdx';
import PartialVectorStoreInstance from '@site/docs/_partial-vector-store-instance.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Weaviate** bundle.

## Weaviate vector store

The **Weaviate** component reads and writes to Weaviate vector stores using an instance of `Weaviate` vector store.

<details>
<summary>About vector store instances</summary>

<PartialVectorStoreInstance />

</details>

<PartialVectorSearchResults />

:::tip
For a tutorial using a vector database in a flow, see [Create a vector RAG chatbot](/chat-with-rag).
:::

### Weaviate parameters

You can inspect a vector store component's parameters to learn more about the inputs it accepts, the features it supports, and how to configure it.

<PartialParams />

<PartialConditionalParams />

For information about accepted values and functionality, see the [Weaviate documentation](https://weaviate.io/developers/weaviate) or inspect [component code](/concepts-components#component-code).

| Name          | Type         | Description                               |
|---------------|--------------|-------------------------------------------|
| weaviate_url  | String | Input parameter. The default instance URL. |
| api_key       | SecretString | Input parameter. The optional API key for authentication. |
| index_name    | String | Input parameter. The optional index name. |
| text_key      | String | Input parameter. The default text extraction key. |
| input         | Data or DataFrame | Input parameter. The document or record.  |
| cache_vector_store | Cache Vector Store | Input parameter. If `true`, the component caches the vector store in memory for faster reads. Default: Enabled (`true`). |
| embedding     | Embeddings | Input parameter. Connect an [embedding model component](/components-embedding-models). |
| number_of_results | Integer | Input parameter. The number of search results to return. Default: `4`. |
| search_by_text | Boolean | Input parameter. Indicates whether to search by text. Default: Disabled (`false`). |


================================================================================
filename: Components/bundles-wikipedia.mdx
================================================================================

---
title: Wikipedia
slug: /bundles-wikipedia
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **Wikipedia** bundle.

:::tip
Wikipedia components are useful [tools for agents](/agents-tools) that need reliable sources for information retrieval, such as tutoring chatbots or generic research assistants.

Rather than broad internet searches or recall from internal model data, these components provide specific data returned from Wikipedia API requests.
:::

## Wikidata API

This component performs a similarity search using the [Wikidata API](https://www.wikidata.org/wiki/Wikidata:REST_API).

It accepts a string text query, and it outputs a [`DataFrame`](/data-types#dataframe) containing the search results.

## Wikipedia API

This component searches and retrieves information from Wikipedia with the [WikiMedia API](https://www.mediawiki.org/wiki/API:Main_page) search endpoints, and then outputs the response as a [`DataFrame`](/data-types#dataframe).

### Wikipedia API parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| input_value | String | Input parameter. The search query input. |
| lang | String | Input parameter. The language code for Wikipedia. Default: `en`. |
| k | Integer | Input parameter. The number of results to return. |
| load_all_available_meta | Boolean | Input parameter. Whether to load all available metadata. |
| doc_content_chars_max | Integer | Input parameter. The maximum number of characters for document content. |

## See also

* [**API Request** component](/components-data#api-request)


================================================================================
filename: Components/bundles-xai.mdx
================================================================================

---
title: xAI
slug: /bundles-xai
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) contain custom components that support specific third-party integrations with Langflow.

This page describes the components that are available in the **xAI** bundle.

For more information about xAI features and functionality used by xAI components, see the [xAI documentation](https://x.ai/).

## xAI text generation

The **xAI** component generates text using xAI models like [Grok](https://x.ai/grok).

It can output either a **Model Response** ([`Message`](/data-types#message)) or a **Language Model** ([`LanguageModel`](/data-types#languagemodel)).

Use the **Language Model** output when you want to use an xAI model as the LLM for another LLM-driven component, such as an **Agent** or **Smart Function** component.

For more information, see [Language model components](/components-models).

### xAI text generation parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| max_tokens | Integer | Input parameter. Maximum number of tokens to generate. Set to `0` for unlimited. Range: `0-128000`. |
| model_kwargs | Dictionary | Input parameter. Additional keyword arguments for the model. |
| json_mode | Boolean | Input parameter. If `true`, outputs JSON regardless of passing a schema. |
| model_name | String | Input parameter. The xAI model to use. Default: `grok-2-latest`. |
| base_url | String | Input parameter. Base URL for API requests. Default: `https://api.x.ai/v1`. |
| api_key | SecretString | Input parameter. Your xAI API key for authentication. |
| temperature | Float | Input parameter. Controls randomness in the output. Range: `[0.0, 2.0]`. Default: `0.1`. |
| seed | Integer | Input parameter. Controls reproducibility of the job. |


================================================================================
filename: Components/components-agents.mdx
================================================================================

---
title: Agents
slug: /components-agents
---

import PartialAgentsWork from '@site/docs/_partial-agents-work.mdx';

Langflow's **Agent** and **MCP Tools** components are critical for building agent flows.
These components define the behavior and capabilities of AI agents in your flows.

<PartialAgentsWork />

## Examples of agent flows

For examples of flows using the **Agent** and **MCP Tools** components, see the following:

* [Langflow quickstart](/get-started-quickstart): Start with the **Simple Agent** template, modify its tools, and then learn how to use an agent flow in an application.

    The **Simple Agent** template creates a basic agent flow with an **Agent** component that can use two other Langflow components as tools.
    The LLM specified in the **Agent** component's settings can use its own built-in functionality as well as the functionality provided by the connected tools when generating responses.

* [Use an agent as a tool](/agents-tools#use-an-agent-as-a-tool): Create a multi-agent flow.

* [Use Langflow as an MCP client](/mcp-client) and [Use Langflow as an MCP server](/mcp-server): Use the **Agent** and **MCP Tools** components to implement the Model Context Protocol (MCP) in your flows.

## Agent component {#agent-component}

The **Agent** component is the primary agent actor in your agent flows.
This component uses an LLM integration to respond to input, such as a chat message or file upload.

The agent can use the tools already available in the base LLM as well as additional tools that you connect to the **Agent** component's **Tools** port.
You can connect any Langflow component as a tool, including other **Agent** components and MCP servers through the [**MCP Tools** component](#mcp-connection).

For more information about using this component, see [Use Langflow agents](/agents).

## MCP Tools component {#mcp-connection}

The **MCP Tools** component connects to a Model Context Protocol (MCP) server and exposes the MCP server's functions as tools for Langflow agents to use to respond to input.

In addition to publicly available MCP servers and your own custom-built MCP servers, you can connect Langflow MCP servers, which allow your agent to use your Langflow flows as tools.
To do this, use the **MCP Tools** component's [SSE mode](/mcp-client#mcp-sse-mode) to connect to your Langflow project's MCP server at the `/api/v1/mcp/sse` endpoint.

For more information, see [Use Langflow as an MCP client](/mcp-client) and [Use Langflow as an MCP server](/mcp-server).

<details>
<summary>Earlier versions of the MCP Tools component</summary>

* In Langflow version 1.5, the **MCP Connection** component was renamed to the **MCP Tools** component.
* In Langflow version 1.3, the **MCP Tools (stdio)** and **MCP Tools (SSE)** components were removed and replaced by the unified **MCP Connection** component, which was later renamed to **MCP Tools**.

</details>

## See also

* [**Message History** component](/components-helpers#message-history)
* [Store chat memory](/memory#store-chat-memory)
* [Bundles](/components-bundle-components)
* [Legacy LangChain components](/bundles-langchain#legacy-langchain-components)


================================================================================
filename: Components/components-bundles.mdx
================================================================================

---
title: About bundles
slug: /components-bundle-components
---

import Icon from "@site/src/components/icon";
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

Bundles contain custom components that support specific third-party integrations with Langflow.
You add them to your flows and configure them in the same way as Langflow's core components.

To browse bundles, click <Icon name="Blocks" aria-hidden="true" /> **Bundles** in the visual editor.

## Bundle maintenance and documentation

Many bundled components are developed by third-party contributors to the Langflow codebase.

Some providers contribute documentation with their bundles, whereas others document their bundles in their own documentation.
Some bundles have no documentation.

To find documentation for a specific bundled component, browse the Langflow docs and your provider's documentation.
If available, you can also find links to relevant documentation, such as API endpoints, through the component itself:

1. Click the component to expose the [component's header menu](/concepts-components#component-menus).
2. Click <Icon name="Ellipsis" aria-hidden="true" /> **More**.
3. Select **Docs**.

The Langflow documentation focuses on using bundles within flows.
For that reason, it focuses on the Langflow-specific configuration steps for bundled components.
For information about provider-specific features or APIs, see the provider's documentation.

## Component parameters

import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<PartialParams />

## Core components and bundles

:::tip
The Langflow documentation doesn't list all bundles or components in bundles.
For the most accurate and up-to-date list of bundles and components for your version of Langflow, check <Icon name="Blocks" aria-hidden="true" /> **Bundles** in the visual editor.

If you can't find a component that you used in an earlier version of Langflow, it may have been removed or marked as a [legacy component](#legacy-bundles).
:::

Langflow offers generic <Icon name="Component" aria-hidden="true" /> **Core components** in addition to third-party, provider-specific bundles.

If you are looking for a specific service or integration, you can <Icon name="Search" aria-hidden="true" /> **Search** components in the visual editor.

If all else fails, you can always create your own [custom components](/components-custom-components).

## Legacy bundles

import PartialLegacy from '@site/docs/_partial-legacy.mdx';

<PartialLegacy />

The following bundles include only legacy components.

### CrewAI bundle

Replace the following legacy CrewAI components with other agentic components, such as the [**Agent** component](/components-agents).

<details>
<summary>CrewAI Agent</summary>

This component represents CrewAI agents, allowing for the creation of specialized AI agents with defined roles goals and capabilities within a crew.
For more information, see the [CrewAI agents documentation](https://docs.crewai.com/core-concepts/Agents/).

This component accepts the following parameters:

| Name | Display Name | Info |
|------|--------------|------|
| role | Role | Input parameter. The role of the agent. |
| goal | Goal | Input parameter. The objective of the agent. |
| backstory | Backstory | Input parameter. The backstory of the agent. |
| tools | Tools | Input parameter. The tools at the agent's disposal. |
| llm | Language Model | Input parameter. The language model that runs the agent. |
| memory | Memory | Input parameter. This determines whether the agent should have memory or not. |
| verbose | Verbose | Input parameter. This enables verbose output. |
| allow_delegation | Allow Delegation | Input parameter. This determines whether the agent is allowed to delegate tasks to other agents. |
| allow_code_execution | Allow Code Execution | Input parameter. This determines whether the agent is allowed to execute code. |
| kwargs | kwargs | Input parameter. Additional keyword arguments for the agent. |
| output | Agent | Output parameter. The constructed CrewAI Agent object. |

</details>

<details>
<summary>CrewAI Hierarchical Crew, CrewAI Hierarchical Task</summary>

The **CrewAI Hierarchical Crew** component represents a group of agents managing how they should collaborate and the tasks they should perform in a hierarchical structure. This component allows for the creation of a crew with a manager overseeing the task execution.
For more information, see the [CrewAI hierarchical crew documentation](https://docs.crewai.com/how-to/Hierarchical/).

It accepts the following parameters:

| Name | Display Name | Info |
|------|--------------|------|
| agents | Agents | Input parameter. The list of Agent objects representing the crew members. |
| tasks | Tasks | Input parameter. The list of HierarchicalTask objects representing the tasks to be executed. |
| manager_llm | Manager LLM | Input parameter. The language model for the manager agent. |
| manager_agent | Manager Agent | Input parameter. The specific agent to act as the manager. |
| verbose | Verbose | Input parameter. This enables verbose output for detailed logging. |
| memory | Memory | Input parameter. The memory configuration for the crew. |
| use_cache | Use Cache | Input parameter. This enables caching of results. |
| max_rpm | Max RPM | Input parameter. This sets the maximum requests per minute. |
| share_crew | Share Crew | Input parameter. This determines if the crew information is shared among agents. |
| function_calling_llm | Function Calling LLM | Input parameter. The language model for function calling. |
| crew | Crew | Output parameter. The constructed Crew object with hierarchical task execution. |

</details>

<details>
<summary>CrewAI Sequential Crew, CrewAI Sequential Task</summary>

The **CrewAI Sequential Crew** component represents a group of agents with tasks that are executed sequentially. This component allows for the creation of a crew that performs tasks in a specific order.
For more information, see the [CrewAI sequential crew documentation](https://docs.crewai.com/how-to/Sequential/).

It accepts the following parameters:

| Name | Display Name | Info |
|------|--------------|------|
| tasks | Tasks | Input parameter. The list of SequentialTask objects representing the tasks to be executed. |
| verbose | Verbose | Input parameter. This enables verbose output for detailed logging. |
| memory | Memory | Input parameter. The memory configuration for the crew. |
| use_cache | Use Cache | Input parameter. This enables caching of results. |
| max_rpm | Max RPM | Input parameter. This sets the maximum requests per minute. |
| share_crew | Share Crew | Input parameter. This determines if the crew information is shared among agents. |
| function_calling_llm | Function Calling LLM | Input parameter. The language model for function calling. |
| crew | Crew | Output parameter. The constructed Crew object with sequential task execution. |

</details>

<details>
<summary>CrewAI Sequential Task Agent</summary>

This component creates a CrewAI Task and its associated agent allowing for the definition of sequential tasks with specific agent roles and capabilities.
For more information, see the [CrewAI sequential agents documentation](https://docs.crewai.com/how-to/Sequential/).

It accepts the following parameters:

| Name | Display Name | Info |
|------|--------------|------|
| role | Role | Input parameter. The role of the agent. |
| goal | Goal | Input parameter. The objective of the agent. |
| backstory | Backstory | Input parameter. The backstory of the agent. |
| tools | Tools | Input parameter. The tools at the agent's disposal. |
| llm | Language Model | Input parameter. The language model that runs the agent. |
| memory | Memory | Input parameter. This determines whether the agent should have memory or not. |
| verbose | Verbose | Input parameter. This enables verbose output. |
| allow_delegation | Allow Delegation | Input parameter. This determines whether the agent is allowed to delegate tasks to other agents. |
| allow_code_execution | Allow Code Execution | Input parameter. This determines whether the agent is allowed to execute code. |
| agent_kwargs | Agent kwargs | Input parameter. The additional kwargs for the agent. |
| task_description | Task Description | Input parameter. The descriptive text detailing the task's purpose and execution. |
| expected_output | Expected Task Output | Input parameter. The clear definition of the expected task outcome. |
| async_execution | Async Execution | Input parameter. Boolean flag indicating asynchronous task execution. |
| previous_task | Previous Task | Input parameter. The previous task in the sequence for chaining. |
| task_output | Sequential Task | Output parameter. The list of SequentialTask objects representing the created tasks. |

</details>

### Embeddings bundle

* **Embedding Similarity**: Replaced by built-in similarity search functionality in vector store components.
* **Text Embedder**: Replaced by the embedding model components.

### Vector Stores bundle

This bundle contains only the legacy **Local DB** component.
All other vector store components can be found within their respective provider-specific bundles, such as the [**DataStax** bundle](/bundles-datastax).

<details>
<summary>Local DB</summary>

Replace the **Local DB** component with the **Chroma DB** vector store component (in the **Chroma** bundle) or another vector store component.

The **Local DB** component reads and writes to a persistent, in-memory Chroma DB instance intended for use with Langflow.
It has separate modes for reads and writes, automatic collection management, and default persistence in your Langflow cache directory.

Set the **Mode** parameter to reflect the operation you want the component to perform, and then configure the other parameters accordingly.
Some parameters are only available for one mode.

<Tabs>
<TabItem value="ingest" label="Ingest">

To create or write to your local Chroma vector store, use **Ingest** mode.

The following parameters are available in **Ingest** mode:

| Name | Type | Description |
|------|------|-------------|
| **Name Your Collection** (`collection_name`) | String | Input parameter. The name for your Chroma vector store collection. Default: `langflow`. Only available in **Ingest** mode. |
| **Persist Directory** (`persist_directory`) | String | Input parameter. The base directory where you want to create and persist the vector store. If you use the **Local DB** component in multiple flows or to create multiple collections, collections are stored at `$PERSISTENT_DIRECTORY/vector_stores/$COLLECTION_NAME`. If not specified, the default location is your Langflow configuration directory. For more information, see [Memory management options](/memory). |
| **Embedding** (`embedding`) | Embeddings | Input parameter. The embedding function to use for the vector store. |
| **Allow Duplicates** (`allow_duplicates`) | Boolean | Input parameter. If `true` (default), writes don't check for existing duplicates in the collection, allowing you to store multiple copies of the same content. If `false`, writes won't add documents that match existing documents already present in the collection. If `false`, it can strictly enforce deduplication by searching the entire collection or only search the number of records, specified in `limit`. Only available in **Ingest** mode. |
| **Ingest Data** (`ingest_data`) | Data or DataFrame | Input parameter. The records to write to the collection. Records are embedded and indexed for semantic search. Only available in **Ingest** mode. |
| **Limit** (`limit`) | Integer | Input parameter. Limit the number of records to compare when **Allow Duplicates** is `false`. This can help improve performance when writing to large collections, but it can result in some duplicate records. Only available in **Ingest** mode. |

</TabItem>
<TabItem value="retrieve" label="Retrieve">

To read from your local Chroma vector store, use **Retrieve** mode.

The following parameters are available in **Retrieve** mode:

| Name | Type | Description |
|------|------|-------------|
| **Persist Directory** (`persist_directory`) | String | Input parameter. The base directory where you want to create and persist the vector store. If you use the **Local DB** component in multiple flows or to create multiple collections, collections are stored at `$PERSISTENT_DIRECTORY/vector_stores/$COLLECTION_NAME`. If not specified, the default location is your Langflow configuration directory. For more information, see [Memory management options](/memory). |
| **Existing Collections** (`existing_collections`) | String | Input parameter. Select a previously-created collection to search. Only available in **Retrieve** mode. |
| **Embedding** (`embedding`) | Embeddings | Input parameter. The embedding function to use for the vector store. |
| **Search Type** (`search_type`) | String | Input parameter. The type of search to perform, either `Similarity` or `MMR`. Only available in **Retrieve** mode. |
| **Search Query** (`search_query`) | String | Input parameter. Enter a query for similarity search. Only available in **Retrieve** mode. |
| **Number of Results** (`number_of_results`) | Integer | Input parameter. Number of search results to return. Default: 10. Only available in **Retrieve** mode. |

</TabItem>
</Tabs>

</details>

### Zep bundle

<details>
<summary>Zep Chat Memory</summary>

The **Zep Chat Memory** component is a legacy component.
Replace this component with the [**Message History** component](/components-helpers#message-history).

This component creates a `ZepChatMessageHistory` instance, enabling storage and retrieval of chat messages using Zep, a memory server for LLMs.

It accepts the following parameters:

| Name          | Type          | Description                                               |
|---------------|---------------|-----------------------------------------------------------|
| url           | MessageText   | Input parameter. The URL of the Zep instance. Required. |
| api_key       | SecretString  | Input parameter. The API Key for authentication with the Zep instance. |
| api_base_path | Dropdown      | Input parameter. The API version to use. Options include api/v1 or api/v2. |
| session_id    | MessageText   | Input parameter. The unique identifier for the chat session. Optional. |
| message_history | BaseChatMessageHistory  | Output parameter. An instance of ZepChatMessageHistory for the session. |

</details>

## See also

* [LangWatch observability and evaluation](/integrations-langwatch)



<!-- Not documented but in Langflow as of 1.5.11 -->
<!--
* AgentQL
* Confluence
* Firecrawl
* Git
* Home Assistant
* Jigsawstack
* LangWatch (Mentioned on integrations-langwatch.mdx)
* Needle
* Not Diamond
* Olivya
* Scrape Graph AI
* SerpApi (Mentioned on components-tools.mdx)
* Tavily (Mentioned on components-tools.mdx)
* Twelve Labs (Mentioned on concepts-file-management.mdx and components-data.mdx)
* Unstructured
* WolframAlpha
* yfinance/Yahoo! Search (Mentioned on components-tools.mdx)
* YouTube (Mentioned on concepts-file-management.mdx and components-data.mdx)
-->


================================================================================
filename: Components/components-custom-components.mdx
================================================================================

---
title: Create custom Python components
slug: /components-custom-components
---

import Icon from "@site/src/components/icon";
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

Custom components extend Langflow's functionality through Python classes that inherit from `Component`. This enables integration of new features, data manipulation, external services, and specialized tools.

In Langflow's node-based environment, each node is a "component" that performs discrete functions. Custom components are Python classes which define:

* **Inputs** â€” Data or parameters your component requires.
* **Outputs** â€” Data your component provides to downstream nodes.
* **Logic** â€” How you process inputs to produce outputs.

The benefits of creating custom components include unlimited extensibility, reusability, automatic field generation in the visual editor based on inputs, and type-safe connections between nodes.

Create custom components for performing specialized tasks, calling APIs, or adding advanced logic.

Custom components in Langflow are built upon:

* The Python class that inherits from `Component`.
* Class-level attributes that identify and describe the component.
* Input and output lists that determine data flow.
* Internal variables for logging and advanced logic.

## Class-level attributes

Define these attributes to control a custom component's appearance and behavior:

```python
class MyCsvReader(Component):
    display_name = "CSV Reader"
    description = "Reads CSV files"
    icon = "file-text"
    name = "CSVReader"
    documentation = "http://docs.example.com/csv_reader"
```

* `display_name`: A user-friendly label shown in the visual editor.
* `description`: A brief summary shown in tooltips and printed below the component name when added to a flow.
* `icon`: A decorative icon from Langflow's icon library, printed next to the name.

    Langflow uses [Lucide](https://lucide.dev/icons) for icons. To assign an icon to your component, set the icon attribute to the name of a Lucide icon as a string, such as `icon = "file-text"`. Langflow renders icons from the Lucide library automatically.

* `name`: A unique internal identifier, typically the same name as the folder containing your component code.
* `documentation`: An optional link to external documentation, such as API or product documentation.

### Structure of a custom component

A Langflow custom component is more than a class with inputs and outputs. It includes an internal structure with optional lifecycle steps, output generation, front-end interaction, and logic organization.

A basic component:

* Inherits from `langflow.custom.Component`.
* Declares metadata like `display_name`, `description`, `icon`, and more.
* Defines `inputs` and `outputs` lists.
* Implements methods matching output specifications.

A minimal custom component skeleton contains the following:

```python
from langflow.custom import Component
from langflow.template import Output

class MyComponent(Component):
    display_name = "My Component"
    description = "A short summary."
    icon = "sparkles"
    name = "MyComponent"

    inputs = []
    outputs = []

    def some_output_method(self):
        return ...
```
### Internal Lifecycle and Execution Flow

Langflow's engine manages:

* **Instantiation**:  A component is created and internal structures are initialized.
* **Assigning Inputs**: Values from the visual editor or connections are assigned to component fields.
* **Validation and Setup**: Optional hooks like `_pre_run_setup`.
* **Outputs Generation**: `run()` or `build_results()` triggers output methods.

**Optional Hooks**:

* `initialize_data` or `_pre_run_setup` can run setup logic before the component's main execution.
* `__call__`, `run()`, or `_run()` can be overridden to customize how the component is called or to define custom execution logic.

### Inputs and outputs

Custom component inputs are defined with properties like:

* `name`, `display_name`
* Optional: `info`, `value`, `advanced`, `is_list`, `tool_mode`, `real_time_refresh`

For example:

* `StrInput`: simple text input.
* `DropdownInput`: selectable options.
* `HandleInput`: specialized connections.

Custom component `Output` properties define:

* `name`, `display_name`, `method`
* Optional: `info`

For more information, see [Custom component inputs and outputs](/components-custom-components#custom-component-inputs-and-outputs).

### Associated Methods

Each output is linked to a method:

* The output method name must match the method name.
* The method typically returns objects like Message, Data, or DataFrame.
* The method can use inputs with `self.<input_name>`.

For example:

```python
Output(
    display_name="File Contents",
    name="file_contents",
    method="read_file"
)
#...
def read_file(self) -> Data:
    path = self.filename
    with open(path, "r") as f:
        content = f.read()
    self.status = f"Read {len(content)} chars from {path}"
    return Data(data={"content": content})
```

### Components with multiple outputs

A component can define multiple outputs.
Each output can have a different corresponding method.
For example:

```python
outputs = [
    Output(display_name="Processed Data", name="processed_data", method="process_data"),
    Output(display_name="Debug Info", name="debug_info", method="provide_debug_info"),
]
```

#### Output Grouping Behavior with `group_outputs`

By default, components in Langflow that produce multiple outputs only allow one output selection in the visual editor.
The component will have only one output port where the user can select the preferred output type.

This behavior is controlled by the `group_outputs` parameter:

- **`group_outputs=False` (default)**: When a component has more than one output and `group_outputs` is `false` or not set, the outputs are grouped in the visual editor, and the user must select one.

    Use this option when the component is expected to return only one type of output when used in a flow.

- **`group_outputs=True`**: All outputs are available simultaneously in the visual editor. The component has one output port for each output, and the user can connect zero or more outputs to other components.

    Use this option when the component is expected to return multiple values that are used in parallel by downstream components or processes.

<Tabs>
<TabItem value="false" label="False or not set" default>

In this example, the visual editor provides a single output port, and the user can select one of the outputs.
Since `group_outputs=False` is the default behavior, it doesn't need to be explicitly set in the component, as shown in this example:

```python
outputs = [
    Output(
        name="structured_output",
        display_name="Structured Output",
        method="build_structured_output",
    ),
    Output(
        name="dataframe_output",
        display_name="DataFrame Output",
        method="build_structured_dataframe",
    ),
]
```

</TabItem>
<TabItem value="true" label="True">

In this example, all outputs are available simultaneously in the visual editor:

2. `group_outputs=True`

```python
outputs = [
    Output(
        name="true_result",
        display_name="True",
        method="true_response",
        group_outputs=True,
    ),
    Output(
        name="false_result",
        display_name="False",
        method="false_response",
        group_outputs=True,
    ),
]
```

</TabItem>
</Tabs>

### Common internal patterns

#### `_pre_run_setup()`

To initialize a custom component with counters set:

```python
def _pre_run_setup(self):
    if not hasattr(self, "_initialized"):
        self._initialized = True
        self.iteration = 0
```

#### Override `run` or `_run`
You can override `async def _run(self): ...` to define custom execution logic, although the default behavior from the base class usually covers most cases.

#### Store data in `self.ctx`
Use `self.ctx` as a shared storage for data or counters across the component's execution flow:

```python
def some_method(self):
    count = self.ctx.get("my_count", 0)
    self.ctx["my_count"] = count + 1
```

## Directory structure requirements

By default, Langflow looks for custom components in the `/components` directory.

If you're creating custom components in a different location using the `LANGFLOW_COMPONENTS_PATH` [environment variable](/environment-variables), components must be organized in a specific directory structure to be properly loaded and displayed in the visual editor:

Each category directory **must** contain an `__init__.py` file for Langflow to properly recognize and load the components.
This is a Python package requirement that ensures the directory is treated as a module.

```
/your/custom/components/path/    # Base directory set by LANGFLOW_COMPONENTS_PATH
    â””â”€â”€ category_name/          # Required category subfolder that determines menu name
        â”œâ”€â”€ __init__.py         # Required
        â””â”€â”€ custom_component.py # Component file
```

Components must be placed inside category folders, not directly in the base directory.

The category folder name determines where the component appears in the Langflow <Icon name="Component" aria-hidden="true" /> **Core components** menu.
For example, to add a component to the **Helpers** category, place it in the `helpers` subfolder:

```
/app/custom_components/          # LANGFLOW_COMPONENTS_PATH
    â””â”€â”€ helpers/                 # Displayed within the "Helpers" category
        â”œâ”€â”€ __init__.py          # Required
        â””â”€â”€ custom_component.py  # Your component
```

You can have multiple category folders to organize components into different categories:
```
/app/custom_components/
    â”œâ”€â”€ helpers/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ helper_component.py
    â””â”€â”€ tools/
        â”œâ”€â”€ __init__.py
        â””â”€â”€ tool_component.py
```

This folder structure is required for Langflow to properly discover and load your custom components. Components placed directly in the base directory aren't loaded.

```
/app/custom_components/          # LANGFLOW_COMPONENTS_PATH
    â””â”€â”€ custom_component.py      # Won't be loaded - missing category folder!
```

## Custom component inputs and outputs

Inputs and outputs define how data flows through the component, how it appears in the visual editor, and how connections to other components are validated.

### Inputs

Inputs are defined in a class-level `inputs` list. When Langflow loads the component, it uses this list to render component fields and [ports](/concepts-components#component-ports) in the visual editor. Users or other components provide values or connections to fill these inputs.

An input is usually an instance of a class from `langflow.io` (such as `StrInput`, `DataInput`, or `MessageTextInput`). The most common constructor parameters are:

* **`name`**: The internal variable name, accessed with `self.<name>`.
* **`display_name`**: The label shown to users in the visual editor.
* **`info`** *(optional)*: A tooltip or short description.
* **`value`** *(optional)*: The default value.
* **`advanced`** *(optional)*: If `true`, moves the field into the "Advanced" section.
* **`required`** *(optional)*: If `true`, forces the user to provide a value.
* **`is_list`** *(optional)*: If `true`, allows multiple values.
* **`input_types`** *(optional)*: Restricts allowed connection types (e.g., `["Data"]`, `["LanguageModel"]`).

Here are the most commonly used input classes and their typical usage.

**Text Inputs**: For simple text entries.
* **`StrInput`** creates a single-line text field.
* **`MultilineInput`** creates a multi-line text area.

**Numeric and Boolean Inputs**: Ensures users can only enter valid numeric or Boolean data.
* **`BoolInput`**, **`IntInput`**, and **`FloatInput`** provide fields for Boolean, integer, and float values, ensuring type consistency.

**Dropdowns**: For selecting from predefined options, useful for modes or levels.
* **`DropdownInput`**

**Secrets**: A specialized input for sensitive data, ensuring input is hidden in the visual editor.
* **`SecretStrInput`** for API keys and passwords.

**Specialized Data Inputs**: Ensures type-checking and color-coded connections in the visual editor.
* **`DataInput`** expects a `Data` object (typically with `.data` and optional `.text`).
* **`MessageInput`** expects a `Message` object, used in chat or agent flows.
* **`MessageTextInput`** simplifies access to the `.text` field of a `Message`.

**Handle-Based Inputs**: Used to connect outputs of specific types, ensuring correct pipeline connections.
- **`HandleInput`**

**File Uploads**: Allows users to upload files directly through the visual editor or receive file paths from other components.
- **`FileInput`**

**Lists**: Set `is_list=True` to accept multiple values, ideal for batch or grouped operations.

This example defines three inputs: a text field (`StrInput`), a Boolean toggle (`BoolInput`), and a dropdown selection (`DropdownInput`).

```python
from langflow.io import StrInput, BoolInput, DropdownInput

inputs = [
    StrInput(name="title", display_name="Title"),
    BoolInput(name="enabled", display_name="Enabled", value=True),
    DropdownInput(name="mode", display_name="Mode", options=["Fast", "Safe", "Experimental"], value="Safe")
]
```

### Outputs

Outputs are defined in a class-level `outputs` list. When Langflow renders a component, each output becomes a connector point in the visual editor. When you connect something to an output, Langflow automatically calls the corresponding method and passes the returned object to the next component.

An output is usually an instance of `Output` from `langflow.io`, with common parameters:

* **`name`**: The internal variable name.
* **`display_name`**: The label shown in the visual editor.
* **`method`**: The name of the method called to produce the output.
* **`info`** *(optional)*: Help text shown on hover.

The method must exist in the class, and it is recommended to annotate its return type for better type checking.
You can also set a `self.status` message inside the method to show progress or logs.

**Common Return Types**:
- **`Message`**: Structured chat messages.
- **`Data`**: Flexible object with `.data` and optional `.text`.
- **`DataFrame`**: Pandas-based tables (`langflow.schema.DataFrame`).
- **Primitive types**: `str`, `int`, `bool` (not recommended if you need type/color consistency).

In this example, the `DataToDataFrame` component defines its output using the outputs list. The `df_out` output is linked to the `build_df` method, so when connected to another component (node), Langflow calls this method and passes its returned `DataFrame` to the next node. This demonstrates how each output maps to a method that generates the actual output data.

```python
from langflow.custom import Component
from langflow.io import DataInput, Output
from langflow.schema import Data, DataFrame

class DataToDataFrame(Component):
    display_name = "Data to DataFrame"
    description = "Convert multiple Data objects into a DataFrame"
    icon = "table"
    name = "DataToDataFrame"

    inputs = [
        DataInput(
            name="items",
            display_name="Data Items",
            info="List of Data objects to convert",
            is_list=True
        )
    ]

    outputs = [
        Output(
            name="df_out",
            display_name="DataFrame Output",
            method="build_df"
        )
    ]

    def build_df(self) -> DataFrame:
        rows = []
        for item in self.items:
            row_dict = item.data.copy() if item.data else {}
            row_dict["text"] = item.get_text() or ""
            rows.append(row_dict)

        df = DataFrame(rows)
        self.status = f"Built DataFrame with {len(rows)} rows."
        return df
```


### Tool Mode

Components that support **Tool Mode** can be used as standalone components (when _not_ in **Tool Mode**) or as tools for other components with a **Tools** input, such as **Agent** components.

You can allow a custom component to support **Tool Mode** by setting `tool_mode=True`:

```python
inputs = [
    MessageTextInput(
        name="message",
        display_name="Mensage",
        info="Enter the message that will be processed directly by the tool",
        tool_mode=True,
    ),
]
```

Langflow currently supports the following input types for **Tool Mode**:

* `DataInput`
* `DataFrameInput`
* `PromptInput`
* `MessageTextInput`
* `MultilineInput`
* `DropdownInput`

## Typed annotations

In Langflow, **typed annotations** allow Langflow to visually guide users and maintain flow consistency.

Typed annotations provide:

* **Color-coding**: Outputs like `-> Data` or `-> Message` get distinct colors.
* **Validation**: Langflow blocks incompatible connections automatically.
* **Readability**: Developers can quickly understand data flow.
* **Development tools**: Better code suggestions and error checking in your code editor.

### Common Return Types

* `Message`: For chat-style outputs. Connects to any of several `Message`-compatible inputs.

    ```python
    def produce_message(self) -> Message:
        return Message(text="Hello! from typed method!", sender="System")
    ```

* `Data`: For structured data like dicts or partial texts. Connects only to `DataInput` (ports that accept `Data`).

    ```python
    def get_processed_data(self) -> Data:
        processed = {"key1": "value1", "key2": 123}
        return Data(data=processed)
    ```

* `DataFrame`: For tabular data. Connects only to `DataFrameInput` (ports that accept `DataFrame`).

    ```python
    def build_df(self) -> DataFrame:
        pdf = pd.DataFrame({"A": [1, 2], "B": [3, 4]})
        return DataFrame(pdf)
    ```

* Primitive Types (`str`, `int`, `bool`): Returning primitives is allowed but wrapping in `Data` or `Message` is recommended for better consistency in the visual editor.

    ```python
    def compute_sum(self) -> int:
        return sum(self.numbers)
    ```

### Tips for typed annotations

When using typed annotations, consider the following best practices:

* **Always Annotate Outputs**: Specify return types like `-> Data`, `-> Message`, or `-> DataFrame` to enable proper visual editor color-coding and validation.
* **Wrap Raw Data**: Use `Data`, `Message`, or `DataFrame` wrappers instead of returning plain structures.
* **Use Primitives Carefully**: Direct `str` or `int` returns are fine for simple flows, but wrapping improves flexibility.
* **Annotate Helpers Too**: Even if internal, typing improves maintainability and clarity.
* **Handle Edge Cases**: Prefer returning structured `Data` with error fields when needed.
* **Stay Consistent**: Use the same types across your components to make flows predictable and easier to build.

## Enable dynamic fields

In **Langflow**, dynamic fields allow inputs to change or appear based on user interactions. You can make an input dynamic by setting `dynamic=True`.
Optionally, setting `real_time_refresh=True` triggers the `update_build_config` method to adjust the input's visibility or properties in real time, creating a contextual visual editor experience that only exposes relevant fields based on the user's choices.

In this example, the operator field triggers updates with `real_time_refresh=True`.
The `regex_pattern` field is initially hidden and controlled with `dynamic=True`.

```python
from langflow.io import DropdownInput, StrInput

class RegexRouter(Component):
    display_name = "Regex Router"
    description = "Demonstrates dynamic fields for regex input."

    inputs = [
        DropdownInput(
            name="operator",
            display_name="Operator",
            options=["equals", "contains", "regex"],
            value="equals",
            real_time_refresh=True,
        ),
        StrInput(
            name="regex_pattern",
            display_name="Regex Pattern",
            info="Used if operator='regex'",
            dynamic=True,
            show=False,
        ),
    ]
```

### Implement `update_build_config`

When a field with `real_time_refresh=True` is modified, Langflow calls the `update_build_config` method, passing the updated field name, value, and the component's configuration to dynamically adjust the visibility or properties of other fields based on user input.

This example will show or hide the `regex_pattern` field when the user selects a different operator.

```python
def update_build_config(self, build_config: dict, field_value: str, field_name: str | None = None) -> dict:
    if field_name == "operator":
        if field_value == "regex":
            build_config["regex_pattern"]["show"] = True
        else:
            build_config["regex_pattern"]["show"] = False
    return build_config
```

### Additional Dynamic Field Controls

You can also modify other properties within `update_build_config`, such as:
* `required`: Set `build_config["some_field"]["required"] = True/False`

* `advanced`: Set `build_config["some_field"]["advanced"] = True`

* `options`: Modify dynamic dropdown options.

### Tips for Managing Dynamic Fields

When working with dynamic fields, consider the following best practices to ensure a smooth user experience:

* **Minimize field changes**: Hide only fields that are truly irrelevant to avoid confusing users.
* **Test behavior**: Ensure that adding or removing fields doesn't accidentally erase user input.
* **Preserve data**: Use `build_config["some_field"]["show"] = False` to hide fields without losing their values.
* **Clarify logic**: Add `info` notes to explain why fields appear or disappear based on conditions.
* **Keep it manageable**: If the dynamic logic becomes too complex, consider breaking it into smaller components, unless it serves a clear purpose in a single node.


## Error handling and logging

In Langflow, robust error handling ensures that your components behave predictably, even when unexpected situations occur, such as invalid inputs, external API failures, or internal logic errors.

### Error handling techniques

* **Raise Exceptions**: If a critical error occurs, you can raise standard Python exceptions such as `ValueError`, or specialized exceptions like `ToolException`. Langflow will automatically catch these and display appropriate error messages in the visual editor, helping users quickly identify what went wrong.

    ```python
    def compute_result(self) -> str:
        if not self.user_input:
            raise ValueError("No input provided.")
        # ...
    ```

* **Return Structured Error Data**: Instead of stopping a flow abruptly, you can return a Data object containing an "error" field. This approach allows the flow to continue operating and enables downstream components to detect and handle the error gracefully.

    ```python
    def run_model(self) -> Data:
        try:
            # ...
        except Exception as e:
            return Data(data={"error": str(e)})
    ```

### Improve debugging and flow management

* **Use `self.status`**: Each component has a status field where you can store short messages about the execution resultâ€”such as success summaries, partial progress, or error notifications. These appear directly in the visual editor, making troubleshooting easier for users.

    ```python
    def parse_data(self) -> Data:
    # ...
    self.status = f"Parsed {len(rows)} rows successfully."
    return Data(data={"rows": rows})
    ```

* **Stop specific outputs with `self.stop(...)`**: You can halt individual output paths when certain conditions fail, without affecting the entire component. This is especially useful when working with components that have multiple output branches.

    ```python
    def some_output(self) -> Data:
    if <some condition>:
        self.stop("some_output")  # Tells Langflow no data flows
        return Data(data={"error": "Condition not met"})
    ```

* **Log events**: You can log key execution details inside components. Logs are displayed in the "Logs" or "Events" section of the component's detail view and can be accessed later through the flow's debug panel or exported files, providing a clear trace of the component's behavior for easier debugging.

    ```python
    def process_file(self, file_path: str):
    self.log(f"Processing file {file_path}")
    # ...
    ```

### Tips for error handling and logging

To build more reliable components, consider the following best practices:

* **Validate inputs early**: Catch missing or invalid inputs at the start to prevent broken logic.
* **Summarize with `self.status`**: Use short success or error summaries to help users understand results quickly.
* **Keep logs concise**: Focus on meaningful messages to avoid cluttering the visual editor.
* **Return structured errors**: When appropriate, return `Data(data={"error": ...})` instead of raising exceptions to allow downstream handling.
* **Stop outputs selectively**: Only halt specific outputs with `self.stop(...)` if necessary, to preserve correct flow behavior elsewhere.

## Contribute custom components to Langflow

See [How to Contribute](/contributing-components) to contribute your custom component to Langflow.


================================================================================
filename: Components/components-data.mdx
================================================================================

---
title: Data
slug: /components-data
---

import Icon from "@site/src/components/icon";
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import PartialParams from '@site/docs/_partial-hidden-params.mdx';
import PartialDevModeWindows from '@site/docs/_partial-dev-mode-windows.mdx';

Data components bring data into your flows from various sources like files, API endpoints, and URLs.
For example:

* **Load files**: Import data from a file or directory with the [**File** component](#file) and [**Directory** component](#directory).

* **Search the web**: Fetch data from the web with components like the [**News Search** component](#news-search), [**RSS Reader** component](#rss-reader), [**Web Search** component](#web-search), and [**URL** component](#url).

* **Make API calls**: Use APIs to trigger flows or perform actions with the [**API Request** component](#api-request) and [**Webhook** component](#webhook).

* **Run SQL queries**: Query an SQL database with the [**SQL Database** component](#sql-database).

Each component runs different commands for retrieval, processing, and type checking.
Some components are a minimal wrapper for a command that you provide, and others include built-in scripts to fetch and process data based on variable inputs.
Additionally, some components return raw data, whereas others can convert, restructure, or validate the data before outputting it.
This means that some similar components might produce different results.

:::tip
Data components pair well with [Processing components](/components-processing) that can perform additional parsing, transformation, and validation after retrieving the data.

This can include basic operations, like saving a file in a specific format, or more complex tasks, like using a **Text Splitter** component to break down a large document into smaller chunks before generating embeddings for vector search.
:::

## Use Data components in flows

Data components are used often in flows because they offer a versatile way to perform common functions.

You can use these components to perform their base functions as isolated steps in your flow, or you can connect them to an **Agent** component as tools.

![An agent flow with three Data components connected to the agent as tools](/img/connect-data-components-to-agent.png)

For example flows, see the following:

* [Create a chatbot that can ingest files](/chat-with-files): Learn how to use a **File** component to load a file as context for a chatbot.
The file and user input are both passed to the LLM so you can ask questions about the file you uploaded.

* [Create a vector RAG chatbot](/chat-with-rag): Learn how to ingest files for use in Retrieval-Augmented Generation (RAG), and then set up a chatbot that can use the ingested files as context.
The two flows in this tutorial prepare files for RAG, and then let your LLM use vector search to retrieve contextually relevant data during a chat session.

* [Configure tools for agents](/agents-tools): Learn how to use any component as a tool for an agent.
When used as tools, the agent autonomously decides when to call a component based on the user's query.

* [Trigger flows with webhooks](/webhook): Learn how to use the **Webhook** component to trigger a flow run in response to an external event.

## API Request

The **API Request** component constructs and sends HTTP requests using URLs or curl commands:

* **URL mode**: Enter one or more comma-separated URLs, and then select the method for the request to each URL.
* **curl mode**: Enter the curl command to execute.

You can enable additional request options and fields in the component's parameters.

Returns a [`Data` object](/data-types#data) containing the response.

For provider-specific API components, see <Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components).

### API Request parameters

<PartialParams />

| Name | Display Name | Info |
|------|--------------|------|
| mode | Mode | Input parameter. Set the mode to either **URL** or **curl**. |
| urls | URL | Input parameter. Enter one or more comma-separated URLs for the request. |
| curl | curl | Input parameter. **curl mode** only. Enter a complete curl command. Other component parameters are populated from the command arguments. |
| method | Method | Input parameter. The HTTP method to use. |
| query_params | Query Parameters | Input parameter. The query parameters to append to the URL. |
| body | Body | Input parameter. The body to send with POST, PATCH, and PUT requests as a dictionary. |
| headers | Headers | Input parameter. The headers to send with the request as a dictionary. |
| timeout | Timeout | Input parameter. The timeout to use for the request. |
| follow_redirects | Follow Redirects | Input parameter. Whether to follow HTTP redirects. The default is enabled (`true`). If disabled (`false`), HTTP redirects aren't followed. |
| save_to_file | Save to File | Input parameter. Whether to save the API response to a temporary file. Default: Disabled (`false`) |
| include_httpx_metadata | Include HTTPx Metadata | Input parameter. Whether to include properties such as `headers`, `status_code`, `response_headers`, and `redirection_history` in the output. Default: Disabled (`false`) |

## Directory

The **Directory** component recursively loads files from a directory, with options for file types, depth, and concurrency.

Files must be of a [supported type and size](#file-type-and-size-limits) to be loaded.

Outputs either a [`Data`](/data-types#data) or [`DataFrame`](/data-types#dataframe) object, depending on the directory contents.

### Directory parameters

<PartialParams />

| Name               | Type             | Description                                        |
| ------------------ | ---------------- | -------------------------------------------------- |
| path               | MessageTextInput | Input parameter. The path to the directory to load files from. Default: Current directory (`.`) |
| types              | MessageTextInput | Input parameter. The file types to load. Select one or more, or leave empty to attempt to load all files. |
| depth              | IntInput         | Input parameter. The depth to search for files. |
| max_concurrency    | IntInput         | Input parameter. The maximum concurrency for loading multiple files. |
| load_hidden        | BoolInput        | Input parameter. If `true`, hidden files are loaded.                  |
| recursive          | BoolInput        | Input parameter. If `true`, the search is recursive.                  |
| silent_errors      | BoolInput        | Input parameter. If `true`, errors don't raise an exception.         |
| use_multithreading | BoolInput        | Input parameter. If `true`, multithreading is used.                   |

## File

The **File** component loads and parses files, converts the content into a `Data`, `DataFrame`, or `Message` object.
It supports multiple file types, provides parameters for parallel processing and error handling, and supports advanced parsing with the Docling library.

You can add files to the **File** component in the visual editor or at runtime, and you can upload multiple files at once.
For more information about uploading files and working with files in flows, see [File management](/concepts-file-management) and [Create a chatbot that can ingest files](/chat-with-files).

### File type and size limits

By default, the maximum file size is 1024 MB.
To modify this value, change the `LANGFLOW_MAX_FILE_SIZE_UPLOAD` [environment variable](/environment-variables).

<details>
<summary>Supported file types</summary>

The following file types are supported by the **File** component.
Use archive and compressed formats to bundle multiple files together, or use the [**Directory** component](#directory) to load all files in a directory.

- `.bz2`
- `.csv`
- `.docx`
- `.gz`
- `.htm`
- `.html`
- `.json`
- `.js`
- `.md`
- `.mdx`
- `.pdf`
- `.py`
- `.sh`
- `.sql`
- `.tar`
- `.tgz`
- `.ts`
- `.tsx`
- `.txt`
- `.xml`
- `.yaml`
- `.yml`
- `.zip`

</details>

If you need to load an unsupported file type, you must use a different component that supports that file type and, potentially, parses it outside Langflow, or you must convert it to a supported type before uploading it.

For images, see [Upload images](/concepts-file-management#upload-images).

For videos, see the **Twelve Labs** and **YouTube** <Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components).

### File parameters

<PartialParams />

| Name | Display Name | Info |
|------|--------------|------|
| path | Files | Input parameter. The path to files to load. Can be local or in [Langflow file management](/concepts-file-management). Supports individual files and bundled archives. |
| file_path | Server File Path | Input parameter. A `Data` object with a `file_path` property pointing to a file in [Langflow file management](/concepts-file-management) or a `Message` object with a path to the file. Supersedes **Files** (`path`) but supports the same file types. |
| separator | Separator | Input parameter. The separator to use between multiple outputs in `Message` format. |
| silent_errors | Silent Errors | Input parameter. If `true`, errors in the component don't raise an exception. Default: Disabled (`false`). |
| delete_server_file_after_processing | Delete Server File After Processing | Input parameter. If `true` (default), the **Server File Path** (`file_path`) is deleted after processing. |
| ignore_unsupported_extensions | Ignore Unsupported Extensions | Input parameter. If enabled (`true`), files with unsupported extensions are accepted but not processed. If disabled (`false`), the **File** component either can throw an error if an unsupported file type is provided. The default is `true`. |
| ignore_unspecified_files | Ignore Unspecified Files | Input parameter. If `true`, `Data` with no `file_path` property is ignored. If `false` (default), the component errors when a file isn't specified. |
| concurrency_multithreading | Processing Concurrency | Input parameter. The number of files to process concurrently if multiple files are uploaded. Default is 1. Values greater than 1 enable parallel processing for 2 or more files. Ignored for single-file uploads and advanced parsing. |
| advanced_parser | Advanced Parser | Input parameter. If `true`, enables [advanced parsing](#advanced-parsing). Only available for single-file uploads of compatible file types. Default: Disabled (`false`). |

### Advanced parsing

Starting in Langflow version 1.6, the **File** component supports advanced document parsing using the [Docling](https://docling-project.github.io/docling/) library for supported file types.

To use advanced parsing, do the following:

1. Complete the following prerequisites, if applicable:

    * **Install Langflow version 1.6 or later**: Earlier versions don't support advanced parsing with the **File** component. For upgrade guidance, see the [Release notes](/release-notes).

    * **Install Docling dependency on macOS Intel (x86_64)**: The Docling dependency isn't installed by default for macOS Intel (x86_64). Use the [Docling installation guide](https://docling-project.github.io/docling/installation/) to install the Docling dependency.

        For all other operating systems, the Docling dependency is installed by default.

    * **Enable Developer Mode for Windows**:
    <PartialDevModeWindows />

        Developer Mode isn't required for Langflow OSS on Windows.

2. Add one valid file to the **File** component.

    :::info Advanced parsing limitations
    * Advanced parsing processes only one file.
    If you select multiple files, the **File** component processes the first file only, ignoring any additional files.
    To process multiple files with advanced parsing, pass each file to a separate **File** components, or use the dedicated [**Docling** components](/bundles-docling).

    * Advanced parsing can process any of the **File** component's supported file types except `.csv`, `.xlsx`, and `.parquet` files because it is designed for document processing, such as extracting text from PDFs.
    For structured data analysis, use the [**Parser** component](/components-processing#parser).
    :::

3. Enable **Advanced Parsing**.

4. Click <Icon name="SlidersHorizontal" aria-hidden="true"/> **Controls** in the [component's header menu](/concepts-components#component-menus) to configure advanced parsing parameters, which are hidden by default:

    | Name | Display Name | Info |
    |------|--------------|------|
    | pipeline | Pipeline | Input parameter, advanced parsing. The Docling pipeline to use, either `standard` (default, recommended) or `vlm` (may produce inconsistent results). |
    | ocr_engine | OCR Engine | Input parameter, advanced parsing. The OCR parser to use if `pipeline` is `standard`. Options are `None` (default) or [`EasyOCR`](https://pypi.org/project/easyocr/). `None` means that no OCR engine is used, and this can produce inconsistent or broken results for some documents. This setting has no effect with the `vlm` pipeline. |
    | md_image_placeholder | Markdown Image Placeholder | Input parameter, advanced parsing. Defines the placeholder for image files if the output type is **Markdown**. Default: `<!-- image -->`. |
    | md_page_break_placeholder | Markdown Page Break Placeholder | Input parameter, advanced parsing. Defines the placeholder for page breaks if the output type is **Markdown**. Default: `""` (empty string). |
    | doc_key | Document Key | Input parameter, advanced parsing. The key to use for the `DoclingDocument` column, which holds the structured information extracted from the source document. See [Docling Document](https://docling-project.github.io/docling/concepts/docling_document/) for details. Default: `doc`. |

    :::tip
    For additional Docling features, including other components and OCR parsers, use the [**Docling** bundle](/bundles-docling).
    :::

### File output

The output of the **File** component depends on the number of files loaded and whether advanced parsing is enabled.
If multiple options are available, you can set the output type near the component's output port.

<Tabs>
<TabItem value="zero" label="No files">

If you run the **File** component with no file selected, it throws an error, or, if **Silent Errors** is enabled, produces no output.

</TabItem>
<TabItem value="one-false" label="One file without advanced parsing">

If advanced parsing is disabled and you upload one file, the following output types are available:

- **Structured Content**: Available only for `.csv`, `.xlsx`, `.parquet`, and `.json` files.

    - For `.csv` files, produces a [`DataFrame`](/data-types#dataframe) representing the table data.
    - For `.json` files, produces a [`Data`](/data-types#data) object with the parsed JSON data.

- **Raw Content**: A [`Message`](/data-types#message) containing the file's raw text content.

- **File Path**: A [`Message`](/data-types#message) containing the path to the file in [Langflow file management](/concepts-file-management).

</TabItem>
<TabItem value="one-true" label="One file with advanced parsing">

If advanced parsing is enabled and you upload one file, the following output types are available:

- **Structured Output**: A [`DataFrame`](/data-types#dataframe) containing the Docling-processed document data with text elements, page numbers, and metadata.

- **Markdown**: A [`Message`](/data-types#message) containing the uploaded document contents in Markdown format with image placeholders.

- **File Path**: A [`Message`](/data-types#message) containing the path to the file in [Langflow file management](/concepts-file-management).

</TabItem>
<TabItem value="multi" label="Multiple files">

If you upload multiple files, the component outputs **Files**, which is a [`DataFrame`](/data-types#dataframe) containing the content and metadata of all selected files.

[Advanced parsing](#advanced-parsing) doesn't support multiple files; it processes only the first file.

</TabItem>
</Tabs>

## News Search

The **News Search** component searches Google News through RSS, and then returns clean article data as a [`DataFrame`](/data-types#dataframe) containing article titles, links, publication dates, and summaries.
The component's `clean_html` method parses the HTML content with the BeautifulSoup library, removes HTML markup, and strips whitespace to output clean data.

For other RSS feeds, use the [**RSS Reader** component](#rss-reader), and for other searches use the [**Web Search** component](#web-search) or provider-specific <Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components).

When used as a standard component in a flow, the **News Search** component must be connected to a component that accepts `DataFrame` input.
You can connect the **News Search** component directly to a compatible component, or you can use a [Processing component](/components-processing) to convert or extract data of a different type between components.

When used in **Tool Mode** with an **Agent** component, the **News Search** component can be connected directly to the **Agent** component's **Tools** port without converting the data.
The agent decides whether to use the **News Search** component based on the user's query, and it can process the `DataFrame` output directly.

### News Search parameters

<PartialParams />

| Name | Display Name | Info |
|------|--------------|------|
| query | Search Query | Input parameter. Search keywords for news articles. |
| hl | Language (hl) | Input parameter. Language code, such as en-US, fr, de. Default: `en-US`. |
| gl | Country (gl) | Input parameter. Country code, such as US, FR, DE. Default: `US`. |
| ceid | Country:Language (ceid) | Input parameter. Language, such as US:en, FR:fr. Default: `US:en`. |
| topic | Topic | Input parameter. One of: `WORLD`, `NATION`, `BUSINESS`, `TECHNOLOGY`, `ENTERTAINMENT`, `SCIENCE`, `SPORTS`, `HEALTH`. |
| location | Location (Geo) | Input parameter. City, state, or country for location-based news. Leave blank for keyword search. |
| timeout | Timeout | Input parameter. Timeout for the request in seconds. |
| articles | News Articles | Output parameter. A `DataFrame` with the key columns `title`, `link`, `published` and `summary`. |

## RSS Reader

The **RSS Reader** component fetches and parses RSS feeds from any valid RSS feed URL, and then returns the feed content as a [`DataFrame`](/data-types#dataframe) containing article titles, links, publication dates, and summaries.

When used as a standard component in a flow, the **RSS Reader** component must be connected to a component that accepts `DataFrame` input.
You can connect the **RSS Reader** component directly to a compatible component, or you can use a [Processing component](/components-processing) to convert or extract data of a different type between components.

When used in **Tool Mode** with an **Agent** component, the **RSS Reader** component can be connected directly to the **Agent** component's **Tools** port without converting the data.
The agent decides whether to use the **RSS Reader** component based on the user's query, and it can process the `DataFrame` output directly.

### RSS Reader parameters

| Name | Display Name | Info |
|------|--------------|------|
| rss_url | RSS Feed URL | Input parameter. URL of the RSS feed to parse, such as `https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml`. |
| timeout | Timeout | Input parameter. Timeout for the RSS feed request in seconds. Default: `5`. |
| articles | Articles | Output parameter. A `DataFrame` containing the key columns `title`, `link`, `published` and `summary`. |

## SQL Database

The **SQL Database** component executes SQL queries on [SQLAlchemy-compatible databases](https://docs.sqlalchemy.org/en/20/).
It supports any SQLAlchemy-compatible database, such as PostgreSQL, MySQL, and SQLite.

For CQL queries, see the [**DataStax** bundle](/bundles-datastax).

### Query an SQL database with natural language prompts

The following example demonstrates how to use the **SQL Database** component in a flow, and then modify the component to support natural language queries through an **Agent** component.

This allows you to use the same **SQL Database** component for any query, rather than limiting it to a single manually entered query or requiring the user, application, or another component to provide valid SQL syntax as input.
Users don't need to master SQL syntax because the **Agent** component translates the users' natural language prompts into SQL queries, passes the query to the **SQL Database** component, and then returns the results to the user.

Additionally, input from applications and other components doesn't have to be extracted and transformed to exact SQL queries.
Instead, you only need to provide enough context for the agent to understand that it should create and run a SQL query according to the incoming data.

1. Use your own sample database or create a test database.

    <details>
    <summary>Create a test SQL database</summary>

    1. Create a database called `test.db`:

        ```shell
        sqlite3 test.db
        ```

    2. Add some values to the database:

        ```shell
        sqlite3 test.db "
        CREATE TABLE users (
            id INTEGER PRIMARY KEY,
            name TEXT,
            email TEXT,
            age INTEGER
        );

        INSERT INTO users (name, email, age) VALUES
            ('John Doe', 'john@example.com', 30),
            ('Jane Smith', 'jane@example.com', 25),
            ('Bob Johnson', 'bob@example.com', 35);
        "
        ```

    3. Verify that the database has been created and contains your data:

        ```shell
        sqlite3 test.db "SELECT * FROM users;"
        ```

        The result should list the text data you entered in the previous step:

        ```shell
        1|John Doe|john@example.com
        2|Jane Smith|jane@example.com
        3|John Doe|john@example.com
        4|Jane Smith|jane@example.com
        ```

    </details>

2. Add an **SQL Database** component to your flow.

3. In the **Database URL** field, add the connection string for your database, such as `sqlite:///test.db`.

    At this point, you can enter an SQL query in the **SQL Query** field or use the [port](/concepts-components#component-ports) to pass a query from another component, such as a **Chat Input** component.
    If you need more space, click <Icon name="Expand" aria-hidden="true"/> **Expand** to open a full-screen text field.

    However, to make this component more dynamic in an agentic context, use an **Agent** component to transform natural language input to SQL queries, as explained in the following steps.

4. Click the **SQL Database** component to expose the [component's header menu](/concepts-components#component-menus), and then enable **Tool Mode**.

    You can now use this component as a tool for an agent.
    In **Tool Mode**, no query is set in the **SQL Database** component because the agent will generate and send one if it determines that the tool is required to complete the user's request.
    For more information, see [Configure tools for agents](/agents-tools).

5. Add an **Agent** component to your flow, and then enter your OpenAI API key.

    The default model is an OpenAI model.
    If you want to use a different model, edit the **Model Provider**, **Model Name**, and **API Key** fields accordingly.

    If you need to execute highly specialized queries, consider selecting a model that is trained for tasks like advanced SQL queries.
    If your preferred model isn't in the **Agent** component's built-in model list, set **Model Provider** to **Connect other models**, and then connect any [language model component](/components-models).

6. Connect the **SQL Database** component's **Toolset** output to the **Agent** component's **Tools** input.

    ![SQL Database component connected to an Agent component](/img/component-sql-database.png)

7. Click **Playground**, and then ask the agent a question about the data in your database, such as `Which users are in my database?`

    The agent determines that it needs to query the database to answer the question, uses the LLM to generate an SQL query, and then uses the **SQL Database** component's `RUN_SQL_QUERY` action to run the query on your database.
    Finally, it returns the results in a conversational format, unless you provide instructions to return raw results or a different format.

    The following example queried a test database with little data, but with a more robust dataset you could ask more detailed or complex questions.

    ```text
    Here are the users in your database:

    1. **John Doe** - Email: john@example.com
    2. **Jane Smith** - Email: jane@example.com
    3. **John Doe** - Email: john@example.com
    4. **Jane Smith** - Email: jane@example.com

    It seems there are duplicate entries for the users.
    ```

### SQL Database parameters

<PartialParams />

| Name | Display Name | Info |
|------|--------------|------|
| database_url | Database URL | Input parameter. The SQLAlchemy-compatible database connection URL. |
| query | SQL Query | Input parameter. The SQL query to execute, which can be entered directly, passed in from another component, or, in **Tool Mode**, automatically provided by an **Agent** component. |
| include_columns | Include Columns | Input parameter. Whether to include column names in the result. The default is enabled (`true`). |
| add_error | Add Error | Input parameter. If enabled, adds any error messages to the result, if any are returned. The default is disabled (`false`). |
| run_sql_query | Result Table | Output parameter. The query results as a [`DataFrame`](/data-types#dataframe). |

## URL

The **URL** component fetches content from one or more URLs, processes the content, and returns it in various formats.
It follows links recursively to a given depth, and it supports output in plain text or raw HTML.

### URL parameters

<PartialParams />

Some of the available parameters include the following:

| Name | Display Name | Info |
|------|--------------|------|
| urls | URLs | Input parameter. One or more URLs to crawl recursively. In the visual editor, click <Icon name="Plus" aria-hidden="true"/> **Add URL** to add multiple URLs. |
| max_depth | Depth | Input parameter. Controls link traversal: how many "clicks" away from the initial page the crawler will go. A depth of 1 limits the crawl to the first page at the given URL only. A depth of 2 means the crawler crawls the first page plus each page directly linked from the first page, then stops. This setting exclusively controls link traversal; it doesn't limit the number of URL path segments or the domain. |
| prevent_outside | Prevent Outside | Input parameter. If enabled, only crawls URLs within the same domain as the root URL. This prevents the crawler from accessing sites outside the given URL's domain, even if they are linked from one of the crawled pages. |
| use_async | Use Async | Input parameter. If enabled, uses asynchronous loading which can be significantly faster but might use more system resources. |
| format | Output Format | Input parameter. Sets the desired output format as **Text** or **HTML**. The default is **Text**. For more information, see [URL output](#url-output).|
| timeout | Timeout | Input parameter. Timeout for the request in seconds. |
| headers | Headers | Input parameter. The headers to send with the request if needed for authentication or otherwise. |

Additional input parameters are available for error handling and encoding.

### URL output

There are two settings that control the output of the **URL** component at different stages:

* **Output Format**: This optional parameter controls the content extracted from the crawled pages:

    * **Text (default)**: The component extracts only the text from the HTML of the crawled pages.
    * **HTML**: The component extracts the entire raw HTML content of the crawled pages.

* **Output data type**: In the component's output field (near the output port) you can select the structure of the outgoing data when it is passed to other components:

    * **Extracted Pages**: Outputs a [`DataFrame`](/data-types#dataframe) that breaks the crawled pages into columns for the entire page content (`text`) and metadata like `url` and `title`.
    * **Raw Content**: Outputs a [`Message`](/data-types#message) containing the entire text or HTML from the crawled pages, including metadata, in a single block of text.

When used as a standard component in a flow, the **URL** component must be connected to a component that accepts the selected output data type (`DataFrame` or `Message`).
You can connect the **URL** component directly to a compatible component, or you can use a [**Type Convert** component](/components-processing#type-convert) to convert the output to another type before passing the data to other components if the data types aren't directly compatible.

Processing components like the **Type Convert** component are useful with the **URL** component because it can extract a large amount of data from the crawled pages.
For example, if you only want to pass specific fields to other components, you can use a [**Parser** component](/components-processing#parser) to extract only that data from the crawled pages before passing the data to other components.

When used in **Tool Mode** with an **Agent** component, the **URL** component can be connected directly to the **Agent** component's **Tools** port without converting the data.
The agent decides whether to use the **URL** component based on the user's query, and it can process the `DataFrame` or `Message` output directly.

## Web Search

The **Web Search** component performs a basic web search using DuckDuckGo's HTML scraping interface.
For other search APIs, see <Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components).

:::info
The **Web Search** component uses web scraping that can be subject to rate limits.

For production use, consider using another search component with more robust API support, such as provider-specific bundles.
:::

### Use the Web Search component in a flow

The following steps demonstrate one way that you can use a **Web Search** component in a flow:

1. Create a flow based on the **Basic Prompting** template.

2. Add a **Web Search** component, and then enter a search query, such as `environmental news`.

3. Add a [**Type Convert** component](/components-processing#type-convert), set the **Output Type** to **Message**, and then connect the **Web Search** component's output to the **Type Convert** component's input.

    By default, the **Web Search** component outputs a `DataFrame`.
    Because the **Prompt Template** component only accepts `Message` data, this conversion is required so that the flow can pass the search results to the **Prompt Template** component.
    For more information, see [Web Search output](#web-search-output).

5. In the **Prompt Template** component's **Template** field, add a variable like `{searchresults}` or `{context}`.

    This adds a field to the **Prompt Template** component that you can use to pass the converted search results to the prompt.
    For more information, see [Define variables in prompts](/components-prompts#define-variables-in-prompts).

6. Connect the **Type Convert** component's output to the new variable field on the **Prompt Template** component.

    ![Type convert web search output to chat](/img/component-type-convert-and-web-search.png)

7. In the **Language Model** component, add your OpenAI API key, or select a different provider and model.

8. Click **Playground**, and then enter `latest news`.

    The LLM processes the request, including the context passed through the **Prompt Template** component, and then prints the response in the **Playground** chat interface.

    <details>
    <summary>Result</summary>

    The following is an example of a possible response.
    Your response may vary based on the current state of the web, your specific query, the model, and other factors.

    ```text
    Here are some of the latest news articles related to the environment:
    Ozone Pollution and Global Warming: A recent study highlights that ozone pollution is a significant global environmental concern, threatening human health and crop production while exacerbating global warming. Read more
    ...
    ```

    </details>

### Web Search parameters

| Name | Display Name | Info |
|------|--------------|------|
| query | Search Query | Input parameter. Keywords to search for. |
| timeout | Timeout | Input parameter. Timeout for the web search request in seconds. Default: `5`. |
| results | Search Results | Output parameter. Returns a `DataFrame` containing `title`, `links`, and `snippets`. For more information, see [Web Search output](#web-search-output). |

### Web Search output

The **Web Search** component outputs a [`DataFrame`](/data-types#dataframe) containing the key columns `title`, `links`, and `snippets`.

When used as a standard component in a flow, the **Web Search** component must be connected to a component that accepts `DataFrame` input, or you must use a [**Type Convert** component](/components-processing#type-convert) to convert the output to `Data` or `Message` type before passing the data to other components.

When used in **Tool Mode** with an **Agent** component, the **Web Search** component can be connected directly to the **Agent** component's **Tools** port without converting the data.
The agent decides whether to use the **Web Search** component based on the user's query, and it can process the `DataFrame` output directly.

## Webhook

The **Webhook** component defines a webhook trigger that runs a flow when it receives an HTTP POST request.

### Trigger the webhook

When you add a **Webhook** component to your flow, a **Webhook curl** tab is added to the flow's [**API Access** pane](/concepts-publish#api-access).
This tab automatically generates an HTTP POST request code snippet that you can use to trigger your flow through the **Webhook** component.
For example:

```bash
curl -X POST \
  "http://$LANGFLOW_SERVER_ADDRESS/api/v1/webhook/$FLOW_ID" \
  -H 'Content-Type: application/json' \
  -H 'x-api-key: $LANGFLOW_API_KEY' \
  -d '{"any": "data"}'
```

For more information, see [Trigger flows with webhooks](/webhook).

### Webhook parameters

| Name | Display Name | Description |
|------|--------------|-------------|
| data | Payload | Input parameter. Receives a payload from external systems through HTTP POST requests. |
| curl | curl | Input parameter. The curl command template for making requests to this webhook. |
| endpoint | Endpoint | Input parameter. The endpoint URL where this webhook receives requests. |
| output_data | Data | Output parameter. The processed data from the webhook input. Returns an empty [`Data`](/data-types#data) object if no input is provided. If the input isn't valid JSON, the **Webhook** component wraps it in a `payload` object so that it can be accepted as input to trigger the flow. |

## Additional Data components

Langflow's core components are meant to be generic and support a range of use cases.
Core components typically aren't limited to a single provider.

If the core components don't meet your needs, you can find provider-specific components in <Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components).

For example, the [**DataStax** bundle](/bundles-datastax) includes components for CQL queries, and the [**Google** bundle](/bundles-google) includes components for Google Search APIs.

## Legacy Data components

import PartialLegacy from '@site/docs/_partial-legacy.mdx';

<PartialLegacy />

The following Data components are in legacy status:

* **Load CSV**
* **Load JSON**

Replace these components with the **File** component, which supports loading CSV and JSON files, as well as many other file types.

## See also

- [**Google** bundle](/bundles-google)
- [**Composio** bundle](/bundles-composio)
- [File management](/concepts-file-management)


================================================================================
filename: Components/components-embedding-models.mdx
================================================================================

---
title: Embedding Model
slug: /components-embedding-models
---

import Icon from "@site/src/components/icon";

Embedding model components in Langflow generate text embeddings using a specified Large Language Model (LLM).

Langflow includes an **Embedding Model** core component that has built-in support for some LLMs.
Alternatively, you can use any [additional embedding model](#additional-embedding-models) in place of the **Embedding Model** core component.

## Use embedding model components in a flow

Use embedding model components anywhere you need to generate embeddings in a flow.

This example shows how to use an embedding model component in a flow to create a semantic search system.
This flow loads a text file, splits the text into chunks, generates embeddings for each chunk, and then loads the chunks and embeddings into a vector store. The input and output components allow a user to query the vector store through a chat interface.

![A semantic search flow that uses Embedding Model, File, Split Text, Chroma DB, Chat Input, and Chat Output components](/img/component-embedding-models-add-chat.png)

1. Create a flow, add a **File** component, and then select a file containing text data, such as a PDF, that you can use to test the flow.

2. Add the **Embedding Model** core component, and then provide a valid OpenAI API key.
You can enter the API key directly or use a <Icon name="Globe" aria-hidden="true"/> [global variable](/configuration-global-variables).

    :::tip My preferred provider or model isn't listed
    If your preferred embedding model provider or model isn't supported by the **Embedding Model** core component, you can use any [additional embedding models](#additional-embedding-models) in place of the core component.

    Browse <Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) or <Icon name="Search" aria-hidden="true" /> **Search** for your preferred provider to find additional embedding models, such as the [**Hugging Face Embeddings Inference** component](/bundles-huggingface#hugging-face-embeddings-inference).
    :::

3. Add a [**Split Text** component](/components-processing#split-text) to your flow.
This component splits text input into smaller chunks to be processed into embeddings.

4. Add a vector store component, such as the **Chroma DB** component, to your flow, and then configure the component to connect to your vector database.
This component stores the generated embeddings so they can be used for similarity search.

5. Connect the components:

    * Connect the **File** component's **Loaded Files** output to the **Split Text** component's **Data or DataFrame** input.
    * Connect the **Split Text** component's **Chunks** output to the vector store component's **Ingest Data** input.
    * Connect the **Embedding Model** component's **Embeddings** output to the vector store component's **Embedding** input.

6. To query the vector store, add [**Chat Input and Output** components](/components-io#chat-io):

    * Connect the **Chat Input** component to the vector store component's **Search Query** input.
    * Connect the vector store component's **Search Results** output to the **Chat Output** component.

7. Click **Playground**, and then enter a search query to retrieve text chunks that are most semantically similar to your query.

## Embedding Model parameters

The following parameters are for the **Embedding Model** core component.
Other embedding model components can have additional or different parameters.

import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<PartialParams />

| Name | Display Name | Type | Description |
|------|--------------|------|-------------|
| provider | Model Provider | List | Input parameter. Select the embedding model provider. |
| model | Model Name | List | Input parameter. Select the embedding model to use.|
| api_key | OpenAI API Key | Secret[String] | Input parameter. The API key required for authenticating with the provider. |
| api_base | API Base URL | String | Input parameter. Base URL for the API. Leave empty for default. |
| dimensions | Dimensions | Integer | Input parameter. The number of dimensions for the output embeddings. |
| chunk_size | Chunk Size | Integer | Input parameter. The size of text chunks to process. Default: `1000`. |
| request_timeout | Request Timeout | Float | Input parameter. Timeout for API requests. |
| max_retries | Max Retries | Integer | Input parameter. Maximum number of retry attempts. Default: `3`. |
| show_progress_bar | Show Progress Bar | Boolean | Input parameter. Whether to display a progress bar during embedding generation. |
| model_kwargs | Model Kwargs | Dictionary | Input parameter. Additional keyword arguments to pass to the model. |
| embeddings | Embeddings | Embeddings | Output parameter. An instance for generating embeddings using the selected provider. |

## Additional embedding models

If your provider or model isn't supported by the **Embedding Model** core component, you can replace this component with any other component that generates embeddings.

To find additional embedding model components, browse <Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) or <Icon name="Search" aria-hidden="true" /> **Search** for your preferred provider.

## Pair models with vector stores

import PartialVectorRagBlurb from '@site/docs/_partial-vector-rag-blurb.mdx';

<PartialVectorRagBlurb />

<details>
<summary>Example: Vector search flow</summary>

import PartialVectorRagFlow from '@site/docs/_partial-vector-rag-flow.mdx';

<PartialVectorRagFlow />

</details>


================================================================================
filename: Components/components-helpers.mdx
================================================================================

---
title: Helpers
slug: /components-helpers
---

import Icon from "@site/src/components/icon";
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

Helper components provide utility functions to help manage data and perform simple tasks in your flow.

## Calculator

The **Calculator** component performs basic arithmetic operations on mathematical expressions.
It supports addition, subtraction, multiplication, division, and exponentiation operations.

For an example of using this component in a flow, see the [**Python Interpreter** component](/components-processing#python-interpreter).

### Calculator parameters

| Name | Type | Description |
|------|------|-------------|
| expression | String | Input parameter. The arithmetic expression to evaluate, such as `4*4*(33/22)+12-20`. |
| result | Data | Output parameter. The calculation result as a [`Data` object](/data-types) containing the evaluated expression. |

## Current Date

The **Current Date** component returns the current date and time in a selected timezone. This component provides a flexible way to obtain timezone-specific date and time information within a Langflow pipeline.

### Current Date parameters

| Name | Type | Description |
|------|------|-------------|
| timezone | String | Input parameter. The timezone for the current date and time. |
| current_date | String | Output parameter. The resulting current date and time in the selected timezone. |

## Message History

The **Message History** component provides combined chat history and message storage functionality.
It can store and retrieve chat messages from either [Langflow storage](/memory) _or_ a dedicated chat memory database like Mem0 or Redis.

:::tip
The **Agent** component has built-in chat memory that is enabled by default and uses Langflow storage.
This built-in chat memory functionality is sufficient for most use cases.

Use the **Message History** component for the following use cases:

* You need to store and retrieve chat memory for a language model component (not an agent).
* You need to retrieve chat memories outside the chat context, such as a sentiment analysis flow that retrieves and analyzes recently stored memories.
* You want to store memories in a specific database that is separate from Langflow storage.

For more information, see [Store chat memory](/memory#store-chat-memory).
:::

### Use the Message History component in a flow

The **Message History** component has two modes, depending on where you want to use it in your flow:

* **Retrieve mode**: The component retrieves chat messages from your Langflow database or external memory.
* **Store mode**: The component stores chat messages in your Langflow database or external memory.

This means that you need multiple **Message History** components in your flow if you want to both store and retrieve chat messages.

<Tabs>
<TabItem value="langflow" label="Use Langflow storage" default>

The following steps explain how to create a chat-based flow that uses **Message History** components to store and retrieve chat memory from your Langflow installation's database:

1. Create or edit a flow where you want to use chat memory.

2. At the beginning of the flow, add a **Message History** component, and then set it to **Retrieve** mode.

3. Optional: In the **Message History** [component's header menu](/concepts-components#component-menus), click <Icon name="SlidersHorizontal" aria-hidden="true"/> **Controls** to enable parameters for memory sorting, filtering, and limits.

3. Add a **Prompt Template** component, add a `{memory}` variable to the **Template** field, and then connect the **Message History** output to the **memory** input.

    The **Prompt Template** component supplies instructions and context to LLMs, separate from chat messages passed through a **Chat Input** component.
    The template can include any text and variables that you want to supply to the LLM, for example:

    ```text
    You are a helpful assistant that answers questions.

    Use markdown to format your answer, properly embedding images and urls.

    History:

    {memory}
    ```

    Variables (`{variable}`) in the template dynamically add fields to the **Prompt Template** component so that your flow can receive definitions for those values from elsewhere, such as other components, Langflow global variables, or runtime input.
    For more information, see [Define variables in prompts](/components-prompts#define-variables-in-prompts).

    In this example, the `{memory}` variable is populated by the retrieved chat memories, which are then passed to a **Language Model** or **Agent** component to provide additional context to the LLM.

4. Connect the **Prompt Template** component's output to a **Language Model** component's **System Message** input.

    This example uses the **Language Model** core component as the central chat driver, but you can also use another language model component or the **Agent** component.

5. Add a **Chat Input** component, and then connect it to the **Language Model** component's **Input** field.

6. Connect the **Language Model** component's output to a **Chat Output** component.

7. At the end of the flow, add another **Message History** component, and then set it to **Store** mode.

    Configure any additional parameters in the second **Message History** component as needed, taking into consideration that this particular component will store chat messages rather than retrieve them.

8. Connect the **Chat Output** component's output to the **Message History** component's **Message** input.

    Each response from the LLM is output from the **Language Model** component to the **Chat Output** component, and then stored in chat memory by the final **Message History** component.

</TabItem>
<TabItem value="external" label="Use external chat memory">

To store and retrieve chat memory from a dedicated, external chat memory database, use the **Message History** component _and_ a provider-specific chat memory component.

The following steps explain how to create a flow that stores and retrieves chat memory from a [**Redis Chat Memory** component](/bundles-redis).
Other options include the [**Mem0 Chat Memory** component](/bundles-mem0) and [**Cassandra Chat Memory** component](/bundles-cassandra#cassandra-chat-memory).

1. Create or edit a flow where you want to use chat memory.

2. At the beginning of the flow, add a **Message History** component and a **Redis Chat Memory** component:

   1. Configure the **Redis Chat Memory** component to connect to your Redis database. For more information, see the [Redis documentation](https://redis.io/docs/latest/).
   2. Set the **Message History** component to **Retrieve** mode.
   3. In the **Message History** [component's header menu](/concepts-components#component-menus), click <Icon name="SlidersHorizontal" aria-hidden="true"/> **Controls**, enable **External Memory**, and then click **Close**.

      In **Controls**, you can also enable parameters for memory sorting, filtering, and limits.

   4. Connect the **Redis Chat Memory** component's output to the **Message History** component's **External Memory** input.

3. Add a **Prompt Template** component, add a `{memory}` variable to the **Template** field, and then connect the **Message History** output to the **memory** input.

    The **Prompt Template** component supplies instructions and context to LLMs, separate from chat messages passed through a **Chat Input** component.
    The template can include any text and variables that you want to supply to the LLM, for example:

    ```text
    You are a helpful assistant that answers questions.

    Use markdown to format your answer, properly embedding images and urls.

    History:

    {memory}
    ```

    Variables (`{variable}`) in the template dynamically add fields to the **Prompt Template** component so that your flow can receive definitions for those values from elsewhere, such as other components, Langflow global variables, or runtime input.
    For more information, see [Define variables in prompts](/components-prompts#define-variables-in-prompts).

    In this example, the `{memory}` variable is populated by the retrieved chat memories, which are then passed to a **Language Model** or **Agent** component to provide additional context to the LLM.

4. Connect the **Prompt Template** component's output to a **Language Model** component's **System Message** input.

    This example uses the **Language Model** core component as the central chat driver, but you can also use another language model component or the **Agent** component.

5. Add a **Chat Input** component, and then connect it to the **Language Model** component's **Input** input.

6. Connect the **Language Model** component's output to a **Chat Output** component.

7. At the end of the flow, add another pair of **Message History** and **Redis Chat Memory** components:

   1. Configure the **Redis Chat Memory** component to connect to your Redis database.
   2. Set the **Message History** component to **Store** mode.
   3. In the **Message History** [component's header menu](/concepts-components#component-menus), click <Icon name="SlidersHorizontal" aria-hidden="true"/> **Controls**, enable **External Memory**, and then click **Close**.

       Configure any additional parameters in this component as needed, taking into consideration that this particular component will store chat messages rather than retrieve them.

   4. Connect the **Redis Chat Memory** component to the **Message History** component's **External Memory** input.

8. Connect the **Chat Output** component's output to the **Message History** component's **Message** input.

    Each response from the LLM is output from the **Language Model** component to the **Chat Output** component, and then stored in chat memory by passing it to the final **Message History** and **Redis Chat Memory** components.

![A flow with Message History and Redis Chat Memory components](/img/component-message-history-external-memory.png)

</TabItem>
</Tabs>

### Message History parameters

import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<PartialParams />

The available parameters depend on whether the component is in **Retrieve** or **Store** mode.

<Tabs>
<TabItem value="retrieve" label="Retrieve mode">

| Name | Type | Description |
|------|------|-------------|
| **Template** (`template`) | String | Input parameter. The template to use for formatting the data. It can contain the keys `{text}`, `{sender}` or any other key in the message data. |
| **External Memory** (`memory`) | External Memory | Input parameter. Retrieve messages from an external memory. If empty, Langflow storage is used. |
| **Number of Messages** (`n_messages`) | Integer | Input parameter. The number of messages to retrieve. Default: 100. |
| **Order** (`order`) | String | Input parameter. The order of the messages. Default: `Ascending`. |
| **Sender Type** (`sender_type`) | String | Input parameter. Filter by sender type, one of `User`, `Machine`, or `Machine and User` (default). |
| **Session ID** (`session_id`) | String | Input parameter. The [session ID](/session-id) of the chat memories to retrieve. If omitted or empty, the current session ID for the flow run is used. |

</TabItem>
<TabItem value="store" label="Store mode">

| Name | Type | Description |
|------|------|-------------|
| **Template** (`template`) | String | Input parameter. The template to use for formatting the data. It can contain the keys `{text}`, `{sender}` or any other key in the message data. |
| **Message** (`message`) | String | Input parameter. The message to store, typically provided by connecting a **Chat Output** component. |
| **External Memory** (`memory`) | External Memory | Input parameter. Store messages in external memory. If empty, Langflow storage is used. |
| **Sender** (`sender`) | String | Input parameter. Choose which messages to store based on sender, one of `User`, `Machine`, or `Machine and User` (default). |
| **Sender Name** (`sender_name`) | String | Input parameter. A backup `sender` label to use if a message doesn't have sender metadata. |
| **Session ID** (`session_id`) | String | Input parameter. The [session ID](/session-id) of the chat memories to store. If omitted or empty, the current session ID for the flow run is used. Use custom session IDs if you need to segregate chat memory for different users or applications that run the same flow. |
| **Sender Type** (`sender_type`) | String | Input parameter. Filter by sender type, one of `User`, `Machine`, or `Machine and User` (default). |

</TabItem>
</Tabs>

### Message History output

Memories can be retrieved in one of two formats:

* **Message**: Retrieve memories as `Message` objects, including `messages_text` containing retrieved chat message text.
This is the typical output format used to pass memories _as chat messages_ to another component.

* **DataFrame**: Returns memories as a `DataFrame` containing the message data.
Useful for cases where you need to retrieve memories in a tabular format rather than as chat messages.

You can set the output type near the component's output port.

## Legacy Helper components

import PartialLegacy from '@site/docs/_partial-legacy.mdx';

<PartialLegacy />

The following Helper components are in legacy status:

* **Message Store**: Replaced by the [**Message History** component](#message-history)
* **Create List**: Replace with [Processing components](/components-processing)
* **ID Generator**: Replace with a component that executes arbitrary code to generate an ID or embed an ID generator script your application code (external to your Langflow flows).
* **Output Parser**: Replace with the [**Structured Output** component](/components-processing#structured-output) and [**Parser** component](/components-processing#parser).
The components you need depend on the data types and complexity of the parsing task.

    The **Output Parser** component transformed the output of a language model into comma-separated values (CSV) format, such as `["item1", "item2", "item3"]`, using LangChain's `CommaSeparatedListOutputParser`.
    The **Structured Output** component is a good alternative for this component because it also formats LLM responses with support for custom schemas and more complex parsing.

    **Parsing** components only provide formatting instructions and parsing functionality.
    _They don't include prompts._
    You must connect parsers to **Prompt Template** components to create prompts that LLMs can use.


================================================================================
filename: Components/components-io.mdx
================================================================================

---
title: Input / Output
slug: /components-io
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

Input and output components define where data enters and exits your flow, but they don't have identical functionality.

Specifically, **Chat Input and Output** components are designed to facilitate conversational interactions where messages are exchanged in a cumulative dialogue.
The data handled by these components includes the message text plus additional metadata like senders, session IDs, and timestamps.

In contrast, **Text Input and Output** components are designed for simple string input and output that doesn't require the additional context and metadata associated with chat messages.
The data handled by these components is pared down to basic text strings.

## Chat Input and Output {#chat-io}

:::warning
**Chat Input and Output** components are required to chat with your flow in the **Playground**.
For more information, see [Test flows in the Playground](/concepts-playground).
:::

**Chat Input and Output** components are designed to handle conversational interactions in Langflow.

### Chat Input

The **Chat Input** component accepts text and file input, such as a chat message or a file.
This data is passed to other components as [`Message` data](/data-types) containing the provided input as well as associated chat metadata, such as the sender, session ID, timestamp, and file attachments.

Initial input should _not_ be provided as a complete `Message` object because the **Chat Input** component constructs the `Message` object that is then passed to other components in the flow.

#### Chat Input parameters

<PartialParams />

| Name | Display Name | Info |
|------|--------------|------|
|input_value|Input Text| Input parameter. The message text string to be passed as input. |
|sender|Sender Type| Input parameter. Identifies the sender as either `User` or `Language Model`.|
|sender_name|Sender Name| Input parameter. The name of the sender. If unspecified, defaults to `User` or `Language Model`. |
|session_id|Session ID| Input parameter. The unique identifier for the chat session. If empty, the current session ID parameter is used.|
|files|Files| Input parameter. The files to be sent with the message.|
|background_color|Background Color| Input parameter. The background color of the icon.|
|chat_icon|Icon| Input parameter. The icon of the message.|
|should_store_message|Store Messages| Input parameter. Whether to store the message in chat history.|
|text_color|Text Color| Input parameter. The text color of the name.|

For information about the resulting `Message` object, including input parameters that are directly mapped to `Message` attributes, see [`Message` data](/data-types#message).

<details>
<summary>Message method for Chat Input</summary>

The `ChatInput` class provides an asynchronous method to create and store a `Message` object based on the input parameters.
The `Message` object is created in the `message_response` method of the `ChatInput` class using the `Message.create()` factory method.

```python
message = await Message.create(
    text=self.input_value,
    sender=self.sender,
    sender_name=self.sender_name,
    session_id=self.session_id,
    files=self.files,
    properties={
        "background_color": background_color,
        "text_color": text_color,
        "icon": icon,
    },
)
```

</details>

### Chat Output

The **Chat Output** component ingests `Message`, `Data`, or `DataFrame` data from other components, transforms it into `Message` data if needed, and then emits the final output as a chat message.
For information about these data types, see [Use Langflow data types](/data-types).

In the **Playground**, chat output is limited to the parts of the `Message` object that are relevant to the chat interface, such as the text response, sender name, and file attachments.
To see the metadata associated with a chat message, inspect the message logs in the **Playground**.

When using the Langflow API, the API response includes the **Chat Output** `Message` object along with other response data from the flow run.
Langflow API responses can be extremely verbose, so your applications must include code to extract relevant data from the response to return to the user.
For an example, see the [Langflow quickstart](/get-started-quickstart).

#### Chat Output parameters

<PartialParams />

| Name | Display Name | Info |
|------|--------------|------|
|input_value|Inputs| Input parameter. The message text string to be passed as output. |
|should_store_message|Store Messages| Input parameter. Whether to store the message in chat history.|
|sender|Sender Type| Input parameter. Identifies the sender as either `User` or `Language Model`.|
|sender_name|Sender Name| Input parameter. The name of the sender. If unspecified, defaults to `User` or `Language Model`. |
|session_id|Session ID| Input parameter. The unique identifier for the chat session. If empty, the current session ID parameter is used.|
|data_template|Data Template| Input parameter. The template to convert [`Data` input](/data-types#data) to `text`. If empty, it is dynamically set to the `Data` object's `text` key.|
|background_color|Background Color| Input parameter. The background color of the icon.|
|chat_icon|Icon| Input parameter. The icon of the message.|
|text_color|Text Color| Input parameter. The text color of the name.|
|clean_data|Basic Clean Data| Input parameter. When enabled, [`DataFrame` input](/data-types#dataframe) is cleaned when converted to text. Cleaning removes empty rows, empty lines in cells, and multiple newlines.|

For information about the resulting `Message` object, including input parameters that are directly mapped to `Message` attributes, see [`Message` data](/data-types#message).

### Use Chat Input and Output components in a flow

To use the **Chat Input** and **Chat Output** components in a flow, connect them to components that accept or emit [`Message` data](/data-types#message).

For example, the following flow connects **Chat Input** and **Chat Output** to a **Language Model** component, creating a simple LLM-based chat flow.

![Chat Input and Output components connected to an OpenAI component](/img/component-chat-io.png)

:::tip
For detailed examples of **Chat Input and Output** components in flows, see the following:

* [Langflow quickstart](/get-started-quickstart): Create and run a basic agent flow.
* **Basic Prompting** template: Create an LLM-based chat flow that accepts chat input as well as a prompt with additional instructions for the LLM. Many other Langflow templates also use **Chat Input and Output** components.
* [Connect applications to agents](/agent-tutorial): Explore more advanced concepts around agent flows and prompting, including triggering agent flows from external applications.
:::

### Send chat input with the Langflow API

You can use the Langflow API to run a flow by sending input to a **Chat Input** component:

```bash
curl --request POST \
  --url "http://$LANGFLOW_SERVER_ADDRESS/api/v1/run/$FLOW_ID" \
  --header "Content-Type: application/json" \
  --header "x-api-key: $LANGFLOW_API_KEY" \
  --data '{
  "input_value": "What's the recommended way to install Docker on Mac M1?",
  "output_type": "chat",
  "input_type": "chat"
}'
```

When triggering flows with the Langflow API, the payload must contain the values for the **Chat Input** component's input parameters, such as `input_value`.

Not all parameters need to be specified in the request.
For example, `session_id` uses the flow's default session ID if omitted.
If you want to use a custom session ID, include `session_id` in your request:

```bash
curl --request POST \
  --url "http://$LANGFLOW_SERVER_ADDRESS/api/v1/run/$FLOW_ID" \
  --header "Content-Type: application/json" \
  --header "x-api-key: $LANGFLOW_API_KEY" \
  --data '{
  "input_value": "Whats the recommended way to install Docker on Mac M1",
  "session_id": "$USER_ID",
  "output_type": "chat",
  "input_type": "chat"
}'
```

For more information, see [Trigger flows with the Langflow API](/concepts-publish).

## Text Input and Output {#text-io}

:::warning
**Text Input and Output** components aren't supported in the **Playground**.
Because the data isn't formatted as a chat message, the data doesn't appear in the **Playground**, and you can't chat with your flow in the **Playground**.

If you want to chat with a flow in the **Playground**, you must use the [**Chat Input and Output** components](#chat-io).
:::

**Text Input and Output** components are designed for flows that ingest or emit simple text strings.
These components don't support full conversational interactions.

Passing chat-like metadata to a **Text Input and Output** component doesn't change the component's behavior; the result is still a simple text string.

### Text Input

The **Text Input** component accepts a text string input that is passed to other components as [`Message` data](/data-types) containing only the provided input text string in the `text` attribute.

It accepts only **Text** (`input_value`), which is the text supplied as input to the component.
This can be entered directly into the component or passed as `Message` data from other components.

Initial input _shouldn't_ be provided as a complete `Message` object because the **Text Input** component constructs the `Message` object that is then passed to other components in the flow.

### Text Output

The **Text Output** component ingests [`Message` data](/data-types#message) from other components, emitting only the `text` attribute in a simplified `Message` object.

It accepts only **Text** (`input_value`), which is the text to be ingested and output as a string.
This can be entered directly into the component or passed as `Message` data from other components.


================================================================================
filename: Components/components-logic.mdx
================================================================================

---
title: Logic
slug: /components-logic
---

import Icon from "@site/src/components/icon";
import PartialParams from '@site/docs/_partial-hidden-params.mdx';

Logic components provide functionalities for routing, conditional processing, and flow management.

## If-Else (conditional router) {#if-else}

The **If-Else** component is a conditional router that routes messages by comparing two strings.
It evaluates a condition by comparing two text inputs using the specified operator, and then routes the message to `true_result` or `false_result` depending on the evaluation.

The operator looks for single strings in the input (`input_text`) based on an operator and match text (`match_text`), but it can also search for multiple words by matching a regex.
Available operators include:

- **equals**: Exact match comparison
- **not equals**: Inverse of exact match
- **contains**: Checks if the `match_text` is found within `input_text`
- **starts with**: Checks if `input_text` begins with `match_text`
- **ends with**: Checks if `input_text` ends with `match_text`
- **regex**: Matches on a case-sensitive pattern

By default, all operators are case insensitive except **regex**.
**regex** is always case sensitive, and you can enable case sensitivity for all other operators in the [If-Else parameters](#if-else-parameters).

### Use the If-Else component in a flow

The following example uses the **If-Else** component to check incoming chat messages with regex matching, and then output a different response depending on whether the match evaluated to true or false.

![A conditional router connected to two OpenAI components](/img/component-conditional-router.png)

1. Add an **If-Else** component to your flow, and then configure it as follows:

    * **Text Input**: Connect the **Text Input** port to a **Chat Input** component or another `Message` input.

        If your input isn't in `Message` format, you can use another component to transform it, such as the [**Type Convert** component](/components-processing#type-convert) or [**Parser** component](/components-processing#parser).
        If your input isn't appropriate for `Message` format, consider using another component for conditional routing, such as the [**Data Operations** component](/components-processing#data-operations).

    * **Match Text**: Enter `.*(urgent|warning|caution).*` so the component looks for these values in incoming input. The regex match is case sensitive, so if you need to look for all permutations of `warning`, enter `warning|Warning|WARNING`.

    * **Operator**: Select **regex**.

    * **Case True**: In the [component's header menu](/concepts-components#component-menus), click <Icon name="SlidersHorizontal" aria-hidden="true"/> **Controls**, enable the **Case True** parameter, click **Close**, and then enter `New Message Detected` in the field.

        The **Case True** message is sent from the **True** output port when the match condition evaluates to true.

        No message is set for **Case False** so the component doesn't emit a message when the condition evaluates to false.

3. Depending on what you want to happen when the outcome is **True**, add components to your flow to execute that logic:

    1. Add a **Language Model**, **Prompt Template**, and **Chat Output** component to your flow.

    2. In the **Language Model** component, enter your OpenAI API key or select a different provider and model.

    3. Connect the **If-Else** component's **True** output port to the **Language Model** component's **Input** port.

    4. In the **Prompt Template** component, enter instructions for the model when the evaluation is true, such as `Send a message that a new warning, caution, or urgent message was received`.

    5. Connect the **Prompt Template** component to the **Language Model** component's **System Message** port.

    6. Connect the **Language Model** component's output to the **Chat Output** component.

4. Repeat the same process with another set of **Language Model**, **Prompt Template**, and **Chat Output** components for the **False** outcome.

    Connect the **If-Else** component's **False** output port to the second **Language Model** component's **Input** port.
    In the second **Prompt Template**, enter instructions for the model when the evaluation is false, such as `Send a message that a new low-priority message was received`.

5. To test the flow, open the **Playground**, and then send the flow some messages with and without your regex strings.
The chat output should reflect the instructions in your prompts based on the regex evaluation.

    ```text
    User: A new user was created.

    AI: A new low-priority message was received.

    User: Sign-in warning: new user locked out.

    AI: A new warning, caution, or urgent message was received. Please review it at your earliest convenience.
    ```

### If-Else parameters

<PartialParams />

| Name           | Type     | Description                                                       |
|----------------|----------|-------------------------------------------------------------------|
| input_text     | String   | Input parameter. The primary text input for the operation. |
| match_text     | String   | Input parameter. The text to compare against. |
| operator       | Dropdown | Input parameter. The operator used to compare texts. Options include `equals`, `not equals`, `contains`, `starts with`, `ends with`, and `regex`. The default is `equals`. |
| case_sensitive | Boolean  | Input parameter. When `true`, the comparison is case sensitive. The default is `false`. This setting doesn't apply to regex comparisons. |
| max_iterations | Integer  | Input parameter. The maximum number of iterations allowed for the conditional router. The default is 10. |
| default_route  | Dropdown | Input parameter. The route to take when max iterations are reached. Options include `true_result` or `false_result`. The default is `false_result`. |
| true_result  | Message | Output parameter. The output produced when the condition is true. |
| false_result | Message | Output parameter. The output produced when the condition is false. |

## Loop

The **Loop** component iterates over a list of input by passing individual items to other components attached at the **Item** output port until there are no items left to process.
Then, the **Loop** component passes the aggregated result of all looping to the component connected to the **Done** port.

### The looping process

The **Loop** component is like a miniature flow within your flow.
Here's a breakdown of the looping process:

1. Accepts a list of [`Data`](/data-types#data) or [`DataFrame`](/data-types#dataframe) objects, such as a CSV file, through the **Loop** component's **Inputs** port.

2. Splits the input into individual items. For example, a CSV file is broken down by rows.

    Specifically, the **Loop** component repeatedly extracts items by `text` key in the `Data` or `DataFrame` objects until there are no more items to extract.
    Each `item` output is a `Data` objects.

3. Iterates over each `item` by passing them to the **Item** output port.

    This port connects to one or more components that perform actions on each item.
    The final component in the **Item** loop connects back to the **Loop** component's **Looping** port to process the next item.

    Only one component connects to the **Item** port, but you can pass the data through as many components as you need, as long as the last component in the chain connects back to the **Looping** port.

    The **If-Else** component isn't compatible with the **Loop** component.
    For more information, see [Conditional looping](#conditional-looping).

4. After processing all items, the results are aggregated into a single `Data` object that is passed from the **Loop** component's **Done** port to the next component in the flow.

The following simplified Python code summarizes how the **Loop** component works.
This _isn't_ the actual component code; it is only meant to help you understand the general process.

```python
for i in input:             # Receive input data as a list
    process_item(i)         # Process each item through components connected at the Item port
    if has_more_items():
        continue            # Loop back to Looping port to process the next item
    else:
        break               # Exit the loop when no more items are left

done = aggregate_results()  # Compile all returned items

print(done)                 # Send the aggregated results from the Done port to another component
```

### Loop example

In the follow example, the **Loop** component iterates over a CSV file until there are no rows left to process.
In this case, the **Item** port passes each row to a **Type Convert** component to convert the row into a `Message` object, passes the `Message` to a **Structured Output** component to be processed into structured data that is then passed back to the **Loop** component's **Looping** port.
After processing all rows, the **Loop** component loads the aggregated list of structured data into a Chroma DB database through the **Chroma DB** component connected to the **Done** port.

![Loop CSV parser](/img/component-loop-csv.png)

:::tip
For more examples of the **Loop** component, try the **Research Translation Loop** template in Langflow, or see the video tutorial [Mastering the Loop Component & Agentic RAG in Langflow](https://www.youtube.com/watch?v=9Wx7WODSKTo).
:::

### Conditional looping

The **If-Else** component isn't compatible with the **Loop** component.
If you need conditional loop events, redesign your flow to process conditions before the loop.
For example, if you are looping over a `DataFrame`, you could use multiple [**DataFrame Operations** components](/components-processing#dataframe-operations) to conditionally filter data, and then run separate loops on each set of filtered data.

![A flow with conditional looping.](/img/conditional-looping.png)

## Notify and Listen

The **Notify** and **Listen** components are used together.

The **Notify** component builds a notification from the current flow's context, including specific data content and a status identifier.

The resulting notification is sent to the **Listen** component.
The notification data can then be passed to other components in the flow, such as the **If-Else** component.

## Run flow

The **Run Flow** component runs another Langflow flow as a subprocess of the current flow.

You can use this component to chain flows together, run flows conditionally, and attach flows to [**Agent** components](/components-agents) as [tools for agents](/agents-tools) to run as needed.

When used with an agent, the `name` and `description` metadata that the agent uses to register the tool are created automatically.

When you select a flow for the **Run Flow** component, it uses the target flow's graph structure to dynamically generate input and output fields on the **Run Flow** component.

### Run Flow parameters

<PartialParams />

| Name              | Type     | Description                                                    |
|-------------------|----------|----------------------------------------------------------------|
| flow_name_selected| Dropdown | Input parameter. The name of the flow to run.                  |
| session_id        | String   | Input parameter. The session ID for the flow run, if you want to pass a custom session ID for the subflow. |
| flow_tweak_data   | Dict     | Input parameter. Dictionary of tweaks to customize the flow's behavior. Available tweaks depend on the selected flow. |
| dynamic inputs    | Various  | Input parameter. Additional inputs are generated based on the selected flow. |
| run_outputs  | A `List` of types (`Data`, `Message`, or `DataFrame`)  | Output parameter. All outputs are generated from running the flow.  |

## Legacy Logic components

import PartialLegacy from '@site/docs/_partial-legacy.mdx';

<PartialLegacy />

The following Logic components are in legacy status:

<details>
<summary>Condition</summary>

As an alternative to this legacy component, see the [**If-Else** component](#if-else).

The **Condition** component routes `Data` objects based on a condition applied to a specified key, including Boolean validation.
It supports `true_output` and `false_output` for routing the results based on the condition evaluation.

This component is useful in workflows that require conditional routing of complex data structures, enabling dynamic decision-making based on data content.

It can process either a single `Data` object or a list of `Data` objects.
The following actions occur when processing a list of `Data` objects:

- Each object in the list is evaluated individually.
- Objects meeting the condition go to `true_output`.
- Objects not meeting the condition go to `false_output`.
- If all objects go to one output, the other output is empty.

The **Condition** component accepts the following parameters:

| Name          | Type     | Description                                 |
|---------------|----------|---------------------------------------------|
| data_input    | Data     | Input parameter. The Data object or list of Data objects to process. This input can handle both single items and lists. |
| key_name      | String   | Input parameter. The name of the key in the Data object to check.     |
| operator      | Dropdown | Input parameter. The operator to apply. Options: `equals`, `not equals`, `contains`, `starts with`, `ends with`, `boolean validator`. Default: `equals`. |
| compare_value | String   | Input parameter. The value to compare against. Not shown/used when operator is `boolean validator`. |

The `operator` options have the following behaviors:

- `equals`: Exact match comparison between the key's value and compare_value.
- `not equals`: Inverse of exact match.
- `contains`: Checks if compare_value is found within the key's value.
- `starts with`: Checks if the key's value begins with compare_value.
- `ends with`: Checks if the key's value ends with compare_value.
- `boolean validator`: Treats the key's value as a Boolean. The following values are considered true:
  - Boolean `true`.
  - Strings: `true`, `1`, `yes`, `y`, `on` (case-insensitive)
  - Any other value is converted using Python's `bool()` function

</details>

<details>
<summary>Pass</summary>

As an alternative to this legacy component, use the [**If-Else** component](#if-else) to pass a message without modification.

The **Pass** component forwards the input message without modification.

It accepts the following parameters:

| Name | Display Name | Info |
|------|--------------|------|
| input_message | Input Message | Input parameter. The message to forward. |
| ignored_message | Ignored Message | Input parameter. A second message that is ignored. Used as a workaround for continuity. |
| output_message | Output Message | Output parameter. The forwarded message from the input. |

</details>

<details>
<summary>Flow As Tool</summary>

This component constructed a tool from a function that ran a loaded flow.

It was deprecated in Langflow version 1.1.2 and replaced by the [**Run Flow** component](#run-flow).

</details>

<details>
<summary>Sub Flow</summary>

This component integrated entire flows as components within a larger workflow.
It dynamically generated inputs based on the selected flow and executed the flow with provided parameters.

It was deprecated in Langflow version 1.1.2 and replaced by the [**Run Flow** component](#run-flow).

</details>


================================================================================
filename: Components/components-models.mdx
================================================================================

---
title: Language Model
slug: /components-models
---

import Icon from "@site/src/components/icon";
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

Language model components in Langflow generate text using a specified Large Language Model (LLM).
These components accept inputs like chat messages, files, and instructions in order to generate a text response.

Langflow includes a **Language Model** core component that has built-in support for many LLMs.
Alternatively, you can use any [additional language model](#additional-language-models) in place of the **Language Model** core component.

## Use language model components in flows

Use language model components anywhere you would use an LLM in a flow.

<Tabs>
<TabItem value="chat" label="Chat" default>

One of the most common use cases of language model components is to chat with LLMs in your flows.

The following example uses a language model component in a chatbot flow similar to the **Basic Prompting** template.

1. Add the **Language Model** core component to your flow, and then enter your OpenAI API key.

    This example uses the **Language Model** core component's default OpenAI model.
    If you want to use a different provider or model, edit the **Model Provider**, **Model Name**, and **API Key** fields accordingly.

    :::tip My preferred provider or model isn't listed
    If you want to use a provider or model that isn't built-in to the **Language Model** core component, you can replace this component with any [additional language model](#additional-language-models).

    Browse <Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) or <Icon name="Search" aria-hidden="true" /> **Search** for your preferred provider to find additional language models.
    :::

3. In the [component's header menu](/concepts-components#component-menus), click <Icon name="SlidersHorizontal" aria-hidden="true"/> **Controls**, enable the **System Message** parameter, and then click **Close**.

4. Add a [**Prompt Template** component](/components-prompts) to your flow.

5. In the **Template** field, enter some instructions for the LLM, such as `You are an expert in geography who is tutoring high school students`.

6. Connect the **Prompt Template** component's output to the **Language Model** component's **System Message** input.

7. Add [**Chat Input** and **Chat Output** components](/components-io#chat-io) to your flow.
These components are required for direct chat interaction with an LLM.

8. Connect the **Chat Input** component to the **Language Model** component's **Input**, and then connect the **Language Model** component's **Message** output to the **Chat Output** component.

    ![A basic prompting flow with Language Model, Prompt Template, Chat Input, and Chat Output components](/img/component-language-model.png)

9. Open the **Playground**, and ask a question to chat with the LLM and test the flow, such as `What is the capital of Utah?`.

    <details>
    <summary>Result</summary>

    The following response is an example of an OpenAI model's response.
    Your actual response may vary based on the model version at the time of your request, your template, and input.

    ```
    The capital of Utah is Salt Lake City. It is not only the largest city in the state but also serves as the cultural and economic center of Utah. Salt Lake City was founded in 1847 by Mormon pioneers and is known for its proximity to the Great Salt Lake and its role in the history of the Church of Jesus Christ of Latter-day Saints. For more information, you can refer to sources such as the U.S. Geological Survey or the official state website of Utah.
    ```

    </details>

10. Optional: Try a different model or provider to see how the response changes.
For example, if you are using the **Language Model** core component, you could try an Anthropic model.

    Then, open the **Playground**, ask the same question as you did before, and then compare the content and format of the responses.

    This helps you understand how different models handle the same request so you can choose the best model for your use case.
    You can also learn more about different models in each model provider's documentation.

    <details>
    <summary>Result</summary>

    The following response is an example of an Anthropic model's response.
    Your actual response may vary based on the model version at the time of your request, your template, and input.

    Note that this response is shorter and includes sources, whereas the previous OpenAI response was more encyclopedic and didn't cite sources.

    ```
    The capital of Utah is Salt Lake City. It is also the most populous city in the state. Salt Lake City has been the capital of Utah since 1896, when Utah became a state.
    Sources:
    Utah State Government Official Website (utah.gov)
    U.S. Census Bureau
    Encyclopedia Britannica
    ```

    </details>

</TabItem>
<TabItem value="drivers" label="Drivers">

Some components use a language model component to perform LLM-driven actions.
Typically, these components prepare data for further processing by downstream components, rather than emitting direct chat output.
For an example, see the [**Smart Function** component](/components-processing#smart-transform).

A component must accept a `LanguageModel` input to use a language model component as a driver, and you must set the language model component's output type to `LanguageModel`.
For more information, see [Language Model output types](#language-model-output-types).

</TabItem>
<TabItem value="agents" label="Agents">

If you don't want to use the **Agent** component's built-in LLMs, you can use a language model component to connect your preferred model:

1. Add a language model component to your flow.

    You can use the **Language Model** core component or browse <Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) to find additional language models.
    Components in bundles may not have `language model` in the name.
    For example, Azure OpenAI LLMs are provided through the [**Azure OpenAI** component](/bundles-azure#azure-openai).

2. Configure the language model component as needed to connect to your preferred model.

3. Change the language model component's output type from **Model Response** to **Language Model**.
The output port changes to a `LanguageModel` port.
This is required to connect the language model component to the **Agent** component.
For more information, see [Language Model output types](#language-model-output-types).

4. Add an **Agent** component to the flow, and then set **Model Provider** to **Connect other models**.

    The **Model Provider** field changes to a **Language Model** (`LanguageModel`) input.

5. Connect the language model component's output to the **Agent** component's **Language Model** input.
The **Agent** component now inherits the language model settings from the connected language model component instead of using any of the built-in models.

</TabItem>
</Tabs>

## Language model parameters

The following parameters are for the **Language Model** core component.
Other language model components can have additional or different parameters.

import PartialParams from '@site/docs/_partial-hidden-params.mdx';

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| provider | String | Input parameter. The model provider to use. |
| model_name | String | Input parameter. The name of the model to use. Options depend on the selected provider. |
| api_key | SecretString | Input parameter. The API Key for authentication with the selected provider. |
| input_value | String | Input parameter. The input text to send to the model. |
| system_message | String | Input parameter. A system message that helps set the behavior of the assistant. |
| stream | Boolean | Input parameter. Whether to stream the response. Default: `false`. |
| temperature | Float | Input parameter. Controls randomness in responses. Range: `[0.0, 1.0]`. Default: `0.1`. |
| model | LanguageModel | Output parameter. Alternative output type to the default `Message` output. Produces an instance of Chat configured with the specified parameters. See [Language Model output types](#language-model-output-types). |

## Language model output types

Language model components, including the core component and bundled components, can produce two types of output:

* **Model Response**: The default output type emits the model's generated response as [`Message` data](/data-types#message).
Use this output type when you want the typical LLM interaction where the LLM produces a text response based on given input.

* **Language Model**: Change the language model component's output type to [`LanguageModel`](/data-types#languagemodel) when you need to attach an LLM to another component in your flow, such as an **Agent** or **Smart Function** component.

    With this configuration, the language model component supports an action completed by another component, rather than a direct chat interaction.
    For an example, the **Smart Function** component uses an LLM to create a function from natural language input.

## Additional language models

If your provider or model isn't supported by the **Language Model** core component, additional language model components are available in <Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components).

You can use these components in the same way that you use the core **Language Model** component, as explained in [Use language model components in flows](#use-language-model-components-in-flows).

## Pair models with vector stores

import PartialVectorRagBlurb from '@site/docs/_partial-vector-rag-blurb.mdx';

<PartialVectorRagBlurb />

<details>
<summary>Example: Vector search flow</summary>

import PartialVectorRagFlow from '@site/docs/_partial-vector-rag-flow.mdx';

<PartialVectorRagFlow />

</details>


================================================================================
filename: Components/components-processing.mdx
================================================================================

---
title: Processing components
slug: /components-processing
---

import Icon from "@site/src/components/icon";
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import PartialParams from '@site/docs/_partial-hidden-params.mdx';
import PartialCurlyBraces from '@site/docs/_partial-escape-curly-braces.mdx';

Processing components process and transform data within a flow.
They have many uses, including:

* Feed instructions and context to your LLMs and agents with the [**Prompt Template** component](#prompt-template).
* Extract content from larger chunks of data with a [**Parser** component](#parser).
* Filter data with natural language with the [**Smart Function** component](#smart-transform).
* Save data to your local machine with the [**Save File** component](#save-file).
* Transform data into a different data type with the [**Type Convert** component](#type-convert) to pass it between incompatible components.

## Prompt Template

See [**Prompt Template** component](/components-prompts).

## Batch Run

The **Batch Run** component runs a language model over _each row of one text column_ in a [`DataFrame`](/data-types#dataframe), and then returns a new `DataFrame` with the original text and an LLM response.
The output contains the following columns:

* `text_input`: The original text from the input `DataFrame`
* `model_response`: The model's response for each input
* `batch_index`: The 0-indexed processing order for all rows in the `DataFrame`
* `metadata` (optional): Additional information about the processing

### Use the Batch Run component in a flow

If you pass the **Batch Run** output to a [**Parser** component](/components-processing#parser), you can use variables in the parsing template to reference these keys, such as `{text_input}` and `{model_response}`.
This is demonstrated in the following example.

![A batch run component connected to OpenAI and a Parser](/img/component-batch-run.png)

1. Connect any language model component to a **Batch Run** component's **Language model** port.

2. Connect `DataFrame` output from another component to the **Batch Run** component's **DataFrame** input.
For example, you could connect a **File** component with a CSV file.

3. In the **Batch Run** component's **Column Name** field, enter the name of the column in the incoming `DataFrame` that contains the text to process.
For example, if you want to extract text from a `name` column in a CSV file, enter `name` in the **Column Name** field.

4. Connect the **Batch Run** component's **Batch Results** output to a **Parser** component's **DataFrame** input.

5. Optional: In the **Batch Run** [component's header menu](/concepts-components#component-menus), click <Icon name="SlidersHorizontal" aria-hidden="true"/> **Controls**, enable the **System Message** parameter, click **Close**, and then enter an instruction for how you want the LLM to process each cell extracted from the file.
For example, `Create a business card for each name.`

6. In the **Parser** component's **Template** field, enter a template for processing the **Batch Run** component's new `DataFrame` columns (`text_input`, `model_response`, and `batch_index`):

    For example, this template uses three columns from the resulting, post-batch `DataFrame`:

    ```text
    record_number: {batch_index}, name: {text_input}, summary: {model_response}
    ```

7. To test the processing, click the **Parser** component, click <Icon name="Play" aria-hidden="true" /> **Run component**, and then click <Icon name="TextSearch" aria-hidden="true" /> **Inspect output** to view the final `DataFrame`.

    You can also connect a **Chat Output** component to the **Parser** component if you want to see the output in the **Playground**.

### Batch Run parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| model | HandleInput | Input parameter. Connect the 'Language Model' output from a language model component. Required. |
| system_message | MultilineInput | Input parameter. A multi-line system instruction for all rows in the DataFrame. |
| df | DataFrameInput | Input parameter. The DataFrame whose column is treated as text messages, as specified by 'column_name'. Required. |
| column_name | MessageTextInput | Input parameter. The name of the DataFrame column to treat as text messages. If empty, all columns are formatted in TOML. |
| output_column_name | MessageTextInput | Input parameter. Name of the column where the model's response is stored. Default=`model_response`. |
| enable_metadata | BoolInput | Input parameter. If `True`, add metadata to the output DataFrame. |
| batch_results | DataFrame | Output parameter. A DataFrame with all original columns plus the model's response column. |

## Data Operations

The **Data Operations** component performs operations on [`Data`](/data-types#data) objects, including extracting, filtering, and editing keys and values in the `Data`.
For all options, see [Available data operations](#available-data-operations).
The output is a new `Data` object containing the modified data after running the selected operation.

### Use the Data Operations component in a flow

The following example demonstrates how to use a **Data Operations** component in a flow using data from a webhook payload:

1. Create a flow with a **Webhook** component and a **Data Operations** component, and then connect the **Webhook** component's output to the **Data Operations** component's **Data** input.

    All operations in the **Data Operations** component require at least one `Data` input from another component.
    If the preceding component doesn't produce `Data` output, you can use another component, such as the **Type Convert** component, to reformat the data before passing it to the **Data Operations** component.
    Alternatively, you could consider using a component that is designed to process the original data type, such as the **Parser** or **DataFrame Operations** components.

2. In the **Operations** field, select the operation you want to perform on the incoming `Data`.
For this example, select the **Select Keys** operation.

    :::tip
    You can select only one operation.
    If you need to perform multiple operations on the data, you can chain multiple **Data Operations** components together to execute each operation in sequence.
    For more complex multi-step operations, consider using a component like the **Smart Function** component.
    :::

3. Under **Select Keys**, add keys for `name`, `username`, and `email`.
Click <Icon name="Plus" aria-hidden="true" /> **Add more** to add a field for each key.

    For this example, assume that the webhook will receive consistent payloads that always contain `name`, `username`, and `email` keys.
    The **Select Keys** operation extracts the value of these keys from each incoming payload.

4. Optional: If you want to view the output in the **Playground**, connect the **Data Operations** component's output to a **Chat Output** component.

    ![A flow with Webhook, Data Operations, and Chat Output components](/img/component-data-operations-select-key.png)

5. To test the flow, send the following request to your flow's webhook endpoint.
For more information about the webhook endpoint, see [Trigger flows with webhooks](/webhook).

    ```bash
    curl -X POST "http://$LANGFLOW_SERVER_URL/api/v1/webhook/$FLOW_ID" \
    -H "Content-Type: application/json" \
    -H "x-api-key: $LANGFLOW_API_KEY" \
    -d '{
      "id": 1,
      "name": "Leanne Graham",
      "username": "Bret",
      "email": "Sincere@april.biz",
      "address": {
        "street": "Main Street",
        "suite": "Apt. 556",
        "city": "Springfield",
        "zipcode": "92998-3874",
        "geo": {
          "lat": "-37.3159",
          "lng": "81.1496"
        }
      },
      "phone": "1-770-736-8031 x56442",
      "website": "hildegard.org",
      "company": {
        "name": "Acme-Corp",
        "catchPhrase": "Multi-layered client-server neural-net",
        "bs": "harness real-time e-markets"
      }
    }'
    ```

6. To view the `Data` resulting from the **Select Keys** operation, do one of the following:

   * If you attached a **Chat Output** component, open the **Playground** to see the result as a chat message.
   * Click <Icon name="TextSearch" aria-hidden="true" /> **Inspect output** on the **Data Operations** component.

### Data Operations parameters

Many parameters are conditional based on the selected **Operation** (`operation`).

| Name | Display Name | Info |
|------|--------------|------|
| data | Data | Input parameter. The `Data` object to operate on. |
| operation | Operation | Input parameter. The operation to perform on the data. See [Available data operations](#available-data-operations) |
| select_keys_input | Select Keys | Input parameter. A list of keys to select from the data. |
| filter_key | Filter Key | Input parameter. The key to filter by. |
| operator | Comparison Operator | Input parameter. The operator to apply for comparing values. |
| filter_values | Filter Values | Input parameter. A list of values to filter by. |
| append_update_data | Append or Update | Input parameter. The data to append or update the existing data with. |
| remove_keys_input | Remove Keys | Input parameter. A list of keys to remove from the data. |
| rename_keys_input | Rename Keys | Input parameter. A list of keys to rename in the data. |

#### Available data operations

Options for the `operations` input parameter are as follows.
All operations act on an incoming `Data` object.

| Name | Required Inputs | Process |
|-----------|----------------|-------------|
| Select Keys | `select_keys_input` | Selects specific keys from the data. |
| Literal Eval | None | Evaluates string values as Python literals. |
| Combine | None | Combines multiple data objects into one. |
| Filter Values | `filter_key`, `filter_values`, `operator` | Filters data based on key-value pair. |
| Append or Update | `append_update_data` | Adds or updates key-value pairs. |
| Remove Keys | `remove_keys_input` | Removes specified keys from the data. |
| Rename Keys | `rename_keys_input` | Renames keys in the data. |

## DataFrame Operations

The **DataFrame Operations** component performs operations on [`DataFrame`](/data-types#dataframe) (table) rows and columns, including schema changes, record changes, sorting, and filtering.
For all options, see [DataFrame Operations parameters](#dataframe-operations-parameters).

The output is a new `DataFrame` containing the modified data after running the selected operation.

### Use the DataFrame Operations component in a flow

The following steps explain how to configure a **DataFrame Operations** component in a flow.
You can follow along with an example or use your own flow.
The only requirement is that the preceding component must create `DataFrame` output that you can pass to the **DataFrame Operations** component.

1. Create a new flow or use an existing flow.

    <details>
    <summary>Example: API response extraction flow</summary>

    The following example flow uses five components to extract `Data` from an API response, transform it to a `DataFrame`, and then perform further processing on the tabular data using a **DataFrame Operations** component.
    The sixth component, **Chat Output**, is optional in this example.
    It only serves as a convenient way for you to view the final output in the **Playground**, rather than inspecting the component logs.

    ![A flow that ingests an API response, extracts it to a DataFrame with a Smart Function component, and the processes it through a DataFrame Operations component](/img/component-dataframe-operations.png)

    If you want to use this example to test the **DataFrame Operations** component, do the following:

    1. Create a flow with the following components:

        * **API Request**
        * **Language Model**
        * **Smart Function**
        * **Type Convert**

    2. Configure the [**Smart Function** component](#smart-transform) and its dependencies:

        * **API Request**: Configure the [**API Request** component](/components-data#api-request) to get JSON data from an endpoint of your choice, and then connect the **API Response** output to the **Smart Function** component's **Data** input.
        * **Language Model**: Select your preferred provider and model, and then enter a valid API key.
        Change the output to **Language Model**, and then connect the `LanguageModel` output to the **Smart Function** component's **Language Model** input.
        * **Smart Function**: In the **Instructions** field, enter natural language instructions to extract data from the API response.
        Your instructions depend on the response content and desired outcome.
        For example, if the response contains a large `result` field, you might provide instructions like `explode the result field out into a Data object`.

    3. Convert the **Smart Function** component's `Data` output to `DataFrame`:

        1. Connect the **Filtered Data** output to the **Type Convert** component's **Data** input.
        2. Set the **Type Convert** component's **Output Type** to **DataFrame**.

    Now the flow is ready for you to add the **DataFrame Operations** component.

    </details>

2. Add a **DataFrame Operations** component to the flow, and then connect `DataFrame` output from another component to the **DataFrame** input.

    All operations in the **DataFrame Operations** component require at least one `DataFrame` input from another component.
    If a component doesn't produce `DataFrame` output, you can use another component, such as the **Type Convert** component, to reformat the data before passing it to the **DataFrame Operations** component.
    Alternatively, you could consider using a component that is designed to process the original data type, such as the **Parser** or **Data Operations** components.

    If you are following along with the example flow, connect the **Type Convert** component's **DataFrame Output** port to the **DataFrame** input.

3. In the **Operations** field, select the operation you want to perform on the incoming `DataFrame`.
For example, the **Filter** operation filters the rows based on a specified column and value.

    :::tip
    You can select only one operation.
    If you need to perform multiple operations on the data, you can chain multiple **DataFrame Operations** components together to execute each operation in sequence.
    For more complex multi-step operations, like dramatic schema changes or pivots, consider using an LLM-powered component, like the **Structured Output** or **Smart Function** component, as a replacement or preparation for the **DataFrame Operations** component.
    :::

    If you're following along with the example flow, select any operation that you want to apply to the data that was extracted by the **Smart Function** component.
    To view the contents of the incoming `DataFrame`, click <Icon name="Play" aria-hidden="true" /> **Run component** on the **Type Convert** component, and then <Icon name="TextSearch" aria-hidden="true" /> **Inspect output**.
    If the `DataFrame` seems malformed, click <Icon name="TextSearch" aria-hidden="true" /> **Inspect output** on each upstream component to determine where the error occurs, and then modify your flow's configuration as needed.
    For example, if the **Smart Function** component didn't extract the expected fields, modify your instructions or verify that the given fields are present in the **API Response** output.

4. Configure the operation's parameters.
The specific parameters depend on the selected operation.
For example, if you select the **Filter** operation, you must define a filter condition using the **Column Name**, **Filter Value**, and **Filter Operator** parameters.
For more information, see [DataFrame Operations parameters](#dataframe-operations-parameters)

5. To test the flow, click <Icon name="Play" aria-hidden="true" /> **Run component** on the **DataFrame Operations** component, and then click <Icon name="TextSearch" aria-hidden="true" /> **Inspect output** to view the new `DataFrame` created from the **Filter** operation.

   If you want to view the output in the **Playground**, connect the **DataFrame Operations** component's output to a **Chat Output** component, rerun the **DataFrame Operations** component, and then click **Playground**.

For another example, see [Conditional looping](/components-logic#conditional-looping).

### DataFrame Operations parameters

Most **DataFrame Operations** parameters are conditional because they only apply to specific operations.

The only permanent parameters are **DataFrame** (`df`), which is the `DataFrame` input, and **Operation** (`operation`), which is the operation to perform on the `DataFrame`.
Once you select an operation, the conditional parameters for that operation appear on the **DataFrame Operations** component.

<Tabs>
<TabItem value="addcolumn" label="Add Column" default>

The **Add Column** operation allows you to add a new column to the `DataFrame` with a constant value.

The parameters are **New Column Name** (`new_column_name`) and **New Column Value** (`new_column_value`).

</TabItem>
<TabItem value="dropcolumn" label="Drop Column">

The **Drop Column** operation allows you to remove a column from the `DataFrame`, specified by **Column Name** (`column_name`).

</TabItem>
<TabItem value="filter" label="Filter">

The **Filter** operation allows you to filter the `DataFrame` based on a specified condition.
The output is a `DataFrame` containing only the rows that matched the filter condition.

Provide the following parameters:

* **Column Name** (`column_name`): The name of the column to filter on.
* **Filter Value** (`filter_value`): The value to filter on.
* **Filter Operator** (`filter_operator`): The operator to use for filtering, one of `equals` (default), `not equals`, `contains`, `starts with`, `ends with`, `greater than`, or `less than`.

</TabItem>
<TabItem value="head" label="Head">

The **Head** operation allows you to retrieve the first `n` rows of the `DataFrame`, where `n` is set in **Number of Rows** (`num_rows`).
The default is `5`.

The output is a `DataFrame` containing only the selected rows.

</TabItem>
<TabItem value="renamecolumn" label="Rename Column">

The **Rename Column** operation allows you to rename an existing column in the `DataFrame`.

The parameters are **Column Name** (`column_name`), which is the current name, and **New Column Name** (`new_column_name`).

</TabItem>
<TabItem value="replacevalue" label="Replace Value">

The **Replace Value** operation allows you to replace values in a specific column of the `DataFrame`.
This operation replaces a target value with a new value.
All cells matching the target value are replaced with the new value in the new `DataFrame` output.

Provide the following parameters:

* **Column Name** (`column_name`): The name of the column to modify.
* **Value to Replace** (`replace_value`): The value that you want to replace.
* **Replacement Value** (`replacement_value`): The new value to use.

</TabItem>
<TabItem value="selectcolumns" label="Select Columns">

The **Select Columns** operation allows you to select one or more specific columns from the `DataFrame`.

Provide a list of column names in **Columns to Select** (`columns_to_select`).
In the visual editor, click <Icon name="Plus" aria-hidden="true"/> **Add More** to add multiple fields, and then enter one column name in each field.

The output is a `DataFrame` containing only the specified columns.

</TabItem>
<TabItem value="sort" label="Sort">

The **Sort** operation allows you to sort the `DataFrame` on a specific column in ascending or descending order.

Provide the following parameters:

* **Column Name** (`column_name`): The name of the column to sort on.
* **Sort Ascending** (`ascending`): Whether to sort in ascending or descending order. If enabled (`true`), sorts in ascending order; if disabled (`false`), sorts in descending order. Default: Enabled (`true`)

</TabItem>
<TabItem value="tail" label="Tail">

The **Tail** operation allows you to retrieve the last `n` rows of the `DataFrame`, where `n` is set in **Number of Rows** (`num_rows`).
The default is `5`.

The output is a `DataFrame` containing only the selected rows.

</TabItem>
<TabItem value="dropduplicates" label="Drop Duplicates">

The **Drop Duplicates** operation removes rows from the `DataFrame` by identifying all duplicate values within a single column.

The only parameter is the **Column Name** (`column_name`).

When the flow runs, all rows with duplicate values in the given column are removed.
The output is a `DataFrame` containing all columns from the original `DataFrame`, but only rows with non-duplicate values.

</TabItem>
</Tabs>

## LLM Router

The **LLM Router** component routes requests to the most appropriate LLM based on [OpenRouter](https://openrouter.ai/docs/quickstart) model specifications.

To use the component in a flow, you connect multiple language model components to the **LLM Router** components.
One model is the judge LLM that analyzes input messages to understand the evaluation context, selects the most appropriate model from the other attached LLMs, and then routes the input to the selected model.
The selected model processes the input, and then returns the generated response.

The following example flow has three language model components.
One is the judge LLM, and the other two are in the LLM pool for request routing.
The input and output components create a seamless chat interaction where you send a message and receive a response without any user awareness of the underlying routing.

![LLM Router component](/img/component-llm-router.png)

### LLM Router parameters

<PartialParams />

| Name | Display Name | Info |
|------|--------------|------|
| `models` | **Language Models** | Input parameter. Connect [`LanguageModel`](/data-types#languagemodel) output from multiple [language model components](/components-models) to create a pool of models. The `judge_llm` selects models from this pool when routing requests. The first model you connect is the default model if there is a problem with model selection or routing. |
| `input_value` | **Input** | Input parameter. The incoming query to be routed to the model selected by the judge LLM. |
| `judge_llm` | **Judge LLM** | Input parameter. Connect `LanguageModel` output from _one_ **Language Model** component to serve as the judge LLM for request routing. |
| `optimization` | **Optimization** | Input parameter. Set a preferred characteristic for model selection by the judge LLM. The options are `quality` (highest response quality), `speed` (fastest response time), `cost` (most cost-effective model), or `balanced` (equal weight for quality, speed, and cost). Default: `balanced` |
| `use_openrouter_specs` | **Use OpenRouter Specs** | Input parameter. Whether to fetch model specifications from the OpenRouter API.
If `false`, only the model name is provided to the judge LLM. Default: Enabled (`true`) |
| `timeout` | **API Timeout** | Input parameter. Set a timeout duration in seconds for API requests made by the router. Default: `10` |
| `fallback_to_first` | **Fallback to First Model** | Input parameter. Whether to use the first LLM in `models` as a backup if routing fails to reach the selected model. Default: Enabled (`true`) |

### LLM Router outputs

The **LLM Router** component provides three output options.
You can set the desired output type near the component's output port.

* **Output**: A `Message` containing the response to the original query as generated by the selected LLM.
Use this output for regular chat interactions.

* **Selected Model Info**: A `Data` object containing information about the selected model, such as its name and version.

* **Routing Decision**: A `Message` containing the judge model's reasoning for selecting a particular model, including input query length and number of models considered.
For example:

    ```text
    Model Selection Decision:
    - Selected Model Index: 0
    - Selected Langflow Model Name: gpt-4o-mini
    - Selected API Model ID (if resolved): openai/gpt-4o-mini
    - Optimization Preference: cost
    - Input Query Length: 27 characters (~5 tokens)
    - Number of Models Considered: 2
    - Specifications Source: OpenRouter API
    ```

    This is useful for debugging if you feel the judge model isn't selecting the best model.

## Parser {#parser}

The **Parser** component extracts text from structured data (`DataFrame` or `Data`) using a template or direct stringification.
The output is a `Message` containing the parsed text.

This is a versatile component for data extraction and manipulation in your flows.
For examples of **Parser** components in flows, see the following:

* [**Batch Run** component example](#batch-run)
* [**Structured Output** component example](#structured-output)
* **Financial Report Parser** template
* [Trigger flows with webhooks](/webhook)
* [Create a vector RAG chatbot](/chat-with-rag)

![A flow that uses a Parser component to extract text from a Structured Output component.](/img/component-parser.png)

### Parsing modes

The **Parser** component has two modes: **Parser** and **Stringify**.

<Tabs>
<TabItem value="template" label="Parser (template) mode" default>

In **Parser** mode, you create a template for text output that can include literal strings and variables for extracted keys.

Use curly braces to define variables anywhere in the template.
Variables must match keys in the `DataFrame` or `Data` input, such as column names.
For example, `{name}` extracts the value of a `name` key.
For more information about the content and structure of `DataFrame` and `Data` objects, see [Langflow data types](/data-types).

<PartialCurlyBraces />

When the flow runs, the **Parser** component iterates over the input, producing a `Message` for each parsed item.
For example, parsing a `DataFrame` creates a `Message` for each row, populated with the unique values from that row.

<details>
<summary>Employee summary template</summary>

This example template extracts employee data into a natural language summary about an employee's hire date and current role:

```text
{employee_first_name} {employee_last_name} was hired on {start_date}.
Their current position is {job_title} ({grade}).
```

The resulting `Message` output replaces the variables with the corresponding extracted values.
For example:

```text
Renlo Kai was hired on 11-July-2017.
Their current position is Software Engineer (Principal).
```

</details>

<details>
<summary>Employee profile template</summary>

This example template uses Markdown syntax and extracted employee data to create an employee profile:

```text
# Employee Profile
## Personal Information
- **Name:** {name}
- **ID:** {id}
- **Email:** {email}
```

When the flow runs, the **Parser** component iterates over each row of the `DataFrame`, populating the template's variables with the appropriate extracted values.
The resulting text for each row is output as a [`Message`](/data-types#message).

</details>

The following parameters are available in **Parser** mode.
<PartialParams />

| Name | Display Name | Info |
|------|--------------|------|
| input_data | Data or DataFrame | Input parameter. The `Data` or `DataFrame` input to parse. |
| pattern | Template | Input parameter. The formatting template using plaintext and variables for keys (`{KEY_NAME}`). See the preceding examples for more information. |
| sep | Separator | Input parameter. A string defining the separator for rows or lines. Default: `\n` (new line). |
| clean_data | Clean Data | Whether to remove empty rows and lines in each cell or key of the `DataFrame` or `Data` input. Default: Enabled (`true`) |

</TabItem>
<TabItem value="stringify" label="Stringify mode">

Use **Stringify** mode to convert the entire input directly to text.
This mode doesn't support templates or key selection.

The following parameters are available in **Stringify** mode.
<PartialParams />

| Name | Display Name | Info |
|------|--------------|------|
| input_data | Data or DataFrame | Input parameter. The `Data` or `DataFrame` input to parse. |
| sep | Separator | Input parameter. A string defining the separator for rows or lines. Default: `\n` (new line). |
| clean_data | Clean Data | Whether to remove empty rows and lines in each cell or key of the `DataFrame` or `Data` input. Default: Enabled (`true`) |

</TabItem>
</Tabs>

### Test and troubleshoot parsed text

To test the **Parser** component, click <Icon name="Play" aria-hidden="true"/> **Run component**, and then click <Icon name="TextSearch" aria-hidden="true"/> **Inspect output** to see the `Message` output with the parsed text.
You can also connect a **Chat Output** component if you want to view the output in the **Playground**.

If the `Message` output from the **Parser** component has empty or unexpected values, there might be a mapping error between the input and the parsing mode, the input has empty values, or the input isn't suitable for plaintext extraction.

For example, assume you use the following template to parse a `DataFrame`:

```text
{employee_first_name} {employee_last_name} is a {job_title} ({grade}).
```

The following `Message` could result from parsing a row where `employee_first_name` was empty and `grade` was `null`:

```text
 Smith is a Software Engineer (null).
```

To troubleshoot missing or unexpected values, you can do the following:

* Make sure the variables in your template map to keys in the incoming `Data` or `DataFrame`.
To see the data being passed directly to the **Parser** component, click <Icon name="TextSearch" aria-hidden="true"/> **Inspect output** on the component that is sending data to the **Parser** component.

* Check the source data for missing or incorrect values.
There are several ways you can address these inconsistencies:

    * Rectify the source data directly.
    * Use other components to amend or filter anomalies before passing the data to the **Parser** component.
    There are many components you can use for this depending on your goal, such as the **Data Operations**, **Structured Output**, and **Smart Function** components.
    * Enable the **Parser** component's **Clean Data** parameter to skip empty rows or lines.

## Python Interpreter

This component allows you to execute Python code with imported packages.

The **Python Interpreter** component can only import packages that are already installed in your Langflow environment.
If you encounter an `ImportError` when trying to use a package, you need to install it first.

To install custom packages, see [Install custom dependencies](/install-custom-dependencies).

### Use the Python Interpreter in a flow

1. To use this component in a flow, in the **Global Imports** field, add the packages you want to import as a comma-separated list, such as `math,pandas`.
At least one import is required.
2. In the **Python Code** field, enter the Python code you want to execute. Use `print()` to see the output.
3. Optional: Enable **Tool Mode**, and then connect the **Python Interpreter** component to an **Agent** component as a tool.
For example, connect a **Python Interpreter** component and a [**Calculator** component](/components-helpers#calculator) as tools for an **Agent** component, and then test how it chooses different tools to solve math problems.
![Python Interpreter and Calculator components connected to an Agent component](/img/component-python-interpreter.png)
4. Ask the agent an easier math question.
The **Calculator** tool can add, subtract, multiple, divide, or perform exponentiation.
The agent executes the `evaluate_expression` tool to correctly answer the question.

Result:
```text
Executed evaluate_expression
Input:
{
  "expression": "2+5"
}
Output:
{
  "result": "7"
}
```

5. Give the agent complete Python code.
This example creates a Pandas DataFrame table with the imported `pandas` packages, and returns the square root of the mean squares.

```python
import pandas as pd
import math

# Create a simple DataFrame
df = pd.DataFrame({
    'numbers': [1, 2, 3, 4, 5],
    'squares': [x**2 for x in range(1, 6)]
})

# Calculate the square root of the mean
result = math.sqrt(df['squares'].mean())
print(f"Square root of mean squares: {result}")
```

The agent correctly chooses the `run_python_repl` tool to solve the problem.

Result:
```text
Executed run_python_repl

Input:

{
  "python_code": "import pandas as pd\nimport math\n\n# Create a simple DataFrame\ndf = pd.DataFrame({\n    'numbers': [1, 2, 3, 4, 5],\n    'squares': [x**2 for x in range(1, 6)]\n})\n\n# Calculate the square root of the mean\nresult = math.sqrt(df['squares'].mean())\nprint(f\"Square root of mean squares: {result}\")"
}
Output:

{
  "result": "Square root of mean squares: 3.3166247903554"
}
```

If you don't include the package imports in the chat, the agent can still create the table using `pd.DataFrame`, because the `pandas` package is imported globally by the **Python Interpreter** component in the **Global Imports** field.

### Python Interpreter parameters

| Name | Type | Description |
|------|------|-------------|
| global_imports | String | Input parameter. A comma-separated list of modules to import globally, such as `math,pandas,numpy`. |
| python_code | Code | Input parameter. The Python code to execute. Only modules specified in Global Imports can be used. |
| results | Data | Output parameter. The output of the executed Python code, including any printed results or errors. |

## Save File

The **Save File** component creates a file containing data produced by another component.
Several file formats are supported, and you can store files in [Langflow storage](/memory) or the local file system.

To configure the **Save File** component and use it in a flow, do the following:

1. Connect [`DataFrame`](/data-types#dataframe), [`Data`](/data-types#data), or [`Message`](/data-types#message) output from another component to the **Save File** component's **Input** port.

    You can connect the same output to multiple **Save File** components if you want to create multiple files, save the data in different file formats, or save files to multiple locations.

2. In **File Name**, enter a file name and an optional path.

    The **File Name** parameter controls where the file is saved.
    It can contain a file name or an entire file path:

    * **Default location**: If you only provide a file name, then the file is stored in the Langflow data directory. For example,`~/Library/Caches/langflow/data` on macOS.

    * **Subdirectory**: To store files in subdirectories, add the path to the **File Name** parameter.
    If a given subdirectory doesn't already exist, Langflow automatically creates it.
    For example, `files/my_file` creates `my_file` in `/data/files`, and it creates the `files` subdirectory if it doesn't already exist.

    * **Absolute or relative path**: To store files elsewhere in your environment or local file storage, provide the absolute or relative path to the desired location.
    For example, `~/Desktop/my_file` saves `my_file` to the desktop.

    Don't include an extension in the file name.
    If you do, the extension is treated as part of the file name; it has no impact on the **File Format** parameter.

3. In the [component's header menu](/concepts-components#component-menus), click <Icon name="SlidersHorizontal" aria-hidden="true"/> **Controls**, select the desired file format, and then click **Close**.

    The available **File Format** options depend on the input data type:

    * `DataFrame` can be saved to CSV (default), Excel (requires `openpyxl` [custom dependency](/install-custom-dependencies)), JSON (fallback default), or Markdown.

    * `Data` can be saved to CSV, Excel (requires `openpyxl` [custom dependency](/install-custom-dependencies)), JSON (default), or Markdown.

    * `Message` can be saved to TXT, JSON (default), or Markdown.

    :::warning Overwrites allowed
    If you have multiple **Save File** components, in one or more flows, with the same file name, path, and extension, the file contains the data from the most recent run only.
    Langflow doesn't block overwrites if a matching file already exists.
    To avoid unintended overwrites, use unique file names and paths.
    :::

4. To test the **Save File** component, click <Icon name="Play" aria-hidden="true" /> **Run component**, and then click <Icon name="TextSearch" aria-hidden="true" /> **Inspect output** to get the filepath where the file was saved.

    The component's literal output is a `Message` containing the original data type, the file name and extension, and the absolute filepath to the file based on the **File Name** parameter.
    For example:

    ```text
    DataFrame saved successfully as 'my_file.csv' at /Users/user.name/Library/Caches/langflow/data/my_file.csv
    ```

    If the **File Name** contains a subdirectory or other non-default path, this is reflected in the `Message` output.
    For example, a CSV file with the file name `~/Desktop/my_file` could produce the following output:

    ```text
    DataFrame saved successfully as '/Users/user.name/Desktop/my_file.csv' at /Users/user.name/Desktop/my_file.csv
    ```


5. Optional: If you want to use the saved file in a flow, you must use an API call or another component to retrieve the file from the given filepath.

## Smart Function {#smart-transform}

In Langflow version 1.5, this component was renamed from **Lambda Filter** to **Smart Function**.

The **Smart Function** component uses an LLM to generate a Lambda function to filter or transform structured data based on natural language instructions.
You must connect this component to a [language model component](/components-models), which is used to generate a function based on the natural language instructions you provide in the **Instructions** parameter.
The LLM runs the function against the data input, and then outputs the results as [`Data`](/data-types#data).

:::tip
Provide brief, clear instructions, focusing on the desired outcome or specific actions, such as `Filter the data to only include items where the 'status' is 'active'`.
One sentence or less is preferred because end punctuation, like periods, can cause errors or unexpected behavior.

If you need to provide more details instructions that aren't directly relevant to the Lambda function, you can input them in the **Language Model** component's **Input** field or through a **Prompt Template** component.
:::

The following example uses the **API Request** endpoint to pass JSON data from the `https://jsonplaceholder.typicode.com/users` endpoint to the **Smart Function** component.
Then, the **Smart Function** component passes the data and the instruction `extract emails` to the attached **Language Model** component.
From there, the LLM generates a filter function that extracts email addresses from the JSON data, returning the filtered data as chat output.

![A small flow using a Smart Function component to extract data from an API response.](/img/component-lambda-filter.png)

### Smart Function parameters

<PartialParams />

| Name | Display Name | Info |
|------|--------------|------|
| data | Data | Input parameter. The structured data to filter or transform using a Lambda function. |
| llm | Language Model | Input parameter. Connect [`LanguageModel`](/data-types#languagemodel) output from a **Language Model** component. |
| filter_instruction | Instructions | Input parameter. The natural language instructions for how to filter or transform the data. The LLM uses these instructions to create a Lambda function. |
| sample_size | Sample Size | Input parameter. For large datasets, the number of characters to sample from the dataset head and tail. Only applied if the dataset meets or exceeds `max_size`. Default: `1000`. |
| max_size | Max Size | Input parameter. The number of characters for the dataset to be considered large, which triggers sampling by the `sample_size` value. Default: `30000`. |

## Split Text

The **Split Text** component splits data into chunks based on parameters like chunk size and separator.
It is often used to chunk data to be tokenized and embedded into vector databases.
For examples, see [Use embedding model components in a flow](/components-embedding-models#use-embedding-model-components-in-a-flow) and [Create a Vector RAG chatbot](/chat-with-rag).

![An embedding generation flow that uses a Split Text component to chunk data.](/img/component-split-text.png)

The component accepts `Message`, `Data`, or `DataFrame`, and then outputs either **Chunks** or **DataFrame**.
The **Chunks** output returns a list of [`Data`](/data-types#data) objects containing individual text chunks.
The **DataFrame** output returns the list of chunks as a structured [`DataFrame`](/data-types#dataframe) with additional `text` and `metadata` columns.

### Split Text parameters

The **Split Text** component's parameters control how the text is split into chunks, specifically the `chunk_size`, `chunk_overlap`, and `separator` parameters.

To test the chunking behavior, add a **Text Input** or **File** component with some sample data to chunk, click <Icon name="Play" aria-hidden="true" /> **Run component** on the **Split Text** component, and then click <Icon name="TextSearch" aria-hidden="true" /> **Inspect output** to view the list of chunks and their metadata. The **text** column contains the actual text chunks created from your chunking settings.
If the chunks aren't split as you expect, adjust the parameters, rerun the component, and then inspect the new output.

<PartialParams />

| Name | Display Name | Info |
|------|--------------|------|
| data_inputs | Input | Input parameter. The data to split. Input must be in `Message`, `Data`, or `DataFrame` format. |
| chunk_overlap | Chunk Overlap | Input parameter. The number of characters to overlap between chunks. This helps maintain context across chunks. When a separator is encountered, the overlap is applied at the point of the separator so that the subsequent chunk contains the last _n_ characters of the preceding chunk. Default: `200`. |
| chunk_size | Chunk Size | Input parameter. The target length for each chunk after splitting. The data is first split by separator, and then chunks smaller than the `chunk_size` are merged up to this limit. However, if the initial separator split produces any chunks larger than the `chunk_size`, those chunks are neither further subdivided nor combined with any smaller chunks; these chunks will be output as-is even though they exceed the `chunk_size`. Default: `1000`. See [Tokenization errors due to chunk size](#chunk-size) for important considerations. |
| separator | Separator | Input parameter. A string defining a character to split on, such as `\n` to split on new line characters, `\n\n` to split at paragraph breaks, or `},` to split at the end of JSON objects. You can directly provide the separator string, or pass a separator string from another component as `Message` input. |
| text_key | Text Key | Input parameter. The key to use for the text column that is extracted from the input and then split. Default: `text`. |
| keep_separator | Keep Separator | Input parameter. Select how to handle separators in output chunks. If `False`, separators are omitted from output chunks. Options include `False` (remove separators), `True` (keep separators in chunks without preference for placement), `Start` (place separators at the beginning of chunks), or `End` (place separators at the end of chunks). Default: `False`. |

### Tokenization errors due to chunk size {#chunk-size}

When using **Split Text** with embedding models (especially NVIDIA models like `nvidia/nv-embed-v1`), you may need to use smaller chunk sizes (`500` or less) even though the model supports larger token limits.
The **Split Text** component doesn't always enforce the exact chunk size you set, and individual chunks may exceed your specified limit.
If you encounter tokenization errors, modify your text splitting strategy by reducing the chunk size, changing the overlap length, or using a more common separator.
Then, test your configuration by running the flow and inspecting the component's output.

### Other text splitters

See [LangChain text splitter components](/bundles-langchain#text-splitters).

## Structured Output

The **Structured Output** component uses an LLM to transform any input into structured data (`Data` or `DataFrame`) using natural language formatting instructions and an output schema definition.
For example, you can extract specific details from documents, like email messages or scientific papers.

### Use the Structured Output component in a flow

To use the **Structured Output** component in a flow, do the following:

1. Provide an **Input Message**, which is the source material from which you want to extract structured data.
This can come from practically any component, but it is typically a **Chat Input**, **File**, or other component that provides some unstructured or semi-structured input.

    :::tip
    Not all source material has to become structured output.
    The power of the **Structured Output** component is that you can specify the information you want to extract, even if that data isn't explicitly labeled or an exact keyword match.
    Then, the LLM can use your instructions to analyze the source material, extract the relevant data, and format it according to your specifications.
    Any irrelevant source material isn't included in the structured output.
    :::

2. Define **Format Instructions** and an **Output Schema** to specify the data to extract from the source material and how to structure it in the final `Data` or `DataFrame` output.

    The instructions are a prompt that tell the LLM what data to extract, how to format it, how to handle exceptions, and any other instructions relevant to preparing the structured data.

    The schema is a table that defines the fields (keys) and data types to organize the data extracted by the LLM into a structured `Data` or `DataFrame` object.
    For more information, see [Output Schema options](#output-schema-options)

3. Attach a [language model component](/components-models) that is set to emit [`LanguageModel`](/data-types#languagemodel) output.

    The LLM uses the **Input Message** and **Format Instructions** from the **Structured Output** component to extract specific pieces of data from the input text.
    The output schema is applied to the model's response to produce the final `Data` or `DataFrame` structured object.

4. Optional: Typically, the structured output is passed to downstream components that use the extracted data for other processes, such as the **Parser** or **Data Operations** components.

![A basic flow with Structured Output, Language Model, Type Convert, and Chat Input and Output components.](/img/component-structured-output.png)

<details>
<summary>Structured Output example: Financial Report Parser template</summary>

The **Financial Report Parser** template provides an example of how the **Structured Output** component can be used to extract structured data from unstructured text.

The template's **Structured Output** component has the following configuration:

* The **Input Message** comes from a **Chat Input** component that is preloaded with quotes from sample financial reports

* The **Format Instructions** are as follows:

    ```text
    You are an AI that extracts structured JSON objects from unstructured text.
    Use a predefined schema with expected types (str, int, float, bool, dict).
    Extract ALL relevant instances that match the schema - if multiple patterns exist, capture them all.
    Fill missing or ambiguous values with defaults: null for missing values.
    Remove exact duplicates but keep variations that have different field values.
    Always return valid JSON in the expected format, never throw errors.
    If multiple objects can be extracted, return them all in the structured format.
    ```

* The **Output Schema** includes keys for `EBITDA`, `NET_INCOME`, and `GROSS_PROFIT`.

The structured `Data` object is passed to a **Parser** component that produces a text string by mapping the schema keys to variables in the parsing template:

```text
EBITDA: {EBITDA}  ,  Net Income: {NET_INCOME} , GROSS_PROFIT: {GROSS_PROFIT}
```

When printed to the **Playground**, the resulting `Message` replaces the variables with the actual values extracted by the **Structured Output** component. For example:

```text
EBITDA: 900 million , Net Income: 500 million , GROSS_PROFIT: 1.2 billion
```

</details>

### Structured Output parameters

<PartialParams />

| Name | Type | Description |
|------|------|-------------|
| Language Model (`llm`) | `LanguageModel` | Input parameter. The [`LanguageModel`](/data-types#languagemodel) output from a **Language Model** component that defines the LLM to use to analyze, extract, and prepare the structured output. |
| Input Message (`input_value`) | String | Input parameter. The input message containing source material for extraction. |
| Format Instructions (`system_prompt`) | String | Input parameter. The instructions to the language model for extracting and formatting the output. |
| Schema Name (`schema_name`) | String | Input parameter. An optional title for the **Output Schema**. |
| Output Schema (`output_schema`)| Table | Input parameter. A table describing the schema of the desired structured output, ultimately determining the content of the `Data` or `DataFrame` output. See [Output Schema options](#output-schema-options). |
| Structured Output (`structured_output`) | `Data` or `DataFrame` | Output parameter. The final structured output produced by the component. Near the component's output port, you can select the output data type as either **Structured Output Data** or **Structured Output DataFrame**. The specific content and structure of the output depends on the input parameters. |

#### Output Schema options {#output-schema-options}

After the LLM extracts the relevant data from the **Input Message** and **Format Instructions**, the data is organized according to the **Output Schema**.

The schema is a table that defines the fields (keys) and data types for the final `Data` or `DataFrame` output from the **Structured Output** component.

The default schema is a single `field` string.

To add a key to the schema, click <Icon name="Plus" aria-hidden="true"/> **Add a new row**, and then edit each column to define the schema:

* **Name**: The name of the output field. Typically a specific key for which you want to extract a value.

    You can reference these keys as variables in downstream components, such as a **Parser** component's template.
    For example, the schema key `NET_INCOME` could be referenced by the variable `{NET_INCOME}`.

* **Description**: An optional metadata description of the field's contents and purpose.

* **Type**: The data type of the value stored in the field.
Supported types are `str` (default), `int`, `float`, `bool`, and `dict`.

* **As List**: Enable this setting if you want the field to contain a list of values rather than a single value.

For simple schemas, you might only extract a few `string` or `int` fields.
For more complex schemas with lists and dictionaries, it might help to refer to the `Data` and `DataFrame` structures and attributes, as described in [Langflow data types](/data-types).
You can also emit a rough `Data` or `DataFrame`, and then use downstream components for further refinement, such as a **Data Operations** component.

## Type Convert

The **Type Convert** component converts data from one type to another.
It supports `Data`, `DataFrame`, and `Message` data types.

<Tabs>
<TabItem value="data" label="Data" default>

A `Data` object is a structured object that contains a primary `text` key and other key-value pairs:

```json
"data": {
  "text": "User Profile",
  "name": "Charlie Lastname",
  "age": 28,
  "email": "charlie.lastname@example.com"
},
```

The larger context associated with a component's `data` dictionary also identifies which key is the primary `text_key`, and it can provide an optional default value if the primary key isn't specified.
For example:

```json
{
  "text_key": "text",
  "data": {
    "text": "User Profile",
    "name": "Charlie Lastname",
    "age": 28,
    "email": "charlie.lastname@example.com"
  },
  "default_value": ""
}
```

</TabItem>
<TabItem value="dataframe" label="DataFrame">

A `DataFrame` is an array that represents a tabular data structure with rows and columns.

It consists of a list (array) of dictionary objects, where each dictionary represents a row.
Each key in the dictionaries corresponds to a column name.
For example, the following `DataFrame` contains two rows with columns for `name`, `age`, and `email`:

```json
[
  {
    "name": "Charlie Lastname",
    "age": 28,
    "email": "charlie.lastname@example.com"
  },
  {
    "name": "Bobby Othername",
    "age": 25,
    "email": "bobby.othername@example.com"
  }
]
```

</TabItem>
<TabItem value="message" label="Message">

A `Message` is primarily for passing a `text` string, such as`"Name: Charlie Lastname, Age: 28, Email: charlie.lastname@example.com"`.
However, the entire `Message` object can include metadata about the message, particularly when used as chat input or output.

</TabItem>
</Tabs>

For more information, see [Langflow data types](/data-types).

### Use the Type Convert component in a flow

The **Type Convert** component is typically used to transform data into a format required by a downstream component.
For example, if a component outputs a `Message`, but the following component requires `Data`, then you can use the **Type Convert** component to reformat the `Message` as `Data` before passing it to the downstream component.

The following example uses the **Type Convert** component to convert the `DataFrame` output from a **Web Search** component into `Message` data that is passed as text input for an LLM:

1. Create a flow based on the **Basic prompting** template.

2. Add a **Web Search** component to the flow, and then enter a search query, such as `environmental news`.

3. In the **Prompt Template** component, replace the contents of the **Template** field with the following text:

    ```text
    Answer the user's question using the {context}
    ```

    The curly braces define a [prompt variable](/components-prompts#define-variables-in-prompts) that becomes an input field on the **Prompt Template** component.
    In this example, you will use the **context** field to pass the search results into the template, as explained in the next steps.

3. Add a **Type Convert** component to the flow, and then set the **Output Type** to **Message**.

    Because the **Web Search** component's `DataFrame` output is incompatible with the **context** variable's `Message` input, you must use the **Type Convert** component to change the `DataFrame` to a `Message` in order to pass the search results to the **Prompt Template** component.

4. Connect the additional components to the rest of the flow:

    * Connect the **Web Search** component's output to the **Type Convert** component's input.
    * Connect the **Type Convert** component's output to the **Prompt Template** component's **context** input.

    ![Convert web search output to text input](/img/component-type-convert-and-web-search.png)

5. In the **Language Model** component, add your OpenAI API key.

    If you want to use a different provider or model, edit the **Model Provider**, **Model Name**, and **API Key** fields accordingly.

6. Click **Playground**, and then ask something relevant to your search query, such as `latest news` or `what's the latest research on the environment?`.

    <details>
    <summary>Result</summary>

    The LLM uses the search results context, your chat message, and it's built-in training data to respond to your question.
    For example:

    ```text
    Here are some of the latest news articles related to the environment:
    Ozone Pollution and Global Warming: A recent study highlights that ozone pollution is a significant global environmental concern, threatening human health and crop production while exacerbating global warming. Read more
    ...
    ```

    </details>

### Type Convert parameters

| Name | Display Name | Info |
|------|--------------|------|
| input_data | Input Data | Input parameter. The data to convert. Accepts `Data`, `DataFrame`, or `Message` input. |
| output_type | Output Type | Input parameter. The desired output type, as one of **Data**, **DataFrame** or **Message**. |
| output | Output | Output parameter. The converted data in the specified format. The output port changes depending on the selected **Output Type**. |

## Legacy Processing components

import PartialLegacy from '@site/docs/_partial-legacy.mdx';

<PartialLegacy />

The following Processing components are in legacy status:

<details>
<summary>Alter Metadata</summary>

Replace this legacy component with the [**Data Operations** component](#data-operations).

This component modifies metadata of input objects. It can add new metadata, update existing metadata, and remove specified metadata fields. The component works with both `Message` and `Data` objects, and can also create a new `Data` object from user-provided text.

It accepts the following parameters:

| Name | Display Name | Info |
|------|--------------|------|
| input_value | Input | Input parameter. Objects to which Metadata should be added. |
| text_in | User Text | Input parameter. Text input; the value is contained in the 'text' attribute of the `Data` object. Empty text entries are ignored. |
| metadata | Metadata | Input parameter. Metadata to add to each object. |
| remove_fields | Fields to Remove | Input parameter. Metadata fields to remove. |
| data | Data | Output parameter. List of Input objects, each with added metadata. |

</details>

<details>
<summary>Combine Data</summary>

Replace this legacy component with the [**Data Operations** component](#data-operations) or the [**Loop** component](/components-logic#loop).

This component combines multiple data sources into a single unified `Data` object.

The component iterates through a list of `Data` objects, merging them into a single `Data` object (`merged_data`).
If the input list is empty, it returns an empty data object.
If there's only one input data object, it returns that object unchanged.

The merging process uses the addition operator to combine data objects.

</details>

<details>
<summary>Combine Text</summary>

Replace this legacy component with the [**Data Operations** component](#data-operations).

This component concatenates two text inputs into a single text chunk using a specified delimiter, outputting a `Message` object with the combined text.

</details>

<details>
<summary>Create Data</summary>

Replace this legacy component with the [**Data Operations** component](#data-operations).

This component dynamically creates a `Data` object with a specified number of fields and a text key.

It accepts the following parameters:

| Name | Display Name | Info |
|------|--------------|------|
| number_of_fields | Number of Fields | Input parameter. The number of fields to be added to the record. |
| text_key | Text Key | Input parameter. Key that identifies the field to be used as the text content. |
| text_key_validator | Text Key Validator | Input parameter. If enabled, checks if the given `Text Key` is present in the given `Data`. |

</details>

<details>
<summary>Extract Key</summary>

Replace this legacy component with the [**Data Operations** component](#data-operations).

This component extracts a specific key from a `Data` object and returns the value associated with that key.

</details>

<details>
<summary>Data to DataFrame/Data to Message</summary>

Replace these legacy components with newer Processing components, such as the [**Data Operations** component](#data-operations) and [**Type Convert** component](#type-convert).

These components converted one or more `Data` objects into a `DataFrame` or `Message` object.

For the **Data to DataFrame** component, each `Data` object corresponds to one row in the resulting `DataFrame`.
Fields from the `.data` attribute become columns, and the `.text` field (if present) is placed in a `text` column.

</details>

<details>
<summary>Filter Data</summary>

Replace this legacy component with the [**Data Operations** component](#data-operations).

This component filters a `Data` object based on a list of keys (`filter_criteria`), returning a new `Data` object (`filtered_data`) that contains only the key-value pairs that match the filter criteria.

</details>

<details>
<summary>Filter Values</summary>

Replace this legacy component with the [**Data Operations** component](#data-operations).

The Filter values component filters a list of data items based on a specified key, filter value, and comparison operator.

It accepts the following parameters:

| Name | Display Name | Info |
|------|--------------|------|
| input_data | Input data | Input parameter. The list of data items to filter. |
| filter_key | Filter Key | Input parameter. The key to filter on. |
| filter_value | Filter Value | Input parameter. The value to filter by. |
| operator | Comparison Operator | Input parameter. The operator to apply for comparing the values. |
| filtered_data | Filtered data | Output parameter. The resulting list of filtered data items. |

</details>

<details>
<summary>JSON Cleaner</summary>

Replace this legacy component with the [**Parser** component](#parser).

This component cleans JSON strings to ensure they are fully compliant with the JSON specification.

It accepts the following parameters:

| Name | Display Name | Info |
|------|--------------|------|
| json_str | JSON String | Input parameter. The JSON string to be cleaned. This can be a raw, potentially malformed JSON string produced by language models or other sources that may not fully comply with JSON specifications. |
| remove_control_chars | Remove Control Characters | Input parameter. If set to `True`, this option removes control characters (ASCII characters 0-31 and 127) from the JSON string. This can help eliminate invisible characters that might cause parsing issues or make the JSON invalid. |
| normalize_unicode | Normalize Unicode | Input parameter. When enabled, this option normalizes Unicode characters in the JSON string to their canonical composition form (NFC). This ensures consistent representation of Unicode characters across different systems and prevents potential issues with character encoding. |
| validate_json | Validate JSON | Input parameter. If set to `True`, this option attempts to parse the JSON string to ensure it is well-formed before applying the final repair operation. It raises a ValueError if the JSON is invalid, allowing for early detection of major structural issues in the JSON. |
| output | Cleaned JSON String | Output parameter. The resulting cleaned, repaired, and validated JSON string that fully complies with the JSON specification. |

</details>

<details>
<summary>Message to Data</summary>

Replace this legacy component with the [**Type Convert** component](#type-convert).

This component converts `Message` objects to `Data` objects.

</details>

<details>
<summary>Parse DataFrame</summary>

Replace this legacy component with the [**DataFrame Operations** component](#dataframe-operations) or [**Parser** component](#parser).

This component converts `DataFrame` objects into plain text using templates.

It accepts the following parameters:

| Name | Display Name | Info |
|------|--------------|------|
| df | DataFrame | Input parameter. The DataFrame to convert to text rows. |
| template | Template | Input parameter. Template for formatting (use `{column_name}` placeholders). |
| sep | Separator | Input parameter. String to join rows in output. |
| text | Text | Output parameter. All rows combined into single text. |

</details>

<details>
<summary>Parse JSON</summary>

Replace this legacy component with the [**Parser** component](#parser).

This component converts and extracts JSON fields in `Message` and `Data` objects using JQ queries, then returns `filtered_data`, which is a list of `Data` objects.

</details>

<details>
<summary>Regex Extractor</summary>

Replace this legacy component with the [**Parser** component](#parser).

This component extracts patterns in text using regular expressions. It can be used to find and extract specific patterns or information in text.

</details>

<details>
<summary>Select Data</summary>

Replace this legacy component with the [**Data Operations** component](#data-operations).

This component selects a single `Data` object from a list.

It accepts the following parameters:

| Name | Display Name | Info |
|------|--------------|------|
| data_list | Data List | Input parameter. List of data to select from |
| data_index | Data Index | Input parameter. Index of the data to select |
| selected_data | Selected Data | Output parameter. The selected `Data` object. |

</details>

<details>
<summary>Update Data</summary>

Replace this legacy component with the [**Data Operations** component](#data-operations).

This component dynamically updates or appends data with specified fields.

It accepts the following parameters:

| Name | Display Name | Info |
|------|--------------|------|
| old_data | Data | Input parameter. The records to update. |
| number_of_fields | Number of Fields | Input parameter. The number of fields to add. The maximum is 15. |
| text_key | Text Key | Input parameter. The key for text content. |
| text_key_validator | Text Key Validator | Input parameter. Validates the text key presence. |
| data | Data | Output parameter. The updated Data objects. |

</details>


================================================================================
filename: Components/components-prompts.mdx
================================================================================

---
title: Prompt Template
slug: /components-prompts
---

import PartialCurlyBraces from '@site/docs/_partial-escape-curly-braces.mdx';

Use the **Prompt Template** core component to create a _prompt_ that supplies instructions and context to an LLM or agent, separate from other input like chat messages and file uploads.

Prompts are structured input that use natural language, fixed values, and dynamic variables to provide baseline context for the LLM.
For example:

* Define a consistent structure for user queries, making it easier for the LLM to understand and respond appropriately.
* Define a specific output format for the LLM, such as JSON or structured text.
* Define a role for the LLM, such as `You are a helpful assistant` or `You are an expert in microbiology`.
* Allow the LLM to reference chat memory.

The **Prompt Template** component can also output variable instructions to other components later in the flow.

## Prompt Template parameters

| Name     | Display Name   | Description                                                       |
|----------|----------------|-------------------------------------------------------------------|
| template | Template       | Input parameter. Create a prompt template with dynamic variables in curly braces, such as `{VARIABLE_NAME}`. <PartialCurlyBraces /> |
| prompt   | Prompt Message | Output parameter. The built prompt message returned by the `build_prompt` method. |

## Define variables in prompts

Variables in a **Prompt Template** component dynamically add fields to the **Prompt Template** component so that your flow can receive definitions for those values from other components, Langflow global variables, or fixed input.

For example, with the [**Message History** component](/components-helpers#message-history), you can use a `{memory}` variable to pass chat history to the prompt.
However, the **Agent** component includes built-in chat memory that is enabled by default.
For more information, see [Memory management options](/memory).

The following steps demonstrate how to add variables to a **Prompt Template** component:

1. Create a flow based on the **Basic prompting** template.

    This template already has a **Prompt Template** component, but the template only contains natural language instructions: `Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh.`

    This prompt defines a role for the LLM's chat interactions, but it doesn't include variables that help you create prompts that adapt dynamically to changing contexts, such as different users and environments.

2. Click the **Prompt Template** component, and then add some variables to the **Template** field.

    Variables are declared by wrapping the variable name in curly braces, like `{variable_name}`.
    For example, the following template creates `context` and `user_question` variables:

    ```text
    Given the context:

    {context}

    Answer the question:

    {user_question}
    ```

    <PartialCurlyBraces />

4. Click **Check & Save** to save the template.

    After adding the variables to the template, new fields are added to the **Prompt Template** component for each variable.

5. Provide input for the variable fields:

   * Connect the fields to other components to pass the output from those components to the variables.
   * Use Langflow global variables.
   * Enter fixed values directly into the fields.

You can add as many variables as you like in your template.
For example, you could add variables for `{references}` and `{instructions}`, and then feed that information in from other components, such as **Text Input**, **URL**, or **File** components.

## See also

* [**LangChain Prompt Hub** component](/bundles-langchain#prompt-hub)
* [Processing components](/components-processing)


================================================================================
filename: Components/components-tools.mdx
================================================================================

---
title: Tools
slug: /components-tools
---

In Langflow version 1.5, the **Tools** category was deprecated.
All components that were in this category were replaced by other components or moved to other component categories.

## MCP Connection component

This component was moved to the **Agents** category and renamed to the [**MCP Tools** component](/components-agents#mcp-connection)

## Legacy Tools components

import PartialLegacy from '@site/docs/_partial-legacy.mdx';

<PartialLegacy />

The following Tools components are in legacy status:

* **Calculator Tool**: Replaced by the [**Calculator** component](/components-helpers#calculator).
* **Python Code Structured**: Replaced by the [**Python Interpreter** component](/components-processing#python-interpreter).
* **Python REPL**: Replaced by the [**Python Interpreter** component](/components-processing#python-interpreter).
* **Search API**: Replaced by the [**SearchApi** bundle](/bundles-searchapi).
* **SearXNG Search**: No direct replacement. Use another provider's search component, create a custom component, or use a core component like the [**API Request** component](/components-data#api-request).
* **Serp Search API**: Replace by the **SerpApi** bundle.
* **Tavily Search API**: Replaced by the **Tavily** bundle.
* **Wikidata API**: Replaced by the [**Wikipedia** bundle](/bundles-wikipedia).
* **Wikipedia API**: Replaced by the [**Wikipedia** bundle](/bundles-wikipedia).
* **Yahoo! Finance**: Replaced by the **Yahoo! Search** bundle.

## See also

* [**API Request** component](/components-data#api-request)
* [**News Search** component](/components-data#news-search)
* [**Web Search** component](/components-data#web-search)
* [**Bing** bundle](/bundles-bing)
* [**DuckDuckGo** bundle](/bundles-duckduckgo)
* [**Exa** bundle](/bundles-exa)
* [**Google** bundle](/bundles-google)
* [**Serper** bundle](/bundles-serper)


================================================================================
filename: Components/concepts-components.mdx
================================================================================

---
title: Components overview
slug: /concepts-components
---

import Icon from "@site/src/components/icon";

Components are the building blocks of your flows.
Like classes in an application, each component is designed for a specific use case or integration.

:::tip
Langflow provides keyboard shortcuts for the workspace.

In the Langflow header, click your profile icon, select **Settings**, and then click **Shortcuts** to view the available shortcuts.
:::

## Add a component to a flow {#component-menus}

To add a component to a flow, drag the component from the <Icon name="Component" aria-hidden="true" /> **Core components** or <Icon name="Blocks" aria-hidden="true" /> **Bundles** menu into the [workspace](/concepts-overview#workspace).

Components are grouped by type or provider, and some components are hidden by default:

* <Icon name="Component" aria-hidden="true" /> **Core components**: Langflow's base components are grouped by purpose, such as **Inputs and Outputs** or **Data**.
These components either provide generic functionality, like loops and parsing, or they provide single components that support multiple third-party integrations.

* <Icon name="Blocks" aria-hidden="true" /> **Bundles**: Bundles contain one or more components that support specific third-party integrations, and they are grouped by service provider.

* **Legacy**: These components are hidden by default.
For more information, see [Legacy components](#legacy-components).

### Configure a component

After adding a component to a flow, configure the component's parameters and connect it to the other components in your flows.

Each component has inputs, outputs, parameters, and controls related to the component's purpose.
By default, components show only required and common options.
To access additional settings and controls, including meta settings, use the [component's header menu](#component-header-menus).

### Component header menus

To access a component's header menu, click the component in your workspace.

![Agent component](/img/agent-component.png)

A few options are available directly on the header menu.
For example:

- **Code**: Modify component settings by directly editing the component's Python code.
- **Controls**: Adjust all component parameters, including optional settings that are hidden by default.
- **Tool Mode**: Enable this option when combining a component with an **Agent** component.

For all other options, including **Delete** and **Duplicate** controls, click <Icon name="Ellipsis" aria-hidden="true" /> **Show More**.

### Rename a component

To modify a component's name or description, click the component in the workspace, and then click <Icon name="PencilLine" aria-hidden="true"/> **Edit**.
Component descriptions accept Markdown syntax.

### Run a component

To run a single component, click <Icon name="Play" aria-label="Play button" /> **Run component**.
A **Last Run** value indicates that the component ran successfully.

Running a single component is different from running an entire flow. In a single component run, the `build_vertex` function is called, which builds and runs only the single component with direct inputs provided through the visual editor (the `inputs_dict` parameter). The `VertexBuildResult` data is passed to the `build_and_run` method that calls the component's `build` method and runs it. Unlike running an entire flow, running a single component doesn't automatically execute its upstream dependencies.

### Inspect component output and logs

To view the output and logs for a single component, click <Icon name="TextSearch" aria-hidden="true" /> **Inspect**.

### Freeze a component

:::info
Freezing a component also freezes all components upstream of the selected component.
:::

Use the freeze option if you expect consistent output from a component _and all upstream components_, and you only need to run those components once.

Freezing a component prevents that component and all upstream components from re-running, and it preserves the last output state for those components.
Any future flow runs use the preserved output.

To freeze a component, click the component in the workspace to expose the component's header menu, click <Icon name="Ellipsis" aria-hidden="true" /> **Show More**, and then select **Freeze**.

## Component ports

Around the border of each component, there are circular port icons like <Icon name="Circle" size="16" aria-label="Indigo message port" style={{ color: '#4f46e5', fill: '#4f46e5' }}/>.
These indicate a component _connection point_ or _port_.

Ports either accept input or produce output of a specific data type.
You can infer the data type from the field the port is attached to or from the [port's color](#port-colors).
For example, the **System Message** field accepts [message data](/data-types#message), as illustrated by the blue port icon: <Icon name="Circle" size="16" aria-label="Indigo message port" style={{ color: '#4f46e5', fill: '#4f46e5' }} />.

![Prompt Template component with multiple inputs](/img/prompt-component.png)

When building flows, connect output ports to input ports of the same type (color) to transfer that type of data between two components.
For information about the programmatic representation of each data type, see [Langflow data types](/data-types).

:::tip
* In the workspace, hover over a port to see connection details for that port.
Click a port to <Icon name="Search" aria-hidden="true" /> **Search** for compatible components.

* If two components have incompatible data types, you can use a processing component like the [**Type Convert** component](/components-processing#type-convert) to convert the data between components.
:::

### Dynamic ports

Some components have ports that are dynamically added or removed.
For example, the **Prompt Template** component accepts [inputs wrapped in curly braces](/components-prompts#define-variables-in-prompts), and new ports are opened when a value wrapped in curly braces is detected in the **Template** field.

![Prompt Template component with multiple inputs](/img/prompt-component-with-multiple-inputs.png)

### Output type selection

All components produce output that is either sent to another component in the flow or returned as the final flow result.

Some components can produce multiple types of output:

* If the component emits all types at once, the component has multiple output ports in the visual editor. In component code, this is represented by `group_outputs=True`

* If the component emits only one type, you must select the output type by clicking the output label near the output port, and then selecting the desired output type. In component code, this is represented by `group_outputs=False` or omitting the `group_outputs` parameter.

For example, a language model component can output _either_ a **Model Response** or **Language Model**.
The **Model Response** output produces [`Message`](/data-types#message) data that can be passed to another component's `Message` port.
The **Language Model** output must be connected to a component with a **Language Model** input, such as the [**Structured Output** component](/components-processing#structured-output), that uses the attached LLM to power the receiving component's reasoning.

![Output type selection in the Language Model component](/img/select-output.png)

### Port colors

Component port colors indicate the data type ingested or emitted by the port.
For example, a **Message** port either accepts or emits `Message` data.

The following table lists the component data types and their corresponding port colors:

| Data type | Port color | Port icon example |
|-----------|--------------|----------|
| Data | Red | <Icon name="Circle" size="16" aria-label="Red data port" style={{ color: '#dc2626', fill: '#dc2626' }} /> |
| DataFrame | Pink | <Icon name="Circle" size="16" aria-label="Pink dataframe port" style={{ color: '#ec4899', fill:'#ec4899' }} /> |
| Embeddings | Emerald | <Icon name="Circle" size="16" aria-label="Emerald embeddings port" style={{ color: '#10b981', fill: '#10b981' }} /> |
| LanguageModel | Fuchsia | <Icon name="Circle" size="16" aria-label="Fuchsia language model port" style={{ color: '#c026d3', fill: '#c026d3' }} /> |
| Memory | Orange | <Icon name="Circle" size="16" aria-label="Orange memory port" style={{ color: '#f97316', fill: '#f97316' }} /> |
| Message | Indigo | <Icon name="Circle" size="16" aria-label="Indigo message port" style={{ color: '#4f46e5', fill: '#4f46e5' }} /> |
| Tool | Cyan | <Icon name="Circle" size="16" aria-label="Cyan tool port" style={{ color: '#06b6d4', fill: '#06b6d4' }} /> |
| Unknown or multiple types | Gray | <Icon name="Circle" size="16" aria-label="Gray unknown port" style={{ color: '#9CA3AF', fill: '#9CA3AF' }} /> |

## Component code

You can edit components in the [workspace](/concepts-overview#workspace) and in code. When editing a flow, select a component, and then click <Icon name="Code" aria-hidden="true"/> **Code** to see and edit the component's underlying Python code.

All components have underlying code that determines how you configure them and what actions they can perform.
In the context of creating and running flows, component code does the following:

* Determines what configuration options to show in the visual editor.
* Validates inputs based on the component's defined input types.
* Processes data using the configured parameters, methods, and functions.
* Passes results to the next component in the flow.

All components inherit from a base `Component` class that defines the component's interface and behavior.
For example, the [**Recursive Character Text Splitter** component](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/components/langchain_utilities/recursive_character.py) is a child of the [`LCTextSplitterComponent`](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/base/textsplitters/model.py) class.

Each component's code includes definitions for inputs and outputs, which are represented in the workspace as [component ports](#component-ports).
For example, the `RecursiveCharacterTextSplitter` has four inputs. Each input definition specifies the input type, such as `IntInput`, as well as the encoded name, display name, description, and other parameters for that specific input.
These values determine the component settings, such as display names and tooltips in the visual editor.

```python
    inputs = [
        IntInput(
            name="chunk_size",
            display_name="Chunk Size",
            info="The maximum length of each chunk.",
            value=1000,
        ),
        IntInput(
            name="chunk_overlap",
            display_name="Chunk Overlap",
            info="The amount of overlap between chunks.",
            value=200,
        ),
        DataInput(
            name="data_input",
            display_name="Input",
            info="The texts to split.",
            input_types=["Document", "Data"],
        ),
        MessageTextInput(
            name="separators",
            display_name="Separators",
            info='The characters to split on.\nIf left empty defaults to ["\\n\\n", "\\n", " ", ""].',
            is_list=True,
        ),
    ]
```

Additionally, components have methods or functions that handle their functionality.
For example, the `RecursiveCharacterTextSplitter` has two methods:

```python
    def get_data_input(self) -> Any:
        return self.data_input

    def build_text_splitter(self) -> TextSplitter:
        if not self.separators:
            separators: list[str] | None = None
        else:
            # check if the separators list has escaped characters
            # if there are escaped characters, unescape them
            separators = [unescape_string(x) for x in self.separators]

        return RecursiveCharacterTextSplitter(
            separators=separators,
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
        )
```

The `get_data_input` method retrieves the text to be split from the component's input, which makes the data available to the class.
The `build_text_splitter` method creates a `RecursiveCharacterTextSplitter` object by calling its parent class's `build` method. Then, the text is split with the created splitter and passed to the next component.

## Component versions

Component versions and states are stored in an internal Langflow database. When you add a component to a flow, you create a detached copy of the component based on the information in the Langflow database.
These copies are detached from the primary Langflow database, and they don't synchronize with any updates that can occur when you upgrade your Langflow version.

In other words, an individual instance of a component retains the version number and state from the moment you add it to a specific flow. For example, if a component is at version 1.0 when you add it to a flow, it remains at version 1.0 _in that flow_ unless you update it.

### Update component versions

When editing a flow in the workspace, Langflow notifies you if a component's workspace version is behind the database version so you can update the component's workspace version:

* **Update ready**: This notification means the component update contains no breaking changes.
* **Update available**: This notification means the component update might contain breaking changes.

    Breaking changes modify component inputs and outputs, causing the components to be disconnected and break the flow. After updating the component, you might need to edit the component settings or reconnect component ports.

There are two ways to update components:

* Click **Update** to update a single component. This is recommended for updates without breaking changes.
* Click **Review** to view all available updates and create a snapshot before updating. This is recommended for updates with breaking changes.

    To save a snapshot of your flow before updating the components, enable **Create backup flow before updating**. Backup flows are stored in the same project folder as the original flow with the suffix `(backup)`.

    To update specific components, select the components you want to update, and then click **Update Components**.

Components are updated to the latest available version, based on the version of Langflow you are running.

## Group components

Multiple components can be grouped into a single component for reuse. This is useful for organizing large flows by combining related components together, such as a RAG **Agent** component and it's associated tools or vector store components.

1. Hold <kbd>Shift</kbd>, and then click and drag to highlight all components you want to merge. Components must be completely within the selection area to be merged.

    Alternatively, to select components for merging one by one, hold <kbd>Ctrl</kbd> on Windows or <kbd>Cmd</kbd> on Mac, and then click each component to add them to the group.

2. Release the mouse and keyboard, and then click **Group** to merge the components into a single, group component.

Grouped components are configured and managed as a single component, including the component name, code, and settings.

To ungroup the components, click the component in the workspace to expose the component's header menu, click <Icon name="Ellipsis" aria-hidden="true" /> **Show More**, and then select **Ungroup**.

If you want to reuse this grouping in other flows, click the component in the workspace to expose the component's header menu, click <Icon name="Ellipsis" aria-hidden="true" /> **Show More**, and then select **Save** to save the component to the <Icon name="Component" aria-hidden="true" /> **Core components** menu as a custom component.

## Legacy components

import PartialLegacy from '@site/docs/_partial-legacy.mdx';

<PartialLegacy />


================================================================================
filename: Contributing/contributing-bundles.mdx
================================================================================

---
title: Contribute bundles
slug: /contributing-bundles
---

import Icon from "@site/src/components/icon";

<Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) are groups of components that are related to a specific service provider.
If you want to contribute your custom components back to the Langflow project, you must put them into a bundle.

Follow these steps to add components to <Icon name="Blocks" aria-hidden="true" /> **Bundles** in the Langflow visual editor.
This example adds a bundle named `DarthVader`.

## Add the bundle to the backend folder

1. Navigate to the backend directory in the Langflow repository and create a new folder for your bundle.
The path for your new component is `src > backend > base > langflow > components > darth_vader`.
You can view the [components folder](https://github.com/langflow-ai/langflow/tree/main/src/backend/base/langflow/components) in the Langflow repository.

2. Within the newly created `darth_vader` folder, add the following files:

* `darth_vader_component.py` â€” This file contains the backend logic for the new bundle. Create multiple `.py` files for multiple components.
* `__init__.py` â€” This file initializes the bundle components. You can use any existing `__init__.py` as an example to see how it should be structured.

For an example of adding multiple components in a bundle, see the [Notion](https://github.com/langflow-ai/langflow/tree/main/src/backend/base/langflow/components/Notion) bundle.


## Add the bundle to the frontend folder

1. Navigate to the frontend directory in the Langflow repository to add your bundle's icon.
The path for your new component icon is `src > frontend > src > icons > DarthVader`
You can view the [icons folder](https://github.com/langflow-ai/langflow/tree/main/src/frontend/src/icons) in the Langflow repository.
To add your icon, create **three** files inside the `icons/darth_vader` folder.

2. In the `icons/darth_vader` folder, add the raw SVG file of your icon, such as `darth_vader-icon.svg`.

    :::tip
    To convert the SVG file to JSX format, you can use an online tool like SVG to JSX.
    It's highly recommended to use the original, lighter version of the SVG.
    :::

3. In the `icons/darth_vader` folder, add the icon as a React component in JSX format, such as `DarthVaderIcon.jsx`.

4. Update the JSX file to include the correct component name and structure.
Ensure you include the `{...props}` spread operator in your JSX file.
For example, here is `DarthVaderIcon.jsx`:

    ```javascript
    const DarthVaderIcon = (props) => (
      <svg
        xmlns="http://www.w3.org/2000/svg"
        width={24}
        height={24}
        viewBox="0 0 32 32"
        fill="none"
        style={{ backgroundColor: "#9100ff", borderRadius: "6px" }}
        {...props}
      >
        <g transform="translate(7, 7)">
          <path
            fillRule="evenodd"
            clipRule="evenodd"
            d="M6.27406 0.685082C8.46664 -0.228361 10.9302 -0.228361 13.1229 0.685082C14.6773 1.33267 16.0054 2.40178 16.9702 3.75502C17.6126 4.65574 17.0835 5.84489 16.045 6.21613L13.5108 7.12189C12.9962 7.30585 12.4289 7.26812 11.9429 7.01756C11.8253 6.95701 11.7298 6.86089 11.6696 6.74266L10.2591 3.97469C10.0249 3.51519 9.37195 3.51519 9.13783 3.97469L7.72731 6.74274C7.66714 6.86089 7.57155 6.95701 7.454 7.01756L4.70187 8.43618C4.24501 8.67169 4.24501 9.3284 4.70187 9.56391L7.454 10.9825C7.57155 11.0431 7.66714 11.1392 7.72731 11.2574L9.13783 14.0254C9.37195 14.4849 10.0249 14.4849 10.2591 14.0254L11.6696 11.2574C11.7298 11.1392 11.8253 11.0431 11.9428 10.9825C12.429 10.7319 12.9965 10.6942 13.5112 10.8781L16.045 11.7838C17.0835 12.1551 17.6126 13.3442 16.9704 14.245C16.0054 15.5982"
            fill={props.isdark === "true" ? "white" : "black"}
          />
        </g>
      </svg>
    );

    export default DarthVaderIcon;
    ```

5. In the `icons/darth_vader` folder, add the React component itself in TypeScript format, such as `index.tsx`.
Ensure the icon's React component name corresponds to the JSX component you just created, such as `DarthVaderIcon`:

    ```typescript
    import { useDarkStore } from "@/stores/darkStore";
    import React, { forwardRef } from "react";
    import DarthVaderIconSVG from "./DarthVaderIcon";

    export const DarthVaderIcon = forwardRef<
      SVGSVGElement,
      React.PropsWithChildren<{}>
    >((props, ref) => {
      const isdark = useDarkStore((state) => state.dark).toString();

      return <DarthVaderIconSVG ref={ref} isdark={isdark} {...props} />;
    });

    export default DarthVaderIcon;
    ```

6. To link your new bundle to the frontend, open `/src/frontend/src/icons/lazyIconImports.ts`.
You can view the [lazyIconImports.ts](https://github.com/langflow-ai/langflow/blob/main/src/frontend/src/icons/lazyIconImports.ts) in the Langflow repository.

7. Add the name of your icon, which should match the icon name you used in the `.tsx` file.
For example:

    ```typescript
      CrewAI: () =>
        import("@/icons/CrewAI").then((mod) => ({ default: mod.CrewAiIcon })),
      DarthVader: () =>
        import("@/icons/DarthVader").then((mod) => ({ default: mod.DarthVaderIcon })),
      DeepSeek: () =>
        import("@/icons/DeepSeek").then((mod) => ({ default: mod.DeepSeekIcon })),
    ```

8. To add your bundle to the <Icon name="Blocks" aria-hidden="true" /> **Bundles** menu, edit the [`SIDEBAR_BUNDLES` array](https://github.com/langflow-ai/langflow/blob/main/src/frontend/src/utils/styleUtils.ts#L231) in `/src/frontend/src/utils/styleUtils.ts`.

    Add an object to the array with the following keys:

    * `display_name`: The text label shown in the Langflow visual editor
    * `name`: The name of the folder you created within the `/src/backend/base/langflow/components` directory
    * `icon`: The name of the bundle's icon that you defined in the previous steps

    For example:

    ```typescript
    { display_name: "AssemblyAI", name: "assemblyai", icon: "AssemblyAI" },
    { display_name: "DarthVader", name: "darth_vader", icon: "DarthVader" },
    { display_name: "DataStax", name: "astra_assistants", icon: "DarthVader" },
    ```

## Update bundle components with icons

In your component bundle, associate the icon variable with your new bundle.

In your `darth_vader_component.py` file, in the component class, include the icon that you defined in the frontend.
The `icon` must point to the directory you created for your icons within the `src > frontend > src > icons` directory.
For example:
```
class DarthVaderAPIComponent(LCToolComponent):
    display_name: str = "Darth Vader Tools"
    description: str = "Use the force to run actions with your agent"
    name = "DarthVaderAPI"
    icon = "DarthVader"
```

## Ensure the application builds your component bundle

1. To rebuild the backend and frontend, run `make install_frontend && make build_frontend && make install_backend && uv run langflow run --port 7860`.

2. Refresh the frontend application.
Your new bundle called `DarthVader` is available in the <Icon name="Blocks" aria-hidden="true" /> **Bundles** menu in the visual editor.


================================================================================
filename: Contributing/contributing-community.mdx
================================================================================

---
title: Join the Langflow community
slug: /contributing-community
---

There are several ways you can interact with the Langflow community and learn more about the Langflow codebase.

## Join the Langflow Discord server

Join the [Langflow Discord Server](https://discord.gg/EqksyE2EX9) to ask questions and showcase your projects.

## Follow Langflow on X

Follow [@langflow_ai](https://twitter.com/langflow_ai) on X to get the latest news about Langflow.

## Star Langflow on GitHub

If you like Langflow, you can star the [Langflow GitHub repository](https://github.com/langflow-ai/langflow).
Stars help other users find Langflow more easily, and quickly understand that other users have found it useful.

Because Langflow is open-source, the more visible the repository is, the more likely the codebase is to attract [contributors](/contributing-how-to-contribute).

## Watch the GitHub repository

You can watch the [Langflow GitHub repository](https://github.com/langflow-ai/langflow) to get notified about new releases and other repository activity.

To get release notifications only, select **Releases only**.

If you select **Watching**, you will receive notifications about new releases as well as issues, discussions, and pull requests, if you are interested in that activity.
For information about customizing repository notifications, see the [GitHub documentation on repository subscriptions](https://docs.github.com/en/account-and-profile/managing-subscriptions-and-notifications-on-github/managing-subscriptions-for-activity-on-github/viewing-your-subscriptions).

## Request enhancements and get help through GitHub

You can also submit feature requests and get help with Langflow through the GitHub repository.
For more information, see [Get help and request enhancements](/contributing-github-issues).


================================================================================
filename: Contributing/contributing-component-tests.mdx
================================================================================

---
title: Contribute component tests
slug: /contributing-component-tests
---

This guide outlines how to structure and implement tests for application components to ensure consistency and adequate coverage.

## File naming

* The test file should follow the same directory structure as the component being tested, but should be placed in the corresponding unit tests folder.

    For example, if the file path for the component is `src/backend/base/langflow/components/prompts/`, then the test file should be located at `src/backend/tests/unit/components/prompts`.

* The test file name should use snake case and follow the pattern `test_<file_name>.py`.

    For example, if the file to be tested is `PromptComponent.py`, then the test file should be named `test_prompt_component.py`.

## File structure

* Each test file should group tests into classes by component. There should be no standalone test functions in the fileâ€” only test methods within classes.
* Class names should follow the pattern `Test<ClassName>`.
For example, if the component being tested is `PromptComponent`, then the test class should be named `TestPromptComponent`.

## Imports, inheritance, and mandatory methods

To standardize component tests, base test classes have been created and should be imported and inherited by all component test classes. These base classes are located in the file `src/backend/tests/unit/base.py`.

To import the base test classes:

```python
from tests.base import ComponentTestBaseWithClient
from tests.base import ComponentTestBaseWithoutClient
```

These base classes enforce mandatory methods that the component test classes must implement. The base classes ensure that components built in previous versions continues to work in the current version. By inheriting from one of these base classes, the developer must define the following methods decorated with `@pytest.fixture`:

* `component_class:` Returns the class of the component to be tested. For example:

    ```python
    @pytest.fixture
    def component_class(self):
        return PromptComponent
    ```

* `default_kwargs:` Returns a dictionary with the default arguments required to instantiate the component. For example:

    ```python
    @pytest.fixture
    def default_kwargs(self):
        return {"template": "Hello {name}!", "name": "John", "_session_id": "123"}
    ```

* `file_names_mapping:` Returns a list of dictionaries representing the relationship between `version`, `module`, and `file_name` that the tested component has had over time. This can be left empty if it is an unreleased component. For example:

    ```python
    @pytest.fixture
    def file_names_mapping(self):
        return [
            {"version": "1.0.15", "module": "prompts", "file_name": "Prompt"},
            {"version": "1.0.16", "module": "prompts", "file_name": "Prompt"},
            {"version": "1.0.17", "module": "prompts", "file_name": "Prompt"},
            {"version": "1.0.18", "module": "prompts", "file_name": "Prompt"},
            {"version": "1.0.19", "module": "prompts", "file_name": "Prompt"},
        ]
    ```

## Testing component functionalities

Once the basic structure of the test file is defined, implement test methods for the component's functionalities. The following guidelines must be followed:

1. Test method names should be descriptive, use snake case, and follow the pattern `test_<case_name>`.
2. Each test should follow the **Arrange, Act, Assert** pattern:
    1. **Arrange**: Prepare the data.
    2. **Act**: Execute the component.
    3. **Assert**: Verify the result.

### Example

1. **Arrange**: Prepare the data.

    It is recommended, but not mandatory, that you use the fixtures defined in the basic structure.

    ```python
    def test_post_code_processing(self, component_class, default_kwargs):
        component = component_class(**default_kwargs)
    ```

2. **Act**: Execute the component.

    Call the `.to_frontend_node()` method of the component prepared during the **Arrange** step.

    ```python
    def test_post_code_processing(self, component_class, default_kwargs):
        component = component_class(**default_kwargs)

        frontend_node = component.to_frontend_node()
    ```

3. **Assert**: Verify the result.

    After executing the `.to_frontend_node()` method, the resulting data is available for verification in the dictionary `frontend_node["data"]["node"]`. Assertions should be clear and cover the expected outcomes.

    ```python
    def test_post_code_processing(self, component_class, default_kwargs):
        component = component_class(**default_kwargs)

        frontend_node = component.to_frontend_node()

        node_data = frontend_node["data"]["node"]
        assert node_data["template"]["template"]["value"] == "Hello {name}!"
        assert "name" in node_data["custom_fields"]["template"]
        assert "name" in node_data["template"]
        assert node_data["template"]["name"]["value"] == "John"
    ```


================================================================================
filename: Contributing/contributing-components.mdx
================================================================================

---
title: Contribute components
slug: /contributing-components
---


New components are added as objects of the [`Component`](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/custom/custom_component/component.py) class.

Dependencies are added to the [pyproject.toml](https://github.com/langflow-ai/langflow/blob/main/pyproject.toml#L148) file.

## Contribute an example component to Langflow

Anyone can contribute an example component. For example, to create a new data component called **DataFrame processor**, follow these steps to contribute it to Langflow.

1. Create a Python file called `dataframe_processor.py`.
2. Write your processor as an object of the [`Component`](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/custom/custom_component/component.py) class. You'll create a new class, `DataFrameProcessor`, that will inherit from `Component` and override the base class's methods.

```python
from typing import Any, Dict, Optional
import pandas as pd
from langflow.custom import Component

class DataFrameProcessor(Component):
    """A component that processes pandas DataFrames with various operations."""
```

3. Define class attributes to provide information about your custom component:
```python
from typing import Any, Dict, Optional
import pandas as pd
from langflow.custom import Component

class DataFrameProcessor(Component):
    """A component that processes pandas DataFrames with various operations."""

    display_name: str = "DataFrame Processor"
    description: str = "Process and transform pandas DataFrames with various operations like filtering, sorting, and aggregation."
    documentation: str = "https://docs.langflow.org/components-dataframe-processor"
    icon: str = "DataframeIcon"
    priority: int = 100
    name: str = "dataframe_processor"
```

   * `display_name`: A user-friendly name shown in the visual editor.
   * `description`: A brief description of what your component does.
   * `documentation`: A link to detailed documentation.
   * `icon`: An emoji or icon identifier for visual representation.
    For more information, see [Contributing bundles](/contributing-bundles#add-the-bundle-to-the-frontend-folder).
   * `priority`: An optional integer to control display order. Lower numbers appear first.
   * `name`: An optional internal identifier that defaults to class name.

4. Define the component's interface by specifying its inputs, outputs, and the method that will process them. The method name must match the `method` field in your outputs list, as this is how Langflow knows which method to call to generate each output.
This example creates a minimal custom component skeleton.
For more information on creating your custom component, see [Create custom Python components](/components-custom-components).
```python
from typing import Any, Dict, Optional
import pandas as pd
from langflow.custom import Component

class DataFrameProcessor(Component):
    """A component that processes pandas DataFrames with various operations."""

    display_name: str = "DataFrame Processor"
    description: str = "Process and transform pandas DataFrames with various operations like filtering, sorting, and aggregation."
    documentation: str = "https://docs.langflow.org/components-dataframe-processor"
    icon: str = "DataframeIcon"
    priority: int = 100
    name: str = "dataframe_processor"

    # input and output lists
    inputs = []
    outputs = []

    # method
    def some_output_method(self):
        return ...
```

5. Save the `dataframe_processor.py` to the `src > backend > base > langflow > components` directory.
This example adds a data component, so add it to the `/data` directory.

6. Add the component dependency to `src > backend > base > langflow > components > data > __init__.py` as `from .DataFrameProcessor import DataFrameProcessor`.
You can view the [/data/__init__.py](https://github.com/langflow-ai/langflow/blob/dev/src/backend/base/langflow/components/data/__init__.py) in the Langflow repository.

7. Add any new dependencies to the [pyproject.toml](https://github.com/langflow-ai/langflow/blob/main/pyproject.toml#L20) file.

8. Submit documentation for your component. For this example component, you would submit documentation to the [Data components page](https://github.com/langflow-ai/langflow/blob/main/docs/docs/Components/components-data.mdx).

9. Submit your changes as a pull request. The Langflow team will review, suggest changes, and add your component to Langflow.

## Best practices for modifying components

When creating or updating components, follow these best practices to maintain backward compatibility and ensure a smooth experience for users.

### Don't rename the class or `name` attribute

Changing the class name or the `name` attribute breaks the component for all existing users. This happens because the frontend tests the `type` attribute, which is set to the class' name or the `name` attribute. If these names change, the component effectively becomes a new component, and the old component disappears.

Instead, do the following:
* Change only the display name if the old name is unclear.
* Change only the display name if functionality changes but remains related.
* If a new internal name is necessary, mark the old component as `legacy=true` and create a new component.

For example:
```python
class MyCustomComponent(BaseComponent):
    name = "my_custom_component_internal"
    legacy = True
```

### Don't remove fields and outputs

Removing fields or outputs can cause edges to disconnect and change the behavior of components.

Instead, mark fields as `deprecated` and keep them in the same location. If removal is absolutely necessary, you must define and document a migration plan. Always clearly communicate any changes in the field's information to users.

### Maintain outdated components as legacy

When updating components, create them as completely separate entities while maintaining the old component as a legacy version. Always ensure backward compatibility and never remove methods and attributes from base classes, such as `LCModelComponent`.

### Favor asynchronous methods

Always favor asynchronous methods and functions in your components. When interacting with files, use `aiofile` and `anyio.Path` for better performance and compatibility.

### Include tests with your component

Include tests for your changes using `ComponentTestBase` classes. For more information, see [Contribute component tests](/contributing-component-tests).

### Documentation

When documenting changes in pull requests, clearly explain *what* changed, such as display name updates or new fields, *why* it changed, such as improvements or bug fixes, and the *impact* on existing users.

For example:

<details>
<summary>Example PR</summary>

```markdown
# Pull request with changes to Notify component

This pull request updates the Notify component.

## What changed
- Added new `timeout` field to control how long the component waits for a response.
- Renamed `message` field to `notification_text` for clarity.
- Added support for async operations.
- Deprecated the `retry_count` field in favor of `max_retries`.

## Why it changed
- `timeout` field addresses user requests for better control over wait times.
- `message` to `notification_text` change makes the field's purpose clearer.
- Async support improves performance in complex flows.
- `retry_count` to `max_retries` aligns with common retry pattern terminology.

## Impact on users
- New `timeout` field is optional (defaults to 30 seconds).
- Users will see a deprecation warning for `retry_count`.
  - Migration: Replace `retry_count` with `max_retries` in existing flows.
  - Both fields will work until version 2.0.
- No action needed for async support - it's backward compatible.
```

</details>

## Example pull request flow

1. Create or update a component.
Maintain the class name and `name` attribute if the purpose remains the same.
Otherwise, create a new component and move the old component to `legacy`.
2. Add tests.
Create tests using one of the `ComponentTestBase` classes.
For more information, see [Contribute component tests](/contributing-component-tests).
3. Flag outdated fields and outputs as `deprecated` and keep them in the same location to ensure backward compatibility.
4. Document your changes.
Include migration instructions if breaking changes occur.


================================================================================
filename: Contributing/contributing-how-to-contribute.mdx
================================================================================

---
title: Contribute to Langflow
slug: /contributing-how-to-contribute
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

This guide is intended to help you start contributing to Langflow.
As an open-source codebase in a rapidly developing field, Langflow welcomes contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.

To contribute code or documentation to Langflow, follow the [pull request guide](#open-a-pull-request).

## Langflow services overview

This overview will help you understand how to set up your development environment.

Langflow consists of two main services:
- Frontend: A React/TypeScript application that provides the user interface.
- Backend: A Python/FastAPI service that handles API requests.

During development, the frontend and backend services run separately on different ports:
- Frontend development server: `http://localhost:3000` (with hot reload).
- Backend API server: `http://localhost:7860`.

When you install Langflow as an end user, the backend serves the frontend internally, making both services available on a single port (`7860` by default).

## Install Langflow from source

Install Langflow from source by forking the repository and setting up your development environment.

### Prerequisites

* [uv](https://docs.astral.sh/uv/getting-started/installation/) version 0.4 or later
* [Node.js](https://nodejs.org/en/download/package-manager)
* [Make](https://www.gnu.org/software/make/#documentation) (Linux and macOS only)

### Clone the Langflow repository

1. Fork the [Langflow GitHub repository](https://github.com/langflow-ai/langflow).

2. Add the new remote to your local repository on your local machine:
```bash
git remote add FORK_NAME https://github.com/GIT_USERNAME/langflow.git
```
Replace the following:
* `FORK_NAME`: A name for your fork of the repository
* `GIT_USERNAME`: Your Git username

### Run Langflow from source

You can run Langflow from source after cloning the repository, even if you aren't contributing to the codebase.
This builds the frontend and serves it through the backend on port `7860`.

The instructions below are for running Langflow from source. For development with hot reload, see [Set up development environment](#set-up-your-langflow-development-environment).

<details>
<summary>Run from source on macOS/Linux</summary>

In your terminal, navigate to the root of the Langflow directory, and then run `make run_cli`.

This command does the following:

- Installs frontend and backend dependencies
- Builds the frontend static files
- Starts the application with default settings

The Langflow frontend is served at `http://localhost:7860`.
</details>

<details>
<summary>Run from source with Windows CMD</summary>

To run Langflow from source on Windows, you can use the Langflow repository's included scripts, or run the commands in the terminal.

Do one of the following:

* To install and run Langflow with the included Windows Batch file, navigate to the `scripts/windows` directory, and then run the `build_and_run.bat` file.

* To run Langflow from the Windows Command Line:

    1. Build the frontend static files:

        ```
        cd src/frontend
        npm install
        npm run build
        ```

    2. Copy the contents of the built `src/frontend/build` directory to `src/backend/base/langflow/frontend`.

    3. Start Langflow:

        ```
        uv run langflow run
        ```

The Langflow frontend is served at `http://localhost:7860`.

</details>

<details>
<summary>Run from source with PowerShell</summary>

To run Langflow from source on Windows, you can use the Langflow repository's included scripts, or run the commands in the terminal.

Do one of the following:

* To install and run Langflow with the included scripts, navigate to the `scripts/windows` directory, and then run the `build_and_run.ps1` file.

* To run Langflow from a PowerShell terminal:

    1. Build the frontend static files:

        ```
        cd src/frontend
        npm install
        npm run build
        ```

    2. Copy the contents of the built `src/frontend/build` directory to `src/backend/base/langflow/frontend`.

    3. Start Langflow:

        ```
        uv run langflow run
        ```

The Langflow frontend is served at `http://localhost:7860`.

</details>

### Set up your Langflow development environment

:::tip
Run `make help` to display all available make commands.
:::

This section is for contributors who want to develop and test code changes with hot reload enabled.

If you just want to run Langflow locally without making code changes, see [Run Langflow from source](#run-langflow-from-source).

<Tabs>
<TabItem value="linux-macos" label="Linux or macOS" default>

1. Set up the Langflow development environment:

    ```bash
    make init
    ```

    This command sets up the development environment by doing the following:

    - Checking for uv and npm.
    - Installing backend and frontend dependencies.
    - Installing pre-commit hooks.

2. Run the backend and frontend in separate terminals for development:

    ```bash
    # Run backend in development mode (includes hot reload)
    make backend
    ```

    ```bash
    # In another terminal, run frontend in development mode (includes hot reload)
    make frontend
    ```

    The `make backend` and `make frontend` commands automatically install dependencies, so you don't need to run install commands separately.

    The frontend is served at `http://localhost:3000` and the backend at `http://localhost:7860`.

3. Optional: Install pre-commit hooks to help keep your changes clean and well-formatted.

    With pre-commit hooks installed, you must use `uv run git commit` instead of `git commit` directly.

    `make init` installs pre-commit hooks automatically, or you can run the following command to install them manually:

    ```bash
    uv sync
    uv run pre-commit install
    ```

4. To test your changes before pushing commits, run `make lint`, `make format`, and `make unit_tests`.
To run all tests, including coverage, unit, and integration, tests, run `make tests`.

</TabItem>
<TabItem value="Windows" label="Windows">

Since Windows doesn't include `make`, building and running Langflow from source uses `npm` and `uv`.

To set up the Langflow development environment, run the frontend and backend in separate terminals:

1. To run the frontend development server with hot reload, run the following commands:

    ```bash
    cd src/frontend
    npm install
    npm run start
    ```

2. In a separate terminal, run the following command to run the backend:

    ```bash
    uv run langflow run --backend-only
    ```

The frontend is served at `http://localhost:3000` and the backend at `http://localhost:7860`. This setup preserves hot reload for frontend development, with no need to copy build files after every edit.

</TabItem>
</Tabs>

### Troubleshoot frontend build issues

If you encounter frontend build problems or are upgrading from an older version of Langflow, run `make run_clic` once.

```bash
make run_clic
```

This command cleans the build cache and rebuilds everything from scratch, which resolves most frontend-related issues when switching between versions.

### Debug

The repo includes a `.vscode/launch.json` file for debugging the backend in VSCode, which is faster than debugging with Docker Compose.

For more information, see the [VSCode documentation](https://code.visualstudio.com/docs/debugtest/debugging#_start-a-debugging-session).

### Additional contribution guides

- [Contribute bundles](./contributing-bundles.mdx)
- [Contribute components](./contributing-components.mdx)
- [Contribute tests](./contributing-component-tests.mdx)
- [Contribute templates](./contributing-templates.mdx)

## Contribute documentation

The documentation is built using [Docusaurus](https://docusaurus.io/) and written in [Markdown](https://docusaurus.io/docs/markdown-features).
For style guidance, see the [Google Developer Documentation Style Guide](https://developers.google.com/style).

1. Install [Node.js](https://nodejs.org/en/download/package-manager) and [Yarn](https://yarnpkg.com/)

2. Fork the [Langflow GitHub repository](https://github.com/langflow-ai/langflow).

3. Add the new remote to your local repository on your local machine:

    ```bash
    git remote add FORK_NAME https://github.com/GIT_USERNAME/langflow.git
    ```

    Replace the following:

    * `FORK_NAME`: A name for your fork of the repository
    * `GIT_USERNAME`: Your Git username

4. From the root of your local Langflow fork, change to the `/docs` directory:

    ```bash
    cd docs
    ```

    If you're using a dev container, run the documentation build from outside the container on your host terminal, not from within the dev container workspace.
    The documentation build may not work properly when run inside the dev container.

5. Install dependencies and start a local Docusaurus static site with hot reload:

    ```bash
    yarn install
    yarn start
    ```

    The documentation is served at `http://localhost:3000`.

6. To edit and create content, work with the `.mdx` files in the `langflow/docs/docs` directory.

    Create new files in `.mdx` format.

    Navigation is defined in `langflow/docs/sidebars.js`.

    Most pages use a `slug` for shorthand cross-referencing, rather than supplying the full or relative directory path.
    For example, if a page has a `slug` of `/cool-page`, you can link to it with `[Cool page](/cool-page)` from any other `/docs` page.

7. Recommended: After making some changes, run `yarn build` to build the site locally with more robust logging.
This can help you find broken links before creating a PR.

## Open a pull request

To submit a pull request, do the following:

1. Open a new GitHub pull request with your patch against the `main` branch.
2. Ensure the PR title follows semantic commit conventions. For example, features are `feat: add new feature` and fixes are `fix: correct issue with X`.

Some additional guidance on pull request titles:
* Ensure the pull request description clearly describes the problem and solution. If the PR fixes an issue, include a link to the fixed issue in the PR description with `Fixes #1234`.
* Pull request titles appear in Langflow's release notes, so they should explain what the PR does as explicitly as possible.
* Pull requests should strive to fix one thing **only**, and should contain a good description of what is being fixed.

3. A Langflow maintainer will review your pull request and may request changes, so ensure you pay attention to your PRs. Thanks for your contribution!

For more information, see the [Python Developer's Guide](https://devguide.python.org/getting-started/pull-request-lifecycle/index.html#making-good-commits).


================================================================================
filename: Contributing/contributing-templates.mdx
================================================================================

---
title: Contribute templates
slug: /contributing-templates
---

import Icon from "@site/src/components/icon";

Follow these best practices when submitting a template to Langflow.

For template formatting examples, see [`/starter_projects`](https://github.com/langflow-ai/langflow/tree/main/src/backend/base/langflow/initial_setup/starter_projects) in the Langflow repository.

## Create a PR to submit your template

Follow these steps to submit your template:

1. Fork the [Langflow repository](https://github.com/langflow-ai/langflow) on GitHub.
2. On your fork, add your `template.json` file to `/starter_projects`.
3. Include the [required items for template submission](#required-items-for-template-submission).
4. Create a Pull Request (PR) from your fork to the main Langflow repository.
5. Include a screenshot of your template in the PR.

The Langflow team will review your PR, offer feedback, and, if approved, merge the template.

## Required items for template submission

Include the following items and follow these guidelines when submitting your template.

### Name

The template name must be concise and contain no more than three words.
Capitalize only the first letter of each word.
For example: **Blog Writer** or **Travel Planning Agent**.

### Description

A brief, informative description that is shown in the visual editor to help users understand the template's purpose and use cases.
For example:

```json
   "description": "Auto-generate a customized blog post from instructions and referenced articles.",
```

### Icons

Use icons from the [Lucide](https://lucide.dev/icons/) icon library.

### Flow

Use only <Icon name="Component" aria-hidden="true" /> **Core components** and <Icon name="Blocks" aria-hidden="true" /> **Bundles**.
Don't use custom components that aren't part of the Langflow codebase.

Include brief README, quickstart, or other essential details in a <Icon name="StickyNote" aria-hidden="true"/> **Note**. Notes accept Markdown syntax.
For example:

```text
# Financial Assistant Agents

The Financial Assistant Agent retrieves web content and writes reports about finance.

## Prerequisites

* [OpenAI API key](https://platform.openai.com/api-keys)
* [Tavily AI Search key](https://docs.tavily.com/welcome)
* [Sambanova API key](https://sambanova.ai/)

## Quickstart

1. In both **Agent** components, add your OpenAI API key.
2. In the **Model Provider** field, select **Sambanova**, and select a model.
3. In the **Sambanova** component, add your **Sambanova API key**.
4. In the **Tavily Search** component, add your **Tavily API key**.
5. Click the **Playground** and ask `Why did Nvidia stock drop in January?`
```

### Format

Submit the template in JSON format.

### Tags
Assign the template to one of the following categories:

- Assistants
- Classification
- Coding
- Content Generation
- Q&A
- Prompting
- RAG
- Agents

For more information, see the Langflow repository's [template categories](https://github.com/langflow-ai/langflow/blob/main/src/frontend/src/modals/templatesModal/index.tsx#L27-L57).


================================================================================
filename: Deployment/deployment-architecture.mdx
================================================================================

---
title: Langflow architecture on Kubernetes
slug: /deployment-architecture
---

There are two broad types of Langflow deployments:

* **Langflow IDE (development)**: Deploy both the Langflow visual editor (frontend) and API (backend). Typically, this is used for development environments where developers use the visual editor to create and manage flows before packaging and serving them through a production runtime deployment.

    The Langflow repository's example [`docker-compose.yml`](https://github.com/langflow-ai/langflow/blob/main/docker_example/docker-compose.yml) file builds a Langflow IDE image.

    For information about IDE deployments on Kubernetes, see [Deploy the Langflow development environment on Kubernetes](/deployment-kubernetes-dev).


* **Langflow runtime (production)**: Deploy the Langflow runtime for production flows, which is headless (backend only) service focused on serving the Langflow API. This is used for production environments where flows are executed programmatically without the need for the visual editor. The server exposes your flows as endpoints, and then runs only the processes necessary to serve each flow.

    An external PostgreSQL database is strongly recommended with this deployment type to improve scalability and reliability as compared to the default SQLite database.

    For information about runtime deployments on Kubernetes, see [Deploy the Langflow production environment on Kubernetes](/deployment-kubernetes-prod).

    :::tip
    You can start Langflow in headless mode with the `LANGFLOW_BACKEND_ONLY` [environment variable](/environment-variables).
    :::

You can also [deploy the Langflow IDE and runtime on Docker](/deployment-docker).

## Benefits of deploying Langflow on Kubernetes

Deploying on Kubernetes offers the following advantages:

* **Scalability**: Kubernetes allows you to scale the Langflow service to meet the demands of your workload.
* **Availability and resilience**: Kubernetes provides built-in resilience features, such as automatic failover and self-healing, to ensure that the Langflow service is always available.
* **Security**: Kubernetes provides security features, such as role-based access control and network isolation, to protect the Langflow service and its data.
* **Portability**: Kubernetes is a portable platform, which means that you can deploy the Langflow service to any Kubernetes cluster, on-premises or in the cloud.

    Langflow can be deployed on cloud platforms like AWS EKS, Google GKE, or Azure AKS. For more information, see the [Langflow Helm charts repository](https://github.com/langflow-ai/langflow-helm-charts).

## Langflow deployment

A typical Langflow deployment includes:

* **Langflow services**: The Langflow API and, for IDE deployments, the visual editor.
* **Kubernetes cluster**: The Kubernetes cluster provides a platform for deploying and managing Langflow and its supporting services.
* **Persistent storage**: Persistent storage is used to store the Langflow service's data, such as models and training data.
* **Ingress controller**: The ingress controller provides a single entry point for traffic to the Langflow service.
* **Load balancer**: Balances traffic across multiple Langflow replicas.
* **Vector database**: If you are using Langflow for RAG, you can integrate with the vector database in Astra Serverless.

![Langflow reference architecture on Kubernetes](/img/langflow-reference-architecture.png)

## Environment isolation

It is recommended to deploy and run separate development and production environments for Langflow:

* Deploy the IDE in your development environment where your developers prototype and test new flows.
* Deploy the runtime in your production environment to serve flows as standalone services.

![Langflow environments](/img/langflow-env.png)

This separation is designed to enhance security, support an organized software development pipeline, and optimize infrastructure resource allocation:

* **Isolation**: By separating the development and production environments, you can better isolate different phases of the application lifecycle. This isolation minimizes the risk of development-related issues impacting the production environments.
* **Access control**: Different security policies and access controls can be applied to each environment. Developers may require broader access in the IDE for testing and debugging, while the runtime environment can be locked down with stricter security measures.
* **Reduced attack surface**: The runtime environment is configured to include only essential components, reducing the attack surface and potential vulnerabilities.
* **Optimized resource usage and cost efficiency**: By separating the two environments, you can allocate resources more effectively. Each flow can be deployed independently, providing fine-grained resource control.
* **Scalability**: The runtime environment can be scaled independently based on application load and performance requirements, without affecting the development environment.

## Next steps

* [Best practices for Langflow on Kubernetes](/deployment-prod-best-practices)
* [Deploy the Langflow development environment on Kubernetes](/deployment-kubernetes-dev)
* [Deploy the Langflow production environment on Kubernetes](/deployment-kubernetes-prod)


================================================================================
filename: Deployment/deployment-caddyfile.mdx
================================================================================

---
title: Deploy Langflow on a remote server
slug: /deployment-caddyfile
---

Learn how to deploy Langflow on your own remote server with secure web access.
This guide walks you through setting up Langflow on a remote server using [Docker](https://docs.docker.com/) and configuring secure web access with [Caddy](https://caddyserver.com/docs/).

## Prerequisites

* A server with a dual-core CPU and at least 2 GB of RAM.
This example uses [Hetzner cloud](https://www.hetzner.com/) for hosting. Your deployment may vary.

## Connect to your remote server with SSH

1. Create an SSH key to connect to your server remotely.
For example:

    ```bash
    ssh-keygen -t ed25519 -C "DANA@EXAMPLE.COM"
    ```

    Replace `DANA@EXAMPLE.COM` with the email address that you want to associate with the SSH key.

2. In your terminal, follow the instructions to create your SSH key pair.
This creates both a private and public key.
To copy the public key from your terminal, enter the following command:

    ```bash
    cat ~/Downloads/host-lf.pub | pbcopy
    ```

3. In your remote server, add the SSH key you copied in the previous step.
For example, if you are using a Hetzner cloud server, click **Server**, and then select **SSH keys** to add an SSH key.

4. To connect to your server with SSH, enter the following command.

    ```bash
    ssh -i PATH_TO_PRIVATE_KEY/PRIVATE_KEY_NAME root@SERVER_IP_ADDRESS
    ```

    Replace the following:

    * `PATH_TO_PRIVATE_KEY/PRIVATE_KEY_NAME`: The path to your private SSH key file that matches the public key you added to your server
    * `SERVER_IP_ADDRESS`: Your server's IP address

5. When prompted for a key fingerprint, type `yes`.

The terminal output indicates if the connection succeeds or fails.
The following response was returned after connecting to a Hetzner cloud server:

```text
System information as of Mon May 19 04:34:44 PM UTC 2025

System load:  0.0               Processes:             129
Usage of /:   1.5% of 74.79GB   Users logged in:       0
Memory usage: 5%                IPv4 address for eth0: 5.161.250.132
Swap usage:   0%                IPv6 address for eth0: 2a01:4ff:f0:4de7::1
```

## Deploy Langflow on your server

Now that your local machine is connected to your remote server with SSH, you can install Docker, create a `docker-compose.yml` file, and serve it publicly with a reverse proxy, such as Caddy.

1. Install Docker on your server.

    Since this example server is an Ubuntu server, it can install snap packages.
    If you aren't using Ubuntu or you prefer a different installation method, see the [official Docker installation guide](https://docs.docker.com/get-started/get-docker/) for instructions for your operating system.

    ```bash
    snap install docker
    ```

2. Create a file called `docker-compose.yml`, and then open it in a text editor:

    ```bash
    touch docker-compose.yml && nano docker-compose.yml
    ```

3. Add the following values to `docker-compose.yml`, and then save the file.

    The following example defines the Langflow service from the `langflow:latest` image and a Caddy service to expose Langflow through a reverse proxy.

    :::tip
    The [host-langflow](https://github.com/datastax/host-langflow) repository offers pre-built copies of this `docker-compose.yml` and `Caddyfile`, if you prefer to fork the repository to your server.
    :::

    ```yml
    version: "3.8"

    services:
      langflow:
        image: langflowai/langflow:latest
        ports:
          - "7860:7860"
        environment:
          - LANGFLOW_HOST=0.0.0.0
          - LANGFLOW_PORT=7860

      caddy:
        image: caddy:latest
        ports:
          - "80:80"
          - "443:443"
        volumes:
          - ./Caddyfile:/etc/caddy/Caddyfile
          - caddy_data:/data
          - caddy_config:/config
        depends_on:
          - langflow

    volumes:
      caddy_data:
      caddy_config:
    ```

4. Create a file called `Caddyfile`:

    ```bash
    touch Caddyfile && nano Caddyfile
    ```

5. Add the following values to `Caddyfile`, and then save the file.

    This Caddyfile configures Caddy to listen on port 80, and forward all incoming requests to port 80 to the Langflow service at port 7860.

    ```
    :80 {
        reverse_proxy langflow:7860
    }
    ```

6. To deploy your server, run `docker-compose up`.

    When the `Welcome to Langflow` message appears, Langflow is running and accessible internally at `http://0.0.0.0:7860` inside the Docker network.

7. To access your Langflow server over the public internet, navigate to your server's public IP address, such as `http://5.161.250.132`.
This address uses HTTP because HTTPS isn't enabled yet.

8. Recommended: Enable HTTPS:

    1. Modify your domain's A record to point to your server's IP address. For example:

        ```
        Type: A
        Name: langflow
        Value: 5.161.250.132  # Set to your server's IP address
        ```

    2. Stop your server.

    3. Modify your Caddyfile to include port `443` so Caddy can forward both HTTP (port 80) and HTTPS (port 443) requests to the Langflow service:

        ```
        :80, :443 {
            reverse_proxy langflow:7860
        }
        ```

    4. Start your server.

        When users visit your domain, Caddy recognizes the incoming traffic and automatically routes it to your server with a secure, encrypted connection.

9. To exit your SSH session, type `exit`.

## See also

To package your local flows into a custom Docker image, see [Containerize a Langflow application](/develop-application).

For a step-by-step guide to deploying Langflow, including deployments to [fly.io](https://fly.io/) and [Flightcontrol.dev](https://www.flightcontrol.dev/), see [How to Host Langflow Anywhere](https://www.youtube.com/watch?v=q4qt5hSnte4).


================================================================================
filename: Deployment/deployment-docker.mdx
================================================================================

---
title: Deploy Langflow on Docker
slug: /deployment-docker
---

import PartialPodmanAlt from '@site/docs/_partial-podman-alt.mdx';

<PartialPodmanAlt />

Running applications in Docker containers ensures consistent behavior across different systems and eliminates dependency conflicts.

You can use the Langflow Docker image to start a Langflow container.

This guide demonstrates several ways to deploy Langflow with [Docker](https://docs.docker.com/) and [Docker Compose](https://docs.docker.com/compose/):

* [Quickstart](#quickstart): Start a Langflow container with default values.
* [Use Docker Compose](#clone): Clone the Langflow repo, and then use Docker Compose to build the Langflow Docker container.
This option provides more control over the configuration, including a persistent PostgreSQL database service, while still using the base Langflow Docker image.
* [Create a custom flow image](#package-your-flow-as-a-docker-image): Use a Dockerfile to package a flow as a Docker image.
* [Create a custom Langflow image](#customize-the-langflow-docker-image): Use a Dockerfile to package a custom Langflow Docker image that includes your own code, custom dependencies, or other modifications.

## Quickstart: Start a Langflow container with default values {#quickstart}

With Docker installed and running on your system, run the following command:

```shell
docker run -p 7860:7860 langflowai/langflow:latest
```

Then, access Langflow at `http://localhost:7860/`.

This container runs a pre-built Docker image with default settings.
For more control over the configuration, see [Clone the repo and run the Langflow Docker container](#clone).

## Clone the repo and run the Langflow Docker container {#clone}

Cloning the Langflow repository and using Docker Compose gives you more control over your configuration, allowing you to customize environment variables, use a persistent PostgreSQL database service (instead of the default SQLite database), and include custom dependencies.

The default deployment with Docker Compose includes the following:

- **Langflow service**: Runs the latest Langflow image with PostgreSQL as the database.
- **PostgreSQL service**: Provides persistent data storage for flows, users, and settings.
- **Persistent volumes**: Ensures your data survives container restarts.

The complete Docker Compose configuration is available in `docker_example/docker-compose.yml`.

1. Clone the Langflow repository:

   ```shell
   git clone https://github.com/langflow-ai/langflow.git
   ```

2. Navigate to the `docker_example` directory:

   ```shell
   cd langflow/docker_example
   ```

3. Run the Docker Compose file:

   ```shell
   docker compose up
   ```

4. Access Langflow at `http://localhost:7860/`.

### Customize your deployment

You can customize the Docker Compose configuration to fit your specific deployment.

For example, to configure the container's database credentials using a `.env` file, do the following:

1. Create a `.env` file with your database credentials in the same directory as `docker-compose.yml`:

    ```text
    # Database credentials
    POSTGRES_USER=myuser
    POSTGRES_PASSWORD=mypassword
    POSTGRES_DB=langflow

    # Langflow configuration
    LANGFLOW_DATABASE_URL=postgresql://myuser:mypassword@postgres:5432/langflow
    LANGFLOW_CONFIG_DIR=/app/langflow
    ```

2. Modify the `docker-compose.yml` file to reference the `.env` file for both the `langflow` and `postgres` services:

    ```yaml
    services:
      langflow:
        environment:
          - LANGFLOW_DATABASE_URL=${LANGFLOW_DATABASE_URL}
          - LANGFLOW_CONFIG_DIR=${LANGFLOW_CONFIG_DIR}
      postgres:
        environment:
          - POSTGRES_USER=${POSTGRES_USER}
          - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
          - POSTGRES_DB=${POSTGRES_DB}
    ```

For a complete list of available environment variables, see [Langflow environment variables](/environment-variables).

For more customization options, see [Customize the Langflow Docker image with your own code](#customize-the-langflow-docker-image).

## Package your flow as a Docker image {#package-your-flow-as-a-docker-image}

This section shows you how to create a Dockerfile that builds a Docker image containing your Langflow flow. This approach is useful when you want to distribute a specific flow as a standalone container or deploy it to environments like Kubernetes.

Unlike the previous sections that use pre-built images, this method builds a custom image with your flow embedded inside it.

1. Create a project directory, and change directory into it.

    ```bash
    mkdir langflow-custom && cd langflow-custom
    ```

2. Add your flow's JSON file to the directory. You can download an example, or use your own:

    ```bash
    # Download an example flow
    wget https://raw.githubusercontent.com/langflow-ai/langflow-helm-charts/refs/heads/main/examples/flows/basic-prompting-hello-world.json

    # Or copy your own flow file
    cp /path/to/your/flow.json .
    ```

3. Create a Dockerfile to build your custom image:

    ```dockerfile
    FROM langflowai/langflow:latest
    RUN mkdir /app/flows
    COPY ./*.json /app/flows/
    ENV LANGFLOW_LOAD_FLOWS_PATH=/app/flows
    ```

This Dockerfile uses the official Langflow image as the base, creates a directory for your flows, copies your JSON flow files into the directory, and sets the environment variable to tell Langflow where to find the flows.

4. Build and test your custom image:

    ```bash
    docker build -t myuser/langflow-custom:1.0.0 .
    docker run -p 7860:7860 myuser/langflow-custom:1.0.0
    ```

5. Push your image to Docker Hub (optional):

    ```bash
    docker push myuser/langflow-custom:1.0.0
    ```

Your custom image now contains your flow and can be deployed anywhere Docker runs. For Kubernetes deployment, see [Deploy the Langflow production environment on Kubernetes](/deployment-kubernetes-prod).

## Customize the Langflow Docker image with your own code {#customize-the-langflow-docker-image}

While the previous section showed how to package a flow with a Docker image, this section shows how to customize the Langflow application itself. This is useful when you need to add custom Python packages or dependencies, modify Langflow's configuration or settings, include custom components or tools, or add your own code to extend Langflow's functionality.

This example demonstrates how to customize the **Message History** component, but the same approach can be used for any code modifications.

```dockerfile
FROM langflowai/langflow:latest

# Set working directory
WORKDIR /app

# Copy your modified memory component
COPY src/backend/base/langflow/components/helpers/memory.py /tmp/memory.py

# Find the site-packages directory where langflow is installed
RUN python -c "import site; print(site.getsitepackages()[0])" > /tmp/site_packages.txt

# Replace the file in the site-packages location
RUN SITE_PACKAGES=$(cat /tmp/site_packages.txt) && \
    echo "Site packages at: $SITE_PACKAGES" && \
    mkdir -p "$SITE_PACKAGES/langflow/components/helpers" && \
    cp /tmp/memory.py "$SITE_PACKAGES/langflow/components/helpers/"

# Clear Python cache in the site-packages directory only
RUN SITE_PACKAGES=$(cat /tmp/site_packages.txt) && \
    find "$SITE_PACKAGES" -name "*.pyc" -delete && \
    find "$SITE_PACKAGES" -name "__pycache__" -type d -exec rm -rf {} +

# Expose the default Langflow port
EXPOSE 7860

# Command to run Langflow
CMD ["python", "-m", "langflow", "run", "--host", "0.0.0.0", "--port", "7860"]
```

To use this custom Dockerfile, do the following:

1. Create a directory for your custom Langflow setup:

    ```bash
    mkdir langflow-custom && cd langflow-custom
    ```

2. Create the necessary directory structure for your custom code.
In this example, Langflow expects `memory.py` to exist in the `/helpers` directory, so you create a directory in that location.

    ```bash
    mkdir -p src/backend/base/langflow/components/helpers
    ```

3. Place your modified `memory.py` file in the `/helpers` directory.

4. Create a new file named `Dockerfile` in your `langflow-custom` directory, and then copy the Dockerfile contents shown above into it.

5. Build and run the image:

    ```bash
    docker build -t myuser/langflow-custom:1.0.0 .
    docker run -p 7860:7860 myuser/langflow-custom:1.0.0
    ```

This approach can be adapted for any other components or custom code you want to add to Langflow by modifying the file paths and component names.


================================================================================
filename: Deployment/deployment-gcp.mdx
================================================================================

---
title: Deploy Langflow on Google Cloud Platform
slug: /deployment-gcp
---

This guide demonstrates how to deploy Langflow on [Google Cloud Platform](https://console.cloud.google.com/) with a Cloud Shell script that walks through the process of setting up a Debian-based VM with the Langflow package, Nginx, and the necessary configurations to run the Langflow development environment in GCP.

To use this script, you need a Google Cloud project with the necessary permissions to create resources.

1. Follow this link to launch the Cloud Shell with the GCP deployment script from the Langflow repository:

   [![Deploy to Google Cloud](https://gstatic.com/cloudssh/images/open-btn.svg)](https://console.cloud.google.com/cloudshell/open?git_repo=https://github.com/langflow-ai/langflow&working_dir=scripts/gcp&shellonly=true&tutorial=walkthroughtutorial.md)

2. Click **Trust repo**.

    Some `gcloud` commands may not run in an ephemeral Cloud Shell environment.

3. Click **Start**, and then follow the tutorial to deploy Langflow.

:::info
This deployment uses a [spot (preemptible) instance](https://cloud.google.com/compute/docs/instances/preemptible) as a cost-effective option to demonstrate how to deploy Langflow on GCP.
However, due to the nature of spot instances, the VM can be terminated at any time if Google Cloud needs to reclaim the resources.

For a more stable deployment, consider using a regular VM instance instead of a spot instance.

For more information, see the [GCP pricing calculator](https://cloud.google.com/products/calculator?hl=en).
:::


================================================================================
filename: Deployment/deployment-hugging-face-spaces.mdx
================================================================================

---
title: Deploy Langflow on Hugging Face Spaces
slug: /deployment-hugging-face-spaces
---

This guide explains how to deploy Langflow on [Hugging Face Spaces](https://huggingface.co/spaces/).

1. Go to the [Langflow Space](https://huggingface.co/spaces/Langflow/Langflow?duplicate=true).
2. Click **Duplicate Space**.
3. Configure the duplicated Space:

   1. Enter a name for your Space.
   2. Select either public or private visibility.
   3. Click **Duplicate Space**.

   ![Hugging Face deployment dialog](/img/hugging-face-deployment.png)

When setup is complete, you're redirected to your new Space automatically, and your Langflow instance is ready to use.


================================================================================
filename: Deployment/deployment-kubernetes-dev.mdx
================================================================================

---
title: Deploy the Langflow development environment on Kubernetes
slug: /deployment-kubernetes-dev
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

The [Langflow integrated development environment (IDE) Helm chart](https://github.com/langflow-ai/langflow-helm-charts/tree/main/charts/langflow-ide) is designed to provide a complete environment for developers to create, test, and debug their flows. It includes both the Langflow API and visual editor.

## Prerequisites

- A [Kubernetes](https://kubernetes.io/docs/setup/) cluster
- [kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl)
- [Helm](https://helm.sh/docs/intro/install/)

## Prepare a Kubernetes cluster

This example uses [Minikube](https://minikube.sigs.k8s.io/docs/start/), but you can use any Kubernetes cluster.

1. Create a Kubernetes cluster on Minikube:

	```shell
	minikube start
	```

2. Set `kubectl` to use Minikube:

	```shell
	kubectl config use-context minikube
	```

## Install the Langflow IDE Helm chart

1. Add the repository to Helm, and then update it:

	```shell
	helm repo add langflow https://langflow-ai.github.io/langflow-helm-charts
	helm repo update
	```

2. Install Langflow with the default options in the `langflow` namespace:

	```shell
	helm install langflow-ide langflow/langflow-ide -n langflow --create-namespace
	```

3. Check the status of the pods:

	```shell
	kubectl get pods -n langflow
	```

## Access the Langflow IDE

Enable local port forwarding to access Langflow from your local machine:

1. Make the Langflow API accessible from your local machine at port 7860:

    ```shell
    kubectl port-forward -n langflow svc/langflow-service-backend 7860:7860
    ```

2. Make the visual editor accessible from your local machine at port 8080:

    ```shell
    kubectl port-forward -n langflow svc/langflow-service 8080:8080
    ```

Now you can do the following:

- Access the Langflow API at `http://localhost:7860`.
- Access the visual editor at `http://localhost:8080`.

## Modify your Langflow IDE deployment

You can modify the Langflow IDE Helm chart's [`values.yaml`](https://github.com/langflow-ai/langflow-helm-charts/blob/main/charts/langflow-ide/values.yaml) file to customize your deployment.
The following sections describe some common modifications.

If you need to set secrets, Kubernetes secrets are recommended.

### Deploy a different Langflow version

The Langflow IDE Helm chart deploys the latest Langflow version by default.

To specify a different Langflow version, set the `langflow.backend.image.tag` and `langflow.frontend.image.tag` values to your preferred version.
For example:

```yaml
langflow:
  backend:
    image:
      tag: "1.0.0a59"
  frontend:
    image:
      tag: "1.0.0a59"
```

### Use external storage for the Langflow database

The Langflow IDE Helm chart uses the default Langflow database configuration, specifically a SQLite database stored in a local persistent disk.

If you want to use an [external PostgreSQL database](/configuration-custom-database), use `postgresql` chart or `externalDatabase` to configure the database connection in `values.yaml`.

<Tabs>
<TabItem value="postgresql" label="postgresql" default>

Use the built-in PostgreSQL chart:

```yaml
postgresql:
  enabled: true
  auth:
    username: "langflow"
    password: "langflow-postgres"
    database: "langflow-db"
```

</TabItem>
<TabItem value="externaldatabase" label="externalDatabase">

If you don't want to use the built-in PostgreSQL chart, set `postgresql.enabled` to `false`, and then configure the database connection in `langflow.backend.externalDatabase`:

```yaml
postgresql:
  enabled: false

langflow:
  backend:
    externalDatabase:
      enabled: true
      driver:
        value: "postgresql"
      port:
        value: "5432"
      user:
        value: "langflow"
      password:
        valueFrom:
          secretKeyRef:
            key: "password"
            name: "your-secret-name"
      database:
        value: "langflow-db"
    sqlite:
      enabled: false
```

</TabItem>
</Tabs>

### Configure scaling

To configure scaling for the Langflow IDE Helm chart deployment, you must set `replicaCount` (horizontal scaling) and `resources` (vertical scaling) for both the `langflow.backend` and `langflow.frontend`.

If your flows rely on a shared state, such as [built-in chat memory](/memory), you must also set up a shared database when scaling horizontally.

```yaml
langflow:
  backend:
    replicaCount: 1
    resources:
      requests:
        cpu: 0.5
        memory: 1Gi
      # limits:
      #   cpu: 0.5
      #   memory: 1Gi

  frontend:
    enabled: true
    replicaCount: 1
    resources:
      requests:
        cpu: 0.3
        memory: 512Mi
      # limits:
      #   cpu: 0.3
      #   memory: 512Mi
```

## See also

* [Best practices for Langflow on Kubernetes](/deployment-prod-best-practices)
* [Deploy the Langflow production environment on Kubernetes](/deployment-kubernetes-prod)
* [Langflow Helm Charts repository](https://github.com/langflow-ai/langflow-helm-charts)


================================================================================
filename: Deployment/deployment-kubernetes-prod.mdx
================================================================================

---
title: Deploy the Langflow production environment on Kubernetes
slug: /deployment-kubernetes-prod
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

The [Langflow runtime Helm chart](https://github.com/langflow-ai/langflow-helm-charts/blob/main/charts/langflow-runtime) is tailored for deploying applications in a production environment. It is focused on stability, performance, isolation, and security to ensure that applications run reliably and efficiently.

:::warning
For security reasons, the default Langflow runtime Helm chart sets [`readOnlyRootFilesystem: true`](https://github.com/langflow-ai/langflow-helm-charts/blob/main/charts/langflow-runtime/values.yaml#L46). This setting prevents modifications to the container's root filesystem at runtime, which is a recommended security measure in production environments.

If `readOnlyRootFilesystem` is disabled (`false`), it degrades your deployment's security posture. Only disable this setting if you understand the security implications and you have implemented other security measures.

For more information, see the [Kubernetes documentation](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/).
:::

## Prerequisites

- A [Kubernetes](https://kubernetes.io/docs/setup/) server
- [kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl)
- [Helm](https://helm.sh/docs/intro/install/)

## Install the Langflow runtime Helm chart

1. Add the repository to Helm:

    ```shell
    helm repo add langflow https://langflow-ai.github.io/langflow-helm-charts
    helm repo update
    ```

2. Install the Langflow app with the default options in the `langflow` namespace.

    <Tabs groupId="">
    <TabItem value="Install chart with custom image" label="Install chart with custom image" default>

    If you have a [custom image with packaged flows](/deployment-docker#package-your-flow-as-a-docker-image), you can deploy Langflow by overriding the default [`values.yaml`](https://github.com/langflow-ai/langflow-helm-charts/blob/main/charts/langflow-runtime/values.yaml) with the `--set` flag:

    ```shell
    helm install my-langflow-app langflow/langflow-runtime -n langflow --create-namespace --set image.repository=myuser/langflow-hello-world --set image.tag=1.0.0
    ```

    </TabItem>
    <TabItem value="Install chart and download flow" label="Install chart and download flow">

    Install the chart and download flows from a URL with the `--set` flag:

    ```shell
    helm install my-langflow-app-with-flow langflow/langflow-runtime \
      -n langflow \
      --create-namespace \
      --set 'downloadFlows.flows[0].url=https://raw.githubusercontent.com/langflow-ai/langflow/dev/tests/data/basic_example.json'
    ```

    If your shell requires escaping square brackets, modify the `--set` path as needed.
    For example, `--set 'downloadFlows.flows\[0\].url=https://raw.githubusercontent.com/langflow-ai/langflow/dev/tests/data/basic_example.json'`.

    </TabItem>
    </Tabs>

3. Check the status of the pods:

    ```shell
    kubectl get pods -n langflow
    ```

## Access the Langflow runtime

1. Get your service name:

    ```shell
    kubectl get svc -n langflow
    ```

    The service name is your release name suffixed by `-langflow-runtime`. For example, if you used `helm install my-langflow-app-with-flow`, then the service name is `my-langflow-app-with-flow-langflow-runtime`.

2. Enable port forwarding to access Langflow from your local machine:

    ```shell
    kubectl port-forward -n langflow svc/my-langflow-app-with-flow-langflow-runtime 7860:7860
    ```

3. Confirm you can access the API by calling `http://localhost:7860/api/v1/flows/`:

    ```shell
    curl -v http://localhost:7860/api/v1/flows/
    ```

    A successful request returns a list of flows.

4. Run a packaged flow.
The following example gets the first flow ID from the flows list, and then runs the flow:

    ```shell
    # Get flow ID
    id=$(curl -s "http://localhost:7860/api/v1/flows/" | jq -r '.[0].id')

    # Run flow
    curl -X POST \
        "http://localhost:7860/api/v1/run/$id?stream=false" \
        -H 'Content-Type: application/json' \
        -d '{
          "input_value": "Hello!",
          "output_type": "chat",
          "input_type": "chat"
        }'
    ```

## Configure secrets and environment variables

Use the `.env` section of the Langflow runtime Helm chart's [`values.yaml`](https://github.com/langflow-ai/langflow-helm-charts/blob/main/charts/langflow-runtime/values.yaml) file to define environment variables for your Langflow deployment.
This includes built-in [Langflow environment variables](/environment-variables), as well as [global variables](/configuration-global-variables) used by your flows.

Langflow can source global variables from your runtime environment, such as Kubernetes secrets referenced in `values.yaml`.
For example, the Langflow runtime Helm chart's [example flow JSON](https://raw.githubusercontent.com/langflow-ai/langflow-helm-charts/refs/heads/main/examples/flows/basic-prompting-hello-world.json) uses a global variable that is a secret.
If you want to run this flow in your Langflow deployment on Kubernetes, you need to include the secret in your runtime configuration.

:::tip
When you export flows as JSON files, it's recommended to omit secrets.
Whether or not a secret is included depends on how you declare the secret in your flow and whether you use the **Save with my API keys** option.
For more information, see [Import and export flows](/concepts-flows-import).
:::

### Set secrets

Kubernetes secrets are the recommended way to store sensitive values and credentials.

Use `secretKeyRef` to reference a Kubernetes secret in `values.yaml`:

```yaml
env:
  - name: OPENAI_API_KEY
    valueFrom:
      secretKeyRef:
        name: openai-credentials
        key: openai-key
```

<details>
<summary>Create and set secrets with `kubectl` and `helm`</summary>

You can use `kubectl` and `helm` commands to create and set secrets:

1. Create a secret:

    ```shell
    kubectl create secret generic openai-credentials \
      --namespace langflow \
      --from-literal=OPENAI_API_KEY=sk...
    ```

2. Verify the secret exists:

    ```shell
    kubectl get secrets -n langflow openai-credentials
    ```

    The result is encrypted.

3. Upgrade the Helm release to use the secret:

    ```shell
    helm upgrade my-langflow-app-image langflow/langflow-runtime -n langflow \
      --reuse-values \
      --set "extraEnv[0].name=OPENAI_API_KEY" \
      --set "extraEnv[0].valueFrom.secretKeyRef.name=openai-credentials" \
      --set "extraEnv[0].valueFrom.secretKeyRef.key=OPENAI_API_KEY"
    ```

    Escape square brackets if required by your shell.

</details>

### Set the log level and other configuration variables

For non-sensitive variables, such as `LANGFLOW_LOG_LEVEL`, you can set the value directly in `values.yaml`:

```yaml
env:
  - name: LANGFLOW_LOG_LEVEL
    value: "INFO"
```

## Configure scaling

Use `replicaCount` and `resources` in the Langflow runtime Helm chart's [`values.yaml`](https://github.com/langflow-ai/langflow-helm-charts/blob/main/charts/langflow-runtime/values.yaml) file to configure scaling:

* **Horizontal scaling**: Use `replicaCount` to set the number of replicas for your Langflow deployment.

    ```yaml
    replicaCount: 3
    ```

* **Vertical scaling**: Use the `resources` section to adjust pod resources depending on your application's needs.

    ```yaml
    resources:
      requests:
        memory: "2Gi"
        cpu: "1000m"
    ```

## See also

* [Best practices for Langflow on Kubernetes](/deployment-prod-best-practices)
* [Langflow Helm Charts repository](https://github.com/langflow-ai/langflow-helm-charts)


================================================================================
filename: Deployment/deployment-nginx-ssl.mdx
================================================================================

---
title: Deploy Langflow with Nginx and SSL
slug: /deployment-nginx-ssl
---

import Icon from "@site/src/components/icon";

Deploy Langflow on a Linux-based server using Nginx as a reverse proxy, Let's Encrypt for SSL certificates, and Certbot for automated certificate management.

This setup encrypts all communications between users and your Langflow server. SSL certificates ensure that sensitive data is protected from eavesdropping and tampering, and the automatic certificate management through Certbot eliminates the complexity of manual SSL configuration.

## Prerequisites

- An Ubuntu or Debian-based Linux server with a dual-core CPU and at least 2 GB of RAM
This example uses [Digital Ocean cloud](https://www.digitalocean.com) for hosting. Your deployment may vary.
- A domain name with external DNS management access
- A DNS record configured to point your domain to your server's external IP address
For example, if your server's IP is `203.0.113.1`, configure your DNS like this:
    ```bash
    Type: A
    Name: langflow.example.com
    Value: 203.0.113.1
    ```

## Connect to your server with SSH

1. Create an SSH key to connect to your server remotely.
For example:

    ```bash
    ssh-keygen -t ed25519 -C "DANA@EXAMPLE.COM"
    ```

    Replace `DANA@EXAMPLE.COM` with the email address that you want to associate with the SSH key.

2. In your terminal, follow the instructions to create your SSH key pair.
This creates both a private and public key.
To copy the public key from your terminal, enter the following command:

    ```bash
    cat ~/Downloads/host-lf.pub | pbcopy
    ```

3. In your server, add the SSH key you copied in the previous step.
For example, if you are using a Digital Ocean cloud server, add this SSH key when the server is created, or with the [Digital Ocean control panel](https://docs.digitalocean.com/products/droplets/how-to/add-ssh-keys/).

4. To connect to your server with SSH, enter the following command.

    ```bash
    ssh -i PATH_TO_PRIVATE_KEY/PRIVATE_KEY_NAME root@SERVER_IP_ADDRESS
    ```

    Replace the following:

    * `PATH_TO_PRIVATE_KEY/PRIVATE_KEY_NAME`: The path to your private SSH key file that matches the public key you added to your server
    * `SERVER_IP_ADDRESS`: Your server's IP address

5. When prompted for a key fingerprint, type `yes`.
The terminal output indicates if the connection succeeds or fails.
The following response was returned after connecting to a Digital Ocean cloud server.
    ```bash
     System information as of Wed Oct  8 21:40:43 UTC 2025

      System load:  0.02              Processes:             103
      Usage of /:   4.1% of 47.35GB   Users logged in:       1
      Memory usage: 10%               IPv4 address for eth0: 165.227.176.236
      Swap usage:   0%                IPv4 address for eth0: 10.17.0.5
    ```

## Install Langflow on your server

To install Langflow on your server, do the following:

1. Update system packages.
    ```bash
    sudo apt update && sudo apt upgrade -y
    ```

2. Install Python and pip.
    ```bash
    sudo apt install python3 python3-pip python3-venv -y
    ```

3. Install uv to manage Python packages.
    Langflow recommends uv for faster installation.
    ```bash
    pip install uv
    ```

4. Create a virtual environment for Langflow.
    ```bash
    uv venv langflow-venv
    source langflow-venv/bin/activate
    ```

5. Install Langflow using uv.
    ```bash
    uv pip install langflow
    ```

6. Optionally, start Langflow.
    ```bash
    uv run langflow run --host 127.0.0.1 --port 7860 &
    ```

    Test access at your `http://YOUR_PUBLIC_IP:7860`.
    Since Langflow is running on `localhost`, you should not be successful.
    In next steps, you will install Nginx as a reverse proxy to handle external access, and Certbot to configure SSL for secure HTTPS access.

## Install Nginx

Nginx is a reverse proxy that receives external requests and forwards them to your Langflow server. In addition to SSL termination, Nginx includes features for load balancing and security. For more information, see the [Nginx documentation](https://nginx.org/en/docs/).

1. Install Nginx on your server.

    ```bash
    sudo apt install nginx -y
    ```

2. Start and enable Nginx on your server.
    ```bash
    sudo systemctl start nginx
    sudo systemctl enable nginx
    ```

3. Create an Nginx configuration file.
Replace **DOMAIN_NAME** with your actual domain name, such as `langflow.example.com`.
    ```bash
    sudo nano /etc/nginx/sites-available/DOMAIN_NAME
    ```

4. Paste the following content to the configuration file you created at `/etc/nginx/sites-available/DOMAIN_NAME`.

    Replace **DOMAIN_NAME** with your actual domain name, such as `langflow.example.com`.
    ```nginx
    server {
        listen 80;
        server_name DOMAIN_NAME;

        # Increase client body size for file uploads
        client_max_body_size 100M;

        location / {
            proxy_pass http://127.0.0.1:7860/;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;

            # WebSocket support for Langflow
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";

            # Timeout settings for long-running flows
            proxy_connect_timeout 60s;
            proxy_send_timeout 60s;
            proxy_read_timeout 300s;

            # Buffer settings
            proxy_buffering off;
            proxy_request_buffering off;
        }
    }
    ```
    This configuration listens on port 80 for standard HTTP. When you install Certbot in a later step, it will modify this configuration to add port 443 for HTTPS.

5. To enable your site configuration, create a symlink between the `/sites-available` and `/sites-enabled` directories. `/sites-available` stores all site configurations, and `/sites-enabled` stores only active configurations which Nginx reads from. Creating this symlink switches this configuration ON.

    ```bash
    sudo ln -s /etc/nginx/sites-available/DOMAIN_NAME /etc/nginx/sites-enabled/DOMAIN_NAME
    ```

6. Check the Nginx configuration file's syntax.
    ```bash
    sudo nginx -t
    ```

7. Restart Nginx.
    ```bash
    sudo systemctl restart nginx
    ```

## Install Certbot and obtain SSL certificates

While Nginx handles SSL termination in encrypting and decrypting traffic, Certbot automatically obtains SSL certificates from Let's Encrypt and configures Nginx to use them.

The Certbot client is [recommended by Let's Encrypt](https://letsencrypt.org/getting-started/#selecting-and-operating-an-acme-client-yourself) for automated certificate management. For more information, see the [Certbot documentation](https://certbot.eff.org/).

Install the Certbot client on your server to manage certificates, and install the [`python3-certbot-nginx` plugin](https://packages.ubuntu.com/source/jammy/python-certbot-nginx) to allow Certbot to modify your server's Nginx configuration.

1. Install Certbot and the `python3-certbot-nginx` plugin.
    ```bash
    sudo apt install certbot python3-certbot-nginx -y
    ```

2. Obtain the SSL certificate for `DOMAIN_NAME` from Let's Encrypt.
    ```bash
    sudo certbot --nginx -d DOMAIN_NAME
    ```

    This result indicates that Certbot succeeded.
    ```bash
    Successfully received certificate.
    Certificate is saved at: /etc/letsencrypt/live/DOMAIN_NAME/fullchain.pem
    Key is saved at: /etc/letsencrypt/live/DOMAIN_NAME/privkey.pem
    ```

    When using `--nginx`, Certbot automatically injects the paths into your Nginx configuration as `ssl_certificate` and `ssl_certificate_key`.

3. Start Langflow from the virtual environment.

    a. Activate the virtual environment.
    ```bash
    source langflow-venv/bin/activate
    ```

    b. Start Langflow in the background.
    ```bash
    uv run langflow run --host 127.0.0.1 --port 7860 &
    ```

4. To test the deployment, navigate to `https://DOMAIN_NAME` in your browser.
5. Verify the SSL certificate is working. Ensure the URL is `https://`, not `http://`.
Your browser's address bar should display a <Icon name="Lock" aria-hidden="true"/> **Lock** icon.
Click <Icon name="Lock" aria-hidden="true"/> **Lock** to view your SSL certificate details.

## See also

- [Deploy Langflow on a remote server with Caddy](/deployment-caddyfile)
- [Deploy Langflow on Docker](/deployment-docker)


================================================================================
filename: Deployment/deployment-overview.mdx
================================================================================

---
title: Langflow deployment overview
slug: /deployment-overview
---

This section includes the different ways to bring your locally-built flows to the world.

* To self-host your local server through an ngrok gateway, see [Deploy a public Langflow server](/deployment-public-server).
This approach uses [ngrok](https://ngrok.com/docs/getting-started/) to forward traffic and share your local Langflow server over the internet, without deploying to a cloud provider or exposing your network directly.

* To build and deploy a Langflow container that includes your flow files, see [Containerize a Langflow application](develop-application).
This approach bundles your flows and dependencies into a portable, reproducible Docker image for easy deployment across different environments.

* To deploy a Langflow server on a remote server with Docker and Caddy, see [Deploy Langflow on a remote server](/deployment-caddyfile).
This approach is good for hosting your own Langflow instance on a remote server with secure web access, using Docker containers and Caddy as a reverse proxy for HTTPS support.

* To deploy Langflow with Nginx and automatic SSL certificates, see [Deploy Langflow with Nginx and Let's Encrypt](/deployment-nginx-ssl).
This approach uses Nginx as a reverse proxy with Let's Encrypt for automatic HTTPS certificate management to provide a secure deployment _without_ Docker.

* To deploy Langflow on Kubernetes, see [Langflow Kubernetes architecture and best practices](/deployment-prod-best-practices)
This approach creates production-grade deployments with high availability, scalability, and robust orchestration.

* For cloud provider-specific deployment guides, see your cloud provider's documentation.
The Langflow documentation provides a few examples, such as [Google Cloud Platform](/deployment-gcp) and [Hugging Face Spaces](/deployment-hugging-face-spaces), to help you get started.


================================================================================
filename: Deployment/deployment-prod-best-practices.mdx
================================================================================

---
title: Best practices for Langflow on Kubernetes
slug: /deployment-prod-best-practices
---

This guide provides best practices for deploying Langflow in production environments on Kubernetes.

## Resources and scaling

Langflow's minimum resource requirements vary by deployment type:

* **IDE (development)**: Deploy both the Langflow visual editor (frontend) and API (backend). Typically, this is used for development environments where developers use the visual editor to create and manage flows before packaging and serving them through a production runtime deployment.

    The frontend service requires a minimum of 512Mi RAM and 0.3 CPU per instance with 1 replica.

    The backend service requires a minimum of 1Gi RAM and 0.5 CPU per instance with 1 replica.

* **Runtime (production)**: Deploy the Langflow runtime for production flows, which is headless (backend only) service focused on serving the Langflow API. This is used for production environments where flows are executed programmatically without the need for the visual editor.

    Minimum requirements include 2Gi RAM and 1000m (1 CPU) per instance with 3 replicas.

For more information about Langflow deployment types, see [Langflow architecture on Kubernetes](/deployment-architecture).

### Estimate, test, and adjust

Start with the minimum recommended resources and replicas, then monitor and scale as needed based on your deployment's requirements and performance testing.
Consider the following factors in your resource estimation and performance testing:

* Flow complexity.
* Volume of concurrent users and requests.

   For IDE (development) deployments, consider that frontend activity also pings the backend service, so you typically need to scale both the frontend and backend together.

* Request payload content and size, particularly for file uploads in production deployments.
* Storage requirements for cache, file management, and the Langflow database.

    An [external PostgreSQL database](#use-an-external-postgresql-database) is recommended for production deployments.

* Infrastructure options that might require more resources, such as multi-core CPUs.

### Use an external PostgreSQL database

An external PostgreSQL database is recommended for production deployments to improve scalability and reliability as compared to the default SQLite database.

Your resource allocation and replication strategy must be able to support the PostgreSQL service and storage.
For example, for a runtime (production) deployment, you might allocate 4Gi RAM, 2 CPU, and multiple replicas for high availability.
Tune PostgreSQL parameters, such as `work_mem` and `shared_buffers`, as needed based on resource requirements and usage metrics.

Recommended configurations include:

* Persistent storage to prevent data loss on container shutdown
* High Availability (HA) or Active-Active for automatic failover, scaling, and load balancing
* Shared databases for multi-instance deployments
* Shared storage, such as NFS or cloud storage, for multi-instance deployments to access large files stored on disk, such as in `/opt/langflow/data/`.

For more information, see [Configure an external PostgreSQL database](/configuration-custom-database) and [Langflow database guide for enterprise DBAs](/enterprise-database-guide).

### Use HPA for dynamic scaling

Load balancing and dynamic scaling are recommended for runtime (production) deployments.

For example, consider using Horizontal Pod Autoscaler (HPA) in Kubernetes to dynamically scale based on CPU or memory usage.
The following example shows a Langflow HPA configuration with CPU-based scaling:

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: langflow-runtime-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: langflow-runtime
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 80
```

## Failure points

Langflow's reliability in production depends on mitigating key failure points, particularly around the database, file system, and instance availability:

* **Database failures**: See [Langflow database guide for enterprise DBAs](/enterprise-database-guide).
* **File system failures**: Concurrency issues in file caching, such as `/app/data/.cache`, can cause IO errors in multi-instance setups.
To avoid this, use a shared, POSIX-compliant file system or cloud storage.
Use persistent volumes instead of ramdisk solutions that result in data loss on container shutdown.
* **Instance failures**: Deploy multiple replicas to avoid service disruptions when a single instance fails. Use health checks to detect and replace failed pods.
* **Network and dependency failures**: External APIs or services used in flows may fail, causing flow errors. Implement retry logic and error handling in flows or application code. Monitor network latency and dependency health.

## Monitoring

Effective monitoring ensures Langflow operates reliably and performs well under varying loads:

* **Database monitoring**: See [Langflow database guide for enterprise DBAs](/enterprise-database-guide).
* **Application logs**: Collect and analyze logs for errors, warnings, and flow execution issues. Centralize logs using tools like ELK Stack or Fluentd. You can also inspect [Langflow logs](/logging).
* **Resource usage**: Track CPU, memory, and disk usage of Langflow instances. Use Prometheus and Grafana for real-time metrics collection and monitoring in Kubernetes.

    To expose your Langflow server's Prometheus metrics, set `LANGFLOW_PROMETHEUS_ENABLED=True` (the default is `false`).
    The default port for the Prometheus metrics is 9090.
    To change the port, set `LANGFLOW_PROMETHEUS_PORT`.

* **API performance**: Monitor response times, error rates, and request throughput. Set alerts for high latency or error spikes.
* **Observability tools**: Integrate with [LangWatch](/integrations-langwatch) or [Opik](/integrations-opik) for detailed flow tracing and metrics. Use these tools to debug flow performance and optimize execution.

## Security

Running Langflow in production requires robust security measures to protect the application, data, and users.
Follow industry best practices and use secure Langflow configurations, such as the following:

* **Container security**: Apply security best practices for containerized applications. For example, set `readOnlyRootFilesystem: true` in runtime (production) containers to prevent unauthorized modifications. Restrict access to files and codebases containing sensitive data and configuration files that shouldn't be exposed to unauthorized users.
* **Secrets management**: Store sensitive data like API keys and PostgreSQL credentials in Kubernetes secrets or external secrets managers like HashiCorp Vault.
* **Authentication, authorization, and access control**: Start your Langflow server with authentication enabled, as explained in [API keys and authentication](/api-keys-and-authentication). Restrict network and resource access with firewalls, network policies, network security groups, or VPCs. For example, restrict PostgreSQL database access to Langflow instances.
* **Encryption and privacy**: Follow industry best practices and legal requirements for data privacy and encryption of data in transit and at rest, including GDPR requirements, HTTPS, TLS, and SSL. For example, configure PostgreSQL with valid SSL certificates and append `?sslmode=require` or `?sslmode=verify-full` to the connection string to enable SSL for database connections.
* **Security posture maintenance**: Conduct regular security audits, keep current with software updates, and monitor for suspicious activity using intrusion detection systems.

## See also

* [Deploy the Langflow production environment on Kubernetes](/deployment-kubernetes-prod)
* [Langflow Helm Charts repository](https://github.com/langflow-ai/langflow-helm-charts)
* [Langflow environment variables](/environment-variables)


================================================================================
filename: Deployment/deployment-public-server.mdx
================================================================================

---
title: Deploy a public Langflow server
slug: /deployment-public-server
---

By default, your Langflow server at `http://localhost:7860` isn't exposed to the public internet.
However, you can forward Langflow server traffic with a forwarding platform like [ngrok](https://ngrok.com/docs/getting-started/) or [zrok](https://docs.zrok.io/docs/getting-started) to make your server public.

When your Langflow server is public, you can do things like [deploy your Langflow MCP server externally](#deploy-your-mcp-server-externally), [serve API requests](#serve-api-requests), and [share a flow's **Playground** publicly](#share-a-flows-playground).

## Prerequisites

On the machine where you plan to host your Langflow installation, [install Langflow](/get-started-installation) and a reverse proxy or forwarding service.

This guide uses ngrok, but you can use any similar reverse proxy or forwarding platform.

If you want to follow along with this guide, [install ngrok](https://ngrok.com/docs/getting-started/#1-install-ngrok) and [create an ngrok authtoken](https://dashboard.ngrok.com/get-started/your-authtoken).

## Expose your Langflow server with ngrok

1. Start Langflow:

    ```bash
    uv run langflow run
    ```

2. In another terminal window, use your ngrok authtoken to authenticate your local ngrok server:

    ```bash
    ngrok config add-authtoken NGROK_AUTHTOKEN
    ```

3. Use ngrok to expose your Langflow server to the public internet:

    ```bash
    ngrok http http://localhost:7860
    ```

    This example assumes that you use the default Langflow listening address at `http://localhost:7860`. If you have a different listening address, you must modify this command accordingly.

    The ngrok session starts in your terminal and deploys an ephemeral domain with no authentication.
    To add authentication or deploy a static domain, see the [ngrok documentation](https://ngrok.com/docs/).

    The `Forwarding` line prints the forwarding address for your Langflow server:

    ```
    Forwarding https://94b1-76-64-171-14.ngrok-free.app -> http://localhost:7860
    ```

    The forwarding address acts as a reverse proxy for your Langflow server, and ngrok forwards your local traffic to this domain.

4. To verify that your Langflow server is publicly available, navigate to the forwarding address URL, such as `https://94b1-76-64-171-14.ngrok-free.app`.

## Use a public Langflow server

When your Langflow server is public, you can do things like [deploy your Langflow MCP server externally](#deploy-your-mcp-server-externally), [serve API requests](#serve-api-requests), and [share a flow's **Playground** publicly](#share-a-flows-playground).

### Deploy your MCP server externally

After you deploy a public Langflow server, you can also access your Langflow projects' MCP servers publicly.

To do this, use your server's forwarding address when you [connect a client to a Langflow MCP server](/mcp-server#connect-clients-to-use-the-servers-actions).

### Serve API requests

To send requests to a public Langflow server's [Langflow API](/api-reference-api-examples) endpoints, use the server's domain as the [base URL](/api-reference-api-examples#base-url) for your API requests.
For example:

```bash
curl -X POST \
  "PUBLIC_SERVER_DOMAIN/api/v1/webhook/FLOW_ID" \
  -H "Content-Type: application/json" \
  -H "x-api-key: LANGFLOW_API_KEY" \
  -d '{"data": "example-data"}'
```

:::tip
When you create flows on public Langflow servers, the code snippets generated in the [**API access** pane](/concepts-publish) automatically use your public server's domain.
:::

You also use your public domain when making Langflow API calls in scripts, including the code snippets that are automatically generated by Langflow.
For example, the following code snippet calls an ngrok domain to trigger the specified flow (`d764c4b8...`):

    ```python
    import requests

    url = "https://3f7c-73-64-93-151.ngrok-free.app/api/v1/run/d764c4b8-5cec-4c0f-9de0-4b419b11901a"  # The complete API endpoint URL for this flow

    # Request payload configuration
    payload = {
        "output_type": "chat",
        "input_type": "chat",
        "input_value": "Hello"
    }

    # Request headers
    headers = {
        "Content-Type": "application/json",
        "x-api-key": "LANGFLOW_API_KEY"
    }

    try:
        # Send API request
        response = requests.request("POST", url, json=payload, headers=headers)
        response.raise_for_status()  # Raise exception for bad status codes

        # Print response
        print(response.text)

    except requests.exceptions.RequestException as e:
        print(f"Error making API request: {e}")
    except ValueError as e:
        print(f"Error parsing response: {e}")
    ```

For a demo of the Langflow API in a script, see the [Quickstart](/get-started-quickstart).

### Share a flow's Playground

After you deploy a public Langflow server, you can use the **Shareable Playground** option to make a flow's **Playground** available at a public URL.
If a user accesses this URL, they can interact with the flow's chat input and output and view the results without installing Langflow or generating a Langflow API key.

For more information, see [Share a flow's Playground](/concepts-playground#share-a-flows-playground).


================================================================================
filename: Deployment/deployment-railway.mdx
================================================================================

---
title: Deploy Langflow on Railway
description: Deploy Langflow to Railway using a one-click template
slug: /deployment-railway
---

This guide explains how to [deploy Langflow on Railway](https://railway.com/?utm_medium=integration&utm_source=docs&utm_campaign=langflow), a cloud infrastructure platform that provides auto-deploy, managed databases, and automatic scaling.

1. Create a Railway account.

   A Hobby account on Railway is sufficient for Langflow's dual-core CPU and 2 GB RAM requirements. For more information, see [Railway pricing](https://railway.com/pricing?utm_medium=integration&utm_source=docs&utm_campaign=langflow).

2. Follow this link to deploy the Langflow template on Railway:

   [![Deploy on Railway](/logos/railway-deploy.svg)](https://railway.com/new/template/JMXEWp?referralCode=MnPSdg&utm_medium=integration&utm_source=docs&utm_campaign=langflow)

3. Optional: Add any custom configuration for your Langflow deployment.

   The Langflow Railway template automatically sets up the infrastructure, deploys Langflow, and then starts the application.

4. Wait for the deployment to complete.

5. Navigate to your Langflow instance at your deployment's public URL, such as `https://APP-NAME.up.railway.app`.


================================================================================
filename: Deployment/deployment-render.mdx
================================================================================

---
title: Deploy Langflow on Render
slug: /deployment-render
---

This guide explains how to deploy Langflow on [Render](https://render.com/), a cloud platform for deploying web applications and APIs.

1. Prepare a Render instance that can support Langflow.

   Langflow requires at least 2 GB of RAM to run, so you must use a Render instance type of **Standard** or better.
   This requires a paid Render account.
   For more information, see [Render Web Services](https://render.com/docs/web-services) and [Render pricing](https://render.com/pricing).

2. Follow this link to start a Langflow deployment on Render:

   [![Deploy to Render](/logos/render-deploy.svg)](https://render.com/deploy?repo=https%3A%2F%2Fgithub.com%2Flangflow-ai%2Flangflow%2Ftree%2Fdev)

3. Enter a blueprint name, select the branch for your `render.yaml` file, and then click **Deploy Blueprint**.

When deployment is complete, your Langflow instance is ready to use.


================================================================================
filename: Deployment/develop-application.mdx
================================================================================

---
title: Containerize a Langflow application
slug: /develop-application
---

import Icon from "@site/src/components/icon";
import PartialPodmanAlt from '@site/docs/_partial-podman-alt.mdx';

<PartialPodmanAlt />

Designing flows in the visual editor is only the first step in building an application that uses Langflow.

Once you have a functional flow, you can use that flow in a larger application, such as a website or mobile app.
Because Langflow is both an IDE and a runtime, you can use Langflow to build and test your flows locally, and then package and serve your flows in a production environment.

This guide introduces application development with Langflow from initial setup through packaging and deployment.
This documentation doesn't explain how to write a complete application; it only describes how to include Langflow in the context of a larger application.

## Directory structure

The following example describes the directory structure for a minimal Langflow application:

```text
LANGFLOW-APPLICATION/
â”œâ”€â”€ docker.env
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ flows/
â”‚   â”œâ”€â”€ flow1.json
â”‚   â””â”€â”€ flow2.json
â”œâ”€â”€ langflow-config-dir/
â”œâ”€â”€ README.md
```

This directory contains the following:

* [`docker.env`](#docker-env): This file is copied to the Docker image as a `.env` file in the container root.
* [`Dockerfile`](#dockerfile): This file controls how your Langflow image is built.
* [`/flows`](#flows): This folder holds the flows you want to host, which are the flows that your application uses.
* `/langflow-config-dir`: This folder is referenced in the Dockerfile as the location for your Langflow deployment's configuration files, database, and logs.
* `README.md`: This is a typical README file for your application's documentation.

This is a minimal example of a Langflow application directory.
Your application might have additional files and folders, such as a `/components` folder for custom components, or a `pyproject.toml` file for additional dependencies.

### Package management

The base Langflow Docker image includes the Langflow core dependencies because it uses `langflowai/langflow:latest` as the parent image.

If your application requires additional dependencies, create a [`pyproject.toml`](https://packaging.python.org/en/latest/guides/writing-pyproject-toml) file for the additional dependencies.
For more information, see [Install custom dependencies](/install-custom-dependencies).

To deploy an application with additional dependencies to Docker, you must copy the `pyproject.toml` and `uv.lock` files to the Docker image.
To do this, add the following to your Langflow application's Dockerfile:

```text
COPY pyproject.toml uv.lock /app/
```

### Environment variables {#docker-env}

The `docker.env` file is a `.env` file loaded into your Docker image.
It contains [Langflow environment variables](/environment-variables) that are used in flows or control Langflow's behavior, such as authentication, database storage, API keys, and server configurations.
For example:

```text
LANGFLOW_AUTO_LOGIN=True
LANGFLOW_SAVE_DB_IN_CONFIG_DIR=True
LANGFLOW_BASE_URL=http://0.0.0.0:7860
OPENAI_API_KEY=sk-...
```

You can set environment variables in the Dockerfile as well.
However, if you set an environment variable in both `docker.env` and the Dockerfile, Langflow uses the value set in `docker.env`.

Langflow can also [create global variables from your environment variables](/configuration-global-variables#add-custom-global-variables-from-the-environment), or [use environment variables as a backup for missing global variables](/configuration-global-variables#use-environment-variables-for-missing-global-variables).

### Secrets

For simplicity, the examples in the Langflow documentation might use direct references to API keys and other sensitive values.
In your own applications, you should always follow industry best practices for managing secrets, such as using environment variables or secret management tools.

For information about generating authentication keys and managing secrets in Langflow, see [API keys and authentication](/api-keys-and-authentication).

### Storage

By default, Langflow uses an [SQLite](https://www.sqlite.org/) database for storage.
If you prefer to use PostgreSQL, see [Configure an external PostgreSQL database](/configuration-custom-database).

For more information about storage, including cache and memory, see [Memory management options](/memory).

### Flows {#flows}

Your local Langflow instance might have many flows for different applications.
When you package Langflow as a dependency of an application, you only want to include the flows your application uses.

1. [Export flows](/concepts-flows-import) that are relevant to your application.

    If you have chained flows (flows that trigger other flows), make sure you export _all_ necessary flows.

2. Add the exported Langflow JSON files to the `/flows` folder in your application directory.

### Components

The <Icon name="Component" aria-hidden="true" /> **Core components** and <Icon name="Blocks" aria-hidden="true" /> [**Bundles**] that you see in the Langflow visual editor are automatically included in the base Langflow Docker image.

If you have any [custom components](/components-custom-components) that you created for your application, you must include these components in your application directory:

1. Create a `/components` folder in your application directory.
2. Add your custom component files to the `/components` folder.
3. Specify the path to `/components` in your `docker.env`.

## Langflow Dockerfile {#dockerfile}

The Dockerfile determines how your Langflow image is built, including the dependencies, flows, components, and configuration files.

At minimum, you need to specify the base Langflow image, create the necessary folders in the container, copy folders and files to the container, and provide a startup command.

```dockerfile
# Use the latest version of the base Langflow image
FROM langflowai/langflow:latest

# Create folders and set the working directory in the container
RUN mkdir /app/flows
RUN mkdir /app/langflow-config-dir
WORKDIR /app

# Copy flows, langflow-config-dir, and docker.env to the container
COPY flows /app/flows
COPY langflow-config-dir /app/langflow-config-dir
COPY docker.env /app/.env

# Optional: Copy custom components to the container
COPY components /app/components

# Optional: Use custom dependencies
COPY pyproject.toml uv.lock /app/

# Set environment variables if not set in docker.env
ENV PYTHONPATH=/app
ENV LANGFLOW_LOAD_FLOWS_PATH=/app/flows
ENV LANGFLOW_CONFIG_DIR=/app/langflow-config-dir
ENV LANGFLOW_COMPONENTS_PATH=/app/components
ENV LANGFLOW_LOG_ENV=container

# Command to run the Langflow server on port 7860
EXPOSE 7860
CMD ["langflow", "run", "--backend-only", "--env-file","/app/.env","--host", "0.0.0.0", "--port", "7860"]
```

The environment variables set directly in this Dockerfile specify resource paths for Langflow.
If these variables are also set in `docker.env`, the values in `docker.env` override the values set in the Dockerfile.

In this example, `ENV LANGFLOW_LOG_ENV=container` sets the logging behavior for serialized JSON to `stdout` to track the application's behavior in a containerized environment. For more information, see [Logging](/logging).

### Backend-only mode

The `--backend-only` flag in `CMD` starts Langflow in backend-only mode, which provides programmatic access only.
This is recommended when running Langflow as a dependency of an application where you don't need access to the visual editor.

If you want to serve the Langflow visual editor _and_ backend, then omit `--backend-only`.

For more information, see [Deploy Langflow on Docker](/deployment-docker).

## Test your Langflow Docker image

Build and run your Langflow Docker image to test it.

This example runs the container locally.
For information about publishing your image on Docker Hub and running a Langflow container remotely, see [Deploy to Docker Hub and Kubernetes](#deploy-docker).

1. Build the Docker image:

    ```bash
    docker build -t langflow-pokedex:1.2.0 .
    ```

2. Run the Docker container to start your Langflow server:

    ```bash
    docker run -p 7860:7860 langflow-pokedex:1.2.0
    ```

3. To confirm that the container is serving your flows as expected, use the Langflow API to run a flow:

    1. Open one of the JSON files in your application's `/flows` folder, and then find the flow's `id` in the  [additional metadata and project information](/concepts-flows-import#additional-metadata-and-project-information).

        There are many `id` values; make sure you get the ID for the entire flow, not the ID for an individual component.
        If your flow is complex, try searching for the flow's name, which is typically near the flow's `id`.

        ```json
        "name": "Basic Prompting",
        "description": "Perform basic prompting with an OpenAI model.",
        "id": "e4167236-938f-4aca-845b-21de3f399858",
        ```

    2. Send a POST request to the [`/v1/run/$FLOW_ID`](/api-flows-run#run-flow) endpoint using the flow ID from the previous step .

        The following example runs a simple LLM chat flow that responds to a chat input string.
        If necessary, modify the payload for your flow.
        For example, if your flow doesn't have a **Chat Input** component, you must modify the payload to provide the expected input for your flow.

        ```bash
        curl --request POST \
          --url 'http://localhost:7860/api/v1/run/e4167236-938f-4aca-845b-21de3f399858?stream=true' \
          --header 'Content-Type: application/json' \
          --data '{
            "input_value": "Tell me about Charizard.",
            "output_type": "chat",
            "input_type": "chat",
            "session_id": "charizard_test_request"
        }'
        ```

        <details>
        <summary>About this example</summary>

        This command runs the **PokÃ©dex** template flow.
        It provides chat input about a specific PokÃ©mon, uses an optional custom `session_id`, and enables response streaming with `?stream=true`.

        The default [session ID](/session-id) is the flow ID.
        Custom session IDs can help isolate unique conversation threads to keep the LLM's context clean, and they can help identify specific conversations in flow logs to make debugging easier.

        This command uses response streaming because the PokÃ©dex flow can return a large amount of text.
        To use batching, set `?stream=false`.

        </details>

      3. Verify that the request succeeds and the response is valid, depending on the specific flow you ran.

          This confirms that your Langflow Docker image is correctly configured and this flow is accessible through the Langflow API server that is hosted on the container.
          When you build and test your entire application stack, your front-end application can use Langflow API requests to trigger the flows served by your Langflow container in the same way you manually tested the flow in the previous step.

          This example triggered a flow by sending chat input to the `/v1/run/$FLOW_ID` endpoint.
          For more examples of flow triggers, see [Trigger flows with webhooks](/webhook) and the tutorial to [Create a chatbot that can ingest files](/chat-with-files).

## Deploy to Docker Hub and Kubernetes {#deploy-docker}

When you're ready to share your application with the world, you need to serve Langflow in a production environment.
For more information about deploying Langflow, see the following:

* [Learn about Langflow deployments](/deployment-overview)
* [Deploy Langflow on Docker](/deployment-docker)
* [Deploy the Langflow production environment on Kubernetes](/deployment-kubernetes-prod)


================================================================================
filename: Develop/api-keys-and-authentication.mdx
================================================================================

---
title: API keys and authentication
slug: /api-keys-and-authentication
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import Icon from "@site/src/components/icon";

:::warning
Never expose Langflow ports directly to the internet without proper security measures.
Set `LANGFLOW_AUTO_LOGIN=False`, use a non-default `LANGFLOW_SECRET_KEY`, and deploy your Langflow server behind a reverse proxy with authentication enabled.
For more information, see [Start a Langflow server with authentication enabled](#start-a-langflow-server-with-authentication-enabled).
:::

Authentication credentials help prevent unauthorized access to your Langflow server, flows, and services connected through components.

There are three types of credentials that you use in Langflow:

* [Langflow API keys](#langflow-api-keys): For authentication with the Langflow API and authorizing server-side Langflow actions like running flows and uploading files.
* [Component API keys](#component-api-keys): For authentication between Langflow and a service connected through a component, such as a model provider or third-party API.
* [Authentication environment variables](#authentication-environment-variables): These environment variables configure how Langflow handles user authentication and authorization.

## Langflow API keys {#langflow-api-keys}

You can use Langflow API keys to interact with Langflow programmatically.

By default, most Langflow API endpoints, such as `/v1/run/$FLOW_ID`, require authentication with a Langflow API key.

To configure authentication for Langflow MCP servers, see [Use Langflow as an MCP server](/mcp-server).

### Langflow API key permissions

A Langflow API key adopts the privileges of the user who created it.
This means that API keys you create have the same permissions and access that you do, including access to your flows, components, and Langflow database.
A Langflow API key cannot be used to access resources outside of your own Langflow server.

In single-user environments, you are always a superuser, and your Langflow API keys always have superuser privileges.

In multi-user environments, users who aren't superusers cannot use their API keys to access other users' resources.
You must [start your Langflow server with authentication enabled](#start-a-langflow-server-with-authentication-enabled) to allow user management and creation of non-superuser accounts.

### Create a Langflow API key

You can generate a Langflow API key in your Langflow **Settings** or with the Langflow CLI.

The CLI option is required if your Langflow server is running in `--backend-only` mode.

<Tabs>
<TabItem value="settings" label="Langflow Settings" default>

1. In the Langflow header, click your profile icon, and then select **Settings**.
2. Click **Langflow API Keys**, and then click **Add New**.
3. Name your key, and then click **Create API Key**.
4. Copy the API key and store it securely.

</TabItem>
<TabItem value="Langflow CLI" label="Langflow CLI">

If you're serving your flow with `--backend-only=true`, you can't create API keys in your Langflow **Settings** because the frontend isn't running.
In this case, you must create API keys with the Langflow CLI.

1. Recommended: [Start your Langflow server with authentication enabled](#start-a-langflow-server-with-authentication-enabled).

    The Langflow team recommends enabling authentication for security reasons to prevent unauthorized creation of API keys and superusers, especially in production environments.
    If authentication isn't enabled (`LANGFLOW_AUTO_LOGIN=True`), all users are effectively superusers, and they can create API keys with the Langflow CLI.

2. Create an API key with [`langflow api-key`](/configuration-cli#langflow-api-key):

    ```shell
    uv run langflow api-key
    ```

    All API keys created with the Langflow CLI have superuser privileges because the command requires superuser authentication, and Langflow API keys adopt the privileges of the user who created them.

</TabItem>
</Tabs>

### Use a Langflow API key

To authenticate Langflow API requests, pass your Langflow API key an `x-api-key` header or query parameter.

<Tabs>
<TabItem value="header" label="HTTP header" default>

```shell
curl -X POST \
  "http://$LANGFLOW_SERVER_ADDRESS/api/v1/run/$FLOW_ID?stream=false" \
  -H "Content-Type: application/json" \
  -H "x-api-key: $LANGFLOW_API_KEY" \
  -d '{"inputs": {"text":""}, "tweaks": {}}'
```

</TabItem>
<TabItem value="parameter" label="Query parameter">

```shell
curl -X POST \
  "http://$LANGFLOW_SERVER_ADDRESS/api/v1/run/$FLOW_ID?x-api-key=$LANGFLOW_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{"inputs": {"text":""}, "tweaks": {}}'
```

</TabItem>
</Tabs>

For more information about forming Langflow API requests, see [Get started with the Langflow API](/api-reference-api-examples) and [Trigger flows with the Langflow API](/concepts-publish).

### Track API key usage

By default, Langflow tracks API key usage through `total_uses` and `last_used_at` records in your [Langflow database](/memory).

To disable API key tracking, set `LANGFLOW_DISABLE_TRACK_APIKEY_USAGE=True` in your [Langflow environment variables](/environment-variables).
This can help avoid database contention during periods of high concurrency.

### Revoke an API key

To revoke and delete an API key, do the following:

1. In the Langflow header, click your profile icon, and then select **Settings**.
2. Click **Langflow API Keys**.
3. Select the keys you want to delete, and then click <Icon name="Trash2" aria-hidden="true"/> **Delete**.

This action immediately invalidates the key and prevents it from being used again.

## Component API keys {#component-api-keys}

Component API keys authorize access to external services that are called by components in your flows, such as model providers, databases, or third-party APIs.
These aren't Langflow API keys or general application credentials.

In Langflow, you can store component API keys in global variables in your **Settings** or import them from your runtime environment.
For more information, see [Global variables](/configuration-global-variables).

You create and manage component API keys within the service provider's platform.
Langflow only stores the encrypted key value or a secure reference to a key stored elsewhere; it doesn't manage the actual credentials at the source.
This means that deleting a global variable from Langflow doesn't delete or invalidate the actual API key in the service provider's system.
You must delete or rotate component API keys directly using the service provider's interface or API.

For added security, you can set `LANGFLOW_REMOVE_API_KEYS=True` to omit API keys and tokens from flow data in your [Langflow database](/memory).
Additionally, when [exporting flows](/concepts-flows-import), you can choose to omit API keys from the exported flow JSON.

## Authentication environment variables

This section describes the available authentication configuration variables.

You can use the [`.env.example`](https://github.com/langflow-ai/langflow/blob/main/.env.example) file in the Langflow repository as a template for your own `.env` file.

### LANGFLOW_AUTO_LOGIN {#langflow-auto-login}

This variable controls whether authentication is required to access your Langflow server, including the visual editor, API, and Langflow CLI:

* If `LANGFLOW_AUTO_LOGIN=False`, automatic login is disabled. Users must sign in to the visual editor, authenticate as a superuser to run certain Langflow CLI commands, and use a Langflow API key for Langflow API requests.
If `false`, the Langflow team recommends that you also explicitly set [`LANGFLOW_SUPERUSER` and `LANGFLOW_SUPERUSER_PASSWORD`](#langflow-superuser) to avoid using the insecure default values.

* If `LANGFLOW_AUTO_LOGIN=True` (default), all API requests require authentication with a Langflow API key, but the visual editor automatically signs in all users as superusers, and Langflow uses _only_ the default [superuser credentials](/api-keys-and-authentication#langflow-superuser).
All users access the same visual editor environment without password protection, they can run all Langflow CLI commands as superusers, and Langflow automatically authenticates internal requests between the backend and frontend based on the users' superuser privileges.
If you also want to bypass authentication for Langflow API requests in addition to other bypassed authentication, see [`LANGFLOW_SKIP_AUTH_AUTO_LOGIN`](/api-keys-and-authentication#langflow-skip-auth-auto-login).

Langflow doesn't allow users to simultaneously edit the same flow in real time.
If two users edit the same flow, Langflow saves only the work of the most recent editor based on the state of that user's [workspace](/concepts-overview#workspace). Any changes made by the other user in the interim are overwritten.

#### Default authentication enforcement and LANGFLOW_SKIP_AUTH_AUTO_LOGIN {#langflow-skip-auth-auto-login}

In Langflow version 1.6, the default settings are `LANGFLOW_AUTO_LOGIN=True` and `LANGFLOW_SKIP_AUTH_AUTO_LOGIN=False`.
This enforces authentication for API requests only, as explained in the preceding section.

For temporary backwards compatibility, you can revert to the fully unauthenticated behavior from earlier versions by setting both variables to `true`.
However, a future release will set `LANGFLOW_AUTO_LOGIN=False` and remove `LANGFLOW_SKIP_AUTH_AUTO_LOGIN`.
At that point, Langflow will strictly enforce API key authentication for API requests, and you can manually disable authentication for some features, like the visual editor, by setting `LANGFLOW_AUTO_LOGIN=True`.

<details>
<summary>Authentication enforcement in earlier versions</summary>

Langflow version 1.5 was the first version that could enforce authentication for Langflow API requests, regardless of the value of `LANGFLOW_AUTO_LOGIN`.
As a temporary bypass for backwards compatibility, this version added the `LANGFLOW_SKIP_AUTH_AUTO_LOGIN` environment variable and set both variables to `true` by default to preserve the fully unauthenticated behavior from earlier versions.
This allowed users to upgrade to version 1.5 with no change in the authentication behavior.

In Langflow versions earlier than 1.5, Langflow API requests didn't require authentication.
Additionally, the default setting of `LANGFLOW_AUTO_LOGIN=True` automatically granted all users superuser privileges in the visual editor, and it allowed all users to run all Langflow CLI commands as superusers.
</details>

### LANGFLOW_ENABLE_SUPERUSER_CLI {#langflow-enable-superuser-cli}

Controls the availability of the `langflow superuser` command in the Langflow CLI.
The default is `true`, but `false` is recommended to prevent unrestricted superuser creation.
For more information, see [`langflow superuser`](/configuration-cli#langflow-superuser).

### LANGFLOW_SUPERUSER and LANGFLOW_SUPERUSER_PASSWORD {#langflow-superuser}

These variables specify the username and password for the Langflow server's superuser.

```text
LANGFLOW_SUPERUSER=administrator
LANGFLOW_SUPERUSER_PASSWORD=securepassword
```

They are required if `LANGFLOW_AUTO_LOGIN=False`.
Otherwise, they aren't relevant.

When you [start a Langflow server with authentication enabled](#start-a-langflow-server-with-authentication-enabled), if these variables are required but _not_ set, then Langflow uses the default values of `langflow` and `langflow`.
These defaults don't apply when using the Langflow CLI command [`langflow superuser`](/configuration-cli#langflow-superuser).

### LANGFLOW_SECRET_KEY {#langflow-secret-key}

This environment variable stores a secret key used for encrypting sensitive data like API keys.
Langflow uses the [Fernet](https://pypi.org/project/cryptography/) library for secret key encryption.

If no secret key is provided, Langflow automatically generates one.

However, you should generate and explicitly set your own key in production environments.
This is particularly important for multi-instance deployments like Kubernetes to ensure consistent encryption across instances.

To generate a secret encryption key for `LANGFLOW_SECRET_KEY`, do the following:

1. Run the command to generate and copy a secret to the clipboard.

    <Tabs>
    <TabItem value="unix" label="macOS or Linux">

    * **macOS**: Generate a secret key and copy it to the clipboard:

        ```bash
        python3 -c "from secrets import token_urlsafe; print(f'LANGFLOW_SECRET_KEY={token_urlsafe(32)}')" | pbcopy
        ```

    * **Linux**: Generate a secret key and copy it to the clipboard:

        ```bash
        python3 -c "from secrets import token_urlsafe; print(f'LANGFLOW_SECRET_KEY={token_urlsafe(32)}')" | xclip -selection clipboard
        ```

    * **Unix**: Generate a secret key and print it to the terminal to manually copy it:

        ```bash
        python3 -c "from secrets import token_urlsafe; print(f'LANGFLOW_SECRET_KEY={token_urlsafe(32)}')"
        ```

    </TabItem>
    <TabItem value="windows" label="Windows">

    * Generate a secret key and copy it to the clipboard:

        ```bash
        python -c "from secrets import token_urlsafe; print(f'LANGFLOW_SECRET_KEY={token_urlsafe(32)}')"
        ```

    * Generate a secret key and print it to the terminal to manually copy it:

        ```bash

        # Or just print
        python -c "from secrets import token_urlsafe; print(f'LANGFLOW_SECRET_KEY={token_urlsafe(32)}')"
        ```

    </TabItem>
    </Tabs>

2. Paste the value into your `.env` file:

    ```text
    LANGFLOW_SECRET_KEY=dBuu...2kM2_fb
    ```

    If you're running Langflow on Docker, reference the `LANGFLOW_SECRET_KEY` from your `.env` file in the `docker-compose.yml` file like this:

        ```yaml
        environment:
          - LANGFLOW_SECRET_KEY=${LANGFLOW_SECRET_KEY}
        ```

### LANGFLOW_NEW_USER_IS_ACTIVE {#langflow-new-user-is-active}

When `LANGFLOW_NEW_USER_IS_ACTIVE=False` (default), a superuser must explicitly activate a new user's account before they can sign in to the visual editor.
The superuser can also deactivate a user's account as needed.

When `LANGFLOW_NEW_USER_IS_ACTIVE=True`, new user accounts are automatically activated.

```text
LANGFLOW_NEW_USER_IS_ACTIVE=False
```

Only superusers can manage user accounts for a Langflow server, but user management only matters if your server has authentication enabled.
For more information, see [Start a Langflow server with authentication enabled](#start-a-langflow-server-with-authentication-enabled).

### LANGFLOW_CORS_* {#cors-configuration-for-authentication}

Cross-Origin Resource Sharing (CORS) configuration controls how authentication credentials are handled when your Langflow frontend and backend are served from different origins.
The following `LANGFLOW_CORS_*` environment variables are available:

| Variable | Format | Default | Description |
|----------|--------|---------|-------------|
| `LANGFLOW_CORS_ALLOW_CREDENTIALS` | Boolean | `True` | Whether to allow credentials, such as cookies and authorization headers, in CORS requests. |
| `LANGFLOW_CORS_ALLOW_HEADERS` | List[String] or String | `*` | The allowed headers for CORS requests. Provide a comma-separated list of headers or use `*` to allow all headers. |
| `LANGFLOW_CORS_ALLOW_METHODS` | List[String] or String | `*` | The allowed HTTP methods for CORS requests. Provide a comma-separated list of methods or use `*` to allow all methods. |
| `LANGFLOW_CORS_ORIGINS` | String | `*` | The allowed CORS origins. Provide a comma-separated list of origins or use `*` for all origins. |

The default configuration enables CORS credentials and uses wildcards (`*`) to allow all origins, headers, and methods:

```text
LANGFLOW_CORS_ORIGINS=*
LANGFLOW_CORS_ALLOW_CREDENTIALS=True
LANGFLOW_CORS_ALLOW_HEADERS=*
LANGFLOW_CORS_ALLOW_METHODS=*
```

:::danger
Langflow's default CORS settings can be a security risk in production environments because any website can make requests to your Langflow API, and any website can include credentials in cross-origin requests, including authentication cookies and authorization headers.

In production deployments, specify exact origins in `LANGFLOW_CORS_ORIGINS`.
You can also specify allowed headers and methods, if needed.
For example:

```text
LANGFLOW_CORS_ORIGINS=https://yourdomain.com,https://app.yourdomain.com
LANGFLOW_CORS_ALLOW_CREDENTIALS=True
LANGFLOW_CORS_ALLOW_HEADERS=Content-Type,Authorization
LANGFLOW_CORS_ALLOW_METHODS=GET,POST,PUT
```
:::

## Start a Langflow server with authentication enabled

This section shows you how to use the [authentication environment variables](/api-keys-and-authentication#authentication-environment-variables) to deploy a Langflow server with authentication enabled.
This involves disabling automatic login, setting superuser credentials, generating a secret encryption key, and enabling user management.

This configuration is recommended for any deployment where Langflow is exposed to a shared or public network, or where multiple users access the same Langflow server.

With authentication enabled, all users must sign in to the visual editor with valid credentials, and API requests require authentication with a Langflow API key.
Additionally, you must sign in as a superuser to manage users and [create a Langflow API key](#create-a-langflow-api-key) with superuser privileges.

### Start the Langflow server

1. Create a `.env` file with the following variables:

    ```text
    LANGFLOW_AUTO_LOGIN=False
    LANGFLOW_SUPERUSER=
    LANGFLOW_SUPERUSER_PASSWORD=
    LANGFLOW_SECRET_KEY=
    LANGFLOW_NEW_USER_IS_ACTIVE=False
    LANGFLOW_ENABLE_SUPERUSER_CLI=False
    ```

    Your `.env` file can have other environment variables.
    This example focuses on authentication variables.

2. Set `LANGFLOW_SUPERUSER` and `LANGFLOW_SUPERUSER_PASSWORD` to your desired superuser credentials.

    For a one-time test, you can use basic credentials like `administrator` and `password`.
    Strong, securely-stored credentials are recommended in genuine development and production environments.

3. Recommended: Generate and set a `LANGFLOW_SECRET_KEY` for encrypting sensitive data.

    If you don't set a secret key, Langflow generates one automatically, but this isn't recommended for production environments.

    For instructions on generating at setting a secret key, see [`LANGFLOW_SECRET_KEY`](#langflow-secret-key).

4. Save your `.env` file with the populated variables. For example:

    ```text
    LANGFLOW_AUTO_LOGIN=False
    LANGFLOW_SUPERUSER=administrator
    LANGFLOW_SUPERUSER_PASSWORD=securepassword
    LANGFLOW_SECRET_KEY=dBuu...2kM2_fb
    LANGFLOW_NEW_USER_IS_ACTIVE=False
    LANGFLOW_ENABLE_SUPERUSER_CLI=False
    ```

5. Start Langflow with the configuration from your `.env` file:

    ```text
    uv run langflow run --env-file .env
    ```

    Starting Langflow with a `.env` file automatically authenticates you as the superuser set in `LANGFLOW_SUPERUSER` and `LANGFLOW_SUPERUSER_PASSWORD`.
    If you don't explicitly set these variables, the default values are `langflow` and `langflow` for system auto-login.

6. Verify the server is running. The default location is `http://localhost:7860`.

Next, you can add users to your Langflow server to collaborate with others on flows.

### Manage users as an administrator

1. To complete your first-time login as a superuser, go to `http://localhost:7860/login`.

    If you aren't using the default location, replace `localhost:7860` with your server's address.

2. Log in with the superuser credentials you set in your `.env` (`LANGFLOW_SUPERUSER` and `LANGFLOW_SUPERUSER_PASSWORD`).

3. To manage users on your server, navigate to `/admin`, such as `http://localhost:7860/admin`, click your profile icon, and then click **Admin Page**.

    As a superuser, you can add users, set permissions, reset passwords, and delete accounts.

4. To add a user, click **New User**, and then complete the user account form:

    1. Enter a username and password.
    2. To activate the account immediately, select **Active**. Inactive users cannot sign in or access flows they created before becoming inactive.
    3. Deselect **Superuser** if you don't want the user to have full administrative privileges.
    4. Click **Save**. The new user appears in the **Admin Page**.

5. To test the new user's access, sign out of Langflow, and then sign in with the new user's credentials.

    Try to access the `/admin` page.
    You are redirected to the `/flows` page if the new user isn't a superuser.

## See also

* [Langflow environment variables](/environment-variables)


================================================================================
filename: Develop/concepts-file-management.mdx
================================================================================

---
title: Manage files
slug: /concepts-file-management
---

import Icon from "@site/src/components/icon";

Each Langflow server has a file management system where you can store files that you want to use in your flows.

Files uploaded to Langflow file management are stored locally in your [Langflow configuration directory](/memory), and they are available to all of your flows.
Local storage is set by `LANGFLOW_STORAGE_TYPE`, which has only one allowed value (`local`).

Uploading files to Langflow file management keeps your files in a central location, and allows you to reuse files across flows without repeated manual uploads.

## Use the file management UI

You can use the file management UI to upload files from your local machine to your own Langflow server.
You can also manage all files that have been uploaded to your Langflow server.

1. Navigate to Langflow file management:

    * **Langflow Desktop**: In Langflow, on the [**Projects** page](/concepts-flows#projects) page, click **My Files** below the list of projects.
    * **Langflow OSS**: From a browser, navigate to your Langflow server's `/files` endpoint, such as `http://localhost:7860/files`. Modify the base URL as needed for your Langflow server.
    * **Backend-only**: For programmatic file management, use the [Langflow API files endpoints](/api-files). However, the following steps assume you're using the file management UI.

2. On the **My Files** page, click **Upload**.

3. Select one or more files to upload.

After uploading files, you can rename, download, copy, or delete files within the file management UI.
To delete a file, hover over a file's icon, select it, and then click <Icon name="Trash2" aria-hidden="true"/> **Delete**.
You can delete multiple files in a single action.
To download a file, hover over a file's icon, select it, and then click <Icon name="Download" aria-hidden="true"/> **Download**.
If you download multiple files in a single action, they are saved together in a zip file.

## Upload and manage files with the Langflow API

With the Langflow API, you can upload and manage files in Langflow file management, and you can send files to flows programmatically at runtime.

For more information and examples, see [Files endpoints](/api-files) and [Create a chatbot that can ingest files](/chat-with-files).

## Set the maximum file size

By default, the maximum file size is 1024 MB.
To modify this value, change the `LANGFLOW_MAX_FILE_SIZE_UPLOAD` [environment variable](/environment-variables).

## Use files in a flow

To use files in your Langflow file management system in a flow, add a component that accepts file input to your flow, such as the **File** component.

For example, add a **File** component to your flow, click **Select files**, and then select files from the **My Files** list.

This list includes all files in your server's file management system, but you can only select [file types that are supported by the **File** component](/components-data#file).
If you need another file type, you must use a different component that supports that file type, or you need to convert it to a supported type before uploading it.

For more information about the **File** component and other data loading components, see [Data components](/components-data).

### Load files at runtime

You can use preloaded files in your flows, and you can load files at runtime, if your flow accepts file input.
To enable file input in your flow, do the following:

1. Add a [**File** component](/components-data#file) to your flow.

2. Click **Share**, select **API access**, and then click **Input Schema** to add [`tweaks`](/concepts-publish#input-schema) to the request payload in the flow's automatically generated code snippets.

3. Expand the **File** section, find the **Files** row, and then enable **Expose Input** to allow the parameter to be set at runtime through the Langflow API.

4. Close the **Input Schema** pane to return to the **API access** pane.
The payload in each code snippet now includes `tweaks` with your **File** component's ID and the `path` key that you enabled in **Input Schema**:

    ```json
    "tweaks": {
	    "File-qYD5w": {
		    "path": []
	    }
	}
    ```

5. When you run this flow programmatically, your script must upload a file to Langflow file management, and then pass the returned `file_path` to the `path` tweak in the `/run` request:

    ```json
    "tweaks": {
        "FILE_COMPONENT_ID": {
            "path": [ "file_path" ]
        }
    }
    ```

    For a complete example see [Create a chatbot that can ingest files](/chat-with-files) and [Files endpoints](/api-files).

    If you want to upload multiple files, you can pass multiple `file_path` values in the `path` array, such as `[ "path1", "path2" ]`.

## Upload images

Langflow supports base64 images in the following formats:

* PNG
* JPG/JPEG
* GIF
* BMP
* WebP

You can upload images to the **Playground** chat interface and as runtime input with the Langflow API.

* In the **Playground**, you can drag-and-drop images into the chat input area, or you can click the **Attach image** icon to select an image to upload.

* When you trigger a flow with the `/api/v1/run/$FLOW_ID` endpoint, you can use the `files` parameter to attach image data as a base64-encoded string:

   ```bash
   curl -X POST "http://$LANGFLOW_SERVER_ADDRESS/api/v1/run/$FLOW_ID" \
   -H "Content-Type: application/json" \
   -H "x-api-key: $LANGFLOW_API_KEY" \
   -d '{
      "session_id": "custom_session_123",
      "input_value": "What is in this image?",
      "input_type": "chat",
      "output_type": "chat",
      "files": ["data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAA..."]
   }'
   ```

For more specialized image processing, browse <Icon name="Blocks" aria-hidden="true" /> [**Bundles**] or [create your own components](/components-custom-components).

## Work with video files

For videos, see the **Twelve Labs** and **YouTube** <Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components).

## See also

* [Data components](/components-data)
* [Processing components](/components-processing)


================================================================================
filename: Develop/concepts-voice-mode.mdx
================================================================================

---
title: Use voice mode
slug: /concepts-voice-mode
---

import Icon from "@site/src/components/icon";

:::info
Voice mode is not available in Langflow Desktop.
To use voice mode, [Install the Langflow OSS Python package](/get-started-installation#install-and-run-the-langflow-oss-python-package).
:::

You can use Langflow's voice mode to interact with your flows verbally through a microphone and speakers.

## Prerequisites

Voice mode requires the following:

* A flow with **Chat Input**, **Language Model**, and **Chat Output** components.

    If your flow has an **Agent** component, make sure the tools in your flow have accurate names and descriptions to help the agent choose which tools to use.

    Additionally, be aware that voice mode overrides typed instructions in the **Agent** component's **Agent Instructions** field.

* An [OpenAI](https://platform.openai.com/) account and an OpenAI API key because Langflow uses the OpenAI API to process voice input and generate responses.

* Optional: An [ElevenLabs](https://elevenlabs.io) API key to enable more voice options for the LLM's response.

* A microphone and speakers.

    A high quality microphone and minimal background noise are recommended for optimal voice comprehension.

## Test voice mode in the Playground

In the **Playground**, click the <Icon name="Mic" aria-hidden="true"/> **Microphone** to enable voice mode and verbally interact with your flows through a microphone and speakers.

The following steps use the **Simple Agent** template to demonstrate how to enable voice mode:

1. Create a flow based on the **Simple Agent** template.

2. Add your **OpenAI API key** credentials to the **Agent** component.

3. Click **Playground**.

4. Click the <Icon name="Mic" aria-hidden="true"/> **Microphone** icon to open the **Voice mode** dialog.

5. Enter your OpenAI API key, and then click **Save**. Langflow saves the key as a [global variable](/configuration-global-variables).

6. If you are prompted to grant microphone access, you must allow microphone access to use voice mode.
If microphone access is blocked, you won't be able to provide verbal input.

7. For **Audio Input**, select the input device to use with voice mode.

8. Optional: Add an ElevenLabs API key to enable more voices for the LLM's response.
Langflow saves this key as a global variable.

9. For **Preferred Language**, select the language you want to use for your conversations with the LLM.
This option changes both the expected input language and the response language.

10. Speak into your microphone to start the chat.

    If configured correctly, the waveform registers your input, and then the agent's logic and response are described verbally and in the **Playground**.

## Develop applications with websockets endpoints

Langflow exposes two OpenAI Realtime API-compatible websocket endpoints for your flows.
You can build applications against these endpoints the same way you would build against [OpenAI Realtime API websockets](https://platform.openai.com/docs/guides/realtime#connect-with-websockets).

The Langflow API's websockets endpoints require an [OpenAI API key](https://platform.openai.com/docs/overview) for authentication, and they support an optional [ElevenLabs](https://elevenlabs.io) integration with an ElevenLabs API key.

Additionally, both endpoints require that you provide the flow ID in the endpoint path.

### Voice-to-voice audio streaming

The `/ws/flow_as_tool/$FLOW_ID` endpoint establishes a connection to OpenAI Realtime voice, and then invokes the specified flow as a tool according to the [OpenAI Realtime model](https://platform.openai.com/docs/guides/realtime-conversations#handling-audio-with-websockets).

This approach is ideal for low latency applications, but it is less deterministic because the OpenAI voice-to-voice model determines when to call your flow.

### Speech-to-text audio transcription

The `/ws/flow_tts/$FLOW_ID` endpoint converts audio to text using [OpenAI Realtime voice transcription](https://platform.openai.com/docs/guides/realtime-transcription), and then directly invokes the specified flow for each transcript.

This approach is more deterministic but has higher latency.

This is the mode used in the Langflow **Playground**.

### Session IDs for websockets endpoints

Both endpoints accept an optional `/$SESSION_ID` path parameter to provide a unique ID for the conversation.
If omitted, Langflow uses the flow ID as the [session ID](/session-id).

However, be aware that voice mode only maintains context within the current conversation instance.
When you close the **Playground** or end a chat, verbal chat history is discarded and not available for future chat sessions.

## See also

* [Test flows in the Playground](/concepts-playground)


================================================================================
filename: Develop/configuration-cli.mdx
================================================================================

---
title: Langflow CLI
slug: /configuration-cli
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import Icon from "@site/src/components/icon";

The Langflow command line interface is the main interface for managing and running the Langflow server.

The Langflow CLI is automatically installed when you [install the Langflow package](/get-started-installation).
It isn't available for Langflow Desktop.

## How to use the CLI

The Langflow CLI can be invoked in several ways, depending on your installation method and environment.

The recommended approach is to run the CLI with `uv run` from within a virtual environment where Langflow is installed.

For example, to start Langflow on the default port, run the following command:

    ```bash
    uv run langflow run
    ```

If Langflow is installed globally or added to your PATH, you can execute the CLI directly with `langflow`.

    ```bash
    langflow run
    ```

## Precedence

Langflow CLI options override the values of [environment variables](/environment-variables) set in your terminal or primary `.env` file.

For example, if you have `LANGFLOW_PORT=7860` defined as an environment variable, and you run the CLI with `--port 7880`, then Langflow sets the port to `7880` because the CLI option overrides the environment variable.

This also applies to Boolean environment variables.
For example, if you set `LANGFLOW_REMOVE_API_KEYS=True` in your `.env` file, you can change it to `False` at runtime by running the CLI with `--no-remove-api-keys`.

## Langflow CLI options

All Langflow CLI commands support options that modify the command's behavior or set environment variables.

To set values for options, you can use either of the following syntax styles:

* `--option value`
* `--option=value`

Values with spaces must be surrounded by quotation marks:

* `--option 'Value with Spaces'`
* `--option="Value with Spaces"`

### Boolean options

Boolean options enable and disable settings.
They have a true (enabled) and false (disabled) form:

* Enabled (true): `--option`
* Disabled (false): `--no-option`

The following examples compare Boolean option forms for `REMOVE_API_KEYS`.

<Tabs>
<TabItem value="true" label="True" default>

`--remove-api-keys` is equivalent to setting `LANGFLOW_REMOVE_API_KEYS=True` in `.env`:

```bash
uv run langflow run --remove-api-keys
```

</TabItem>
<TabItem value="false" label="False">

`--no-remove-api-keys` is equivalent to `LANGFLOW_REMOVE_API_KEYS=False` in `.env`:

```bash
uv run langflow run --no-remove-api-keys
```

</TabItem>
</Tabs>

In the following command references, default values for Booleans include both the CLI flag and the equivalent Boolean evaluation, such as "`--option` (true)" and "`--no-option` (false)".

### Universal options

The following options are available for all Langflow CLI commands:

* `--version`, `-v`: Show the version and exit.
* `--install-completion`: Install auto-completion for the current shell.
* `--show-completion`: Show the location of the auto-completion config file, if installed.
* `--help`: Print information about command usage, options, and arguments.

## CLI commands

The following sections describe the available CLI commands and any additional options (beyond the [universal options](#universal-options)) available for each command.

### langflow

Running the CLI without any arguments prints a list of available options and commands.

<Tabs groupId="Invocation">
  <TabItem value="uv (recommended)" label="uv (recommended)" default>

```bash
uv run langflow
```

  </TabItem>
  <TabItem value="Direct" label="Direct">

```bash
langflow
```

  </TabItem>
</Tabs>

### langflow api-key {#langflow-api-key}

Creates a Langflow API key.

You must be a superuser to create API keys with the CLI.
For more information, see [Langflow API keys](/api-keys-and-authentication#langflow-api-keys).

<Tabs groupId="Invocation">
  <TabItem value="uv (recommended)" label="uv (recommended)" default>

```bash
uv run langflow api-key
```

  </TabItem>
  <TabItem value="Direct" label="Direct">

```bash
langflow api-key
```

  </TabItem>
</Tabs>

#### Options

| Option | Default | Type | Description |
|--------|---------|--------|-------------|
| `--log-level` | `error` | String | The logging level. One of `debug`, `info`, `warning`, `error`, or `critical`. |

### langflow copy-db

Copies the Langflow database files from the cache directory to the current Langflow installation directory, which is the directory containing `__main__.py`.
You can find the copy target directory by running `which langflow`.

The following files are copied if they exist in the cache directory:

* `langflow.db`: The main Langflow database, stored in the user cache directory
* `langflow-pre.db`: The pre-release database, if it exists

<Tabs groupId="Invocation">
  <TabItem value="uv (recommended)" label="uv (recommended)" default>

```bash
uv run langflow copy-db
```

  </TabItem>
  <TabItem value="Direct" label="Direct">

```bash
langflow copy-db
```

  </TabItem>
</Tabs>

### langflow migration

Manages Langflow database schema changes using [Alembic](https://alembic.sqlalchemy.org/en/latest/), a database migration tool for SQLAlchemy.

The `migration` command has two modes:

* **Test mode (default)**: Checks if migrations can be applied safely without actually running the migrations.
Use this mode to previews the changes that would be made to the database schema before proceeding with the migrations.

* **Fix mode**: Applies the migrations to update the database schema.

    :::warning
    `langflow migration --fix` is a destructive operation that can delete data.
    Always run `langflow migration` first to preview the changes.
    :::

<Tabs groupId="Invocation">
  <TabItem value="uv (recommended)" label="uv (recommended)" default>

1. Run test mode:

  ```bash
  uv run langflow migration
  ```

2. Preview the changes returned by the test to determine if it's safe to proceed with the migration.

3. Run fix mode to apply the changes:

  ```bash
  uv run langflow migration --fix
  ```

  </TabItem>
  <TabItem value="Global" label="Global">

1. Run test mode:

  ```bash
  langflow migration
  ```

2. Preview the changes returned by the test to determine if it's safe to proceed with the migration.

3. Run fix mode to apply the changes:

  ```bash
  langflow migration --fix
  ```

  </TabItem>
</Tabs>

### langflow run {#langflow-run}

Starts the Langflow server.

<Tabs groupId="Invocation">
  <TabItem value="uv (recommended)" label="uv (recommended)" default>

```bash
uv run langflow run [OPTIONS]
```

  </TabItem>
  <TabItem value="Direct" label="Direct">

```bash
langflow run [OPTIONS]
```

  </TabItem>
</Tabs>

#### Options

This command supports some common and non-sensitive configuration options for your Langflow server.
Other options must be set in the `.env` or your terminal.
For more information Langflow configuration options, see [Langflow environment variables](/environment-variables).

| Option | Default | Type | Description |
|--------|---------|--------|-------------|
| `--auto-saving` | `--auto-saving` (true) | Boolean | Whether to enable flow auto-saving in the visual editor. Use `--no-auto-saving` to disable flow auto-saving. |
| `--auto-saving-interval` | `1000` | Integer | The interval for flow auto-saving in milliseconds. |
| `--backend-only` | `--no-backend-only` (false) | Boolean | Whether to run Langflow's backend service only (no frontend). Omit or use `--no-backend-only` to start both the frontend and backend. See [Start Langflow in headless mode](#start-langflow-in-headless-mode). |
| `--cache` | `async` | String | The type of [cache storage](/memory) to use. One of `async`, `redis`, `memory`, or `disk`. |
| `--components-path` | Not set | String | The path to the directory containing your custom components. |
| `--dev` | `--no-dev` (false) | Boolean | Whether to run in development mode (may contain bugs). |
| `--env-file` | Not set | String | The path to the `.env` file containing Langflow environment variables. See [Start Langflow with a specific .env file](#start-langflow-with-a-specific-env-file). |
| `--frontend-path` | Not set | String | The path to the frontend directory containing build files. This is only used when [contributing to the Langflow codebase](/contributing-how-to-contribute) or developing a custom Langflow image that includes customized frontend code. |
| `--health-check-max-retries` | `5` | Integer | The maximum number of retries for your Langflow server's health check. |
| `--host` | `localhost` | String | The host on which the Langflow server will run. |
| `--log-file` | `logs/langflow.log` | String | The path to the log file for Langflow. |
| `--log-level` | `critical` | String | The logging level as one of `debug`, `info`, `warning`, `error`, or `critical`. |
| `--log-rotation` | Not set | String | The log rotation interval, either a time duration or file size. |
| `--max-file-size-upload` | `1024` | Integer | The maximum size in megabytes for file uploads. |
| `--open-browser` | `--no-open-browser` (false) | Boolean | Whether to open the system web browser on startup. Use `--open-browser` to open the system's default web browser when Langflow starts. |
| `--port` | `7860` | Integer | The port on which the Langflow server will run. The server automatically selects a free port if the specified port is in use. |
| `--remove-api-keys` | `--no-remove-api-keys` (false) | Boolean | Whether to remove API keys and tokens from flows saved in the Langflow database. |
| `--ssl-cert-file-path` | Not set | String | The path to the SSL certificate file on the local system for SSL-encrypted connections. |
| `--ssl-key-file-path` | Not set | String | The path to the SSL key file on the local system for SSL-encrypted connections. |
| `--worker-timeout` | `300` | Integer | The Langflow server worker timeout in seconds. |
| `--workers` | `1` | Integer | The number of Langflow server worker processes. |

#### Start Langflow with a specific .env file {#start-langflow-with-a-specific-env-file}

The `--env-file` option starts Langflow using the configuration defined in the given `.env` file.
Additional options appended to this command override the values in the `.env` file if there are duplicates.

If `--env-file` is omitted or doesn't include all required variables, Langflow uses the default values for those variables.

<Tabs groupId="Invocation">
  <TabItem value="uv (recommended)" label="uv (recommended)" default>

```bash
uv run langflow run --env-file PATH/TO/LANGFLOW/.env
```

  </TabItem>
  <TabItem value="Direct" label="Direct">

```bash
langflow run --env-file PATH/TO/LANGFLOW/.env
```

  </TabItem>
</Tabs>

#### Start Langflow in headless mode {#start-langflow-in-headless-mode}

The `--backend-only` option starts Langflow's backend service only.
This headless mode has no frontend (visual editor), and you can only access the server programmatically with the Langflow API and CLI.

<Tabs groupId="Invocation">
  <TabItem value="uv (recommended)" label="uv (recommended)" default>

```bash
uv run langflow run --backend-only
```

  </TabItem>
  <TabItem value="Direct" label="Direct">

```bash
langflow run --backend-only
```

  </TabItem>
</Tabs>

### langflow superuser {#langflow-superuser}

Creates a superuser account with the given username and password.

<Tabs groupId="Invocation">
  <TabItem value="uv (recommended)" label="uv (recommended)" default>

```bash
uv run langflow superuser --username [NAME] --password [PASSWORD] [OPTIONS]
```

  </TabItem>
  <TabItem value="Direct" label="Direct">

```bash
langflow superuser --username [NAME] --password [PASSWORD] [OPTIONS]
```

  </TabItem>
</Tabs>

#### Options

| Option | Default | Type | Description |
|--------|---------|--------|-------------|
| `--log-level` | `error` | String | The logging level. One of `debug`, `info`, `warning`, `error`, or `critical`. |

For this command, `--username` and `--password` aren't optional, and they have no default value.
The command fails if you don't provide these arguments.
For more information, see [`LANGFLOW_SUPERUSER` and `LANGFLOW_SUPERUSER_PASSWORD`](/api-keys-and-authentication#langflow-superuser).

#### Disable CLI superuser creation

The `langflow superuser` command is controlled by the [`LANGFLOW_ENABLE_SUPERUSER_CLI`](/api-keys-and-authentication#langflow-enable-superuser-cli) environment variable:

* **`LANGFLOW_ENABLE_SUPERUSER_CLI=True` (default)**: The `langflow superuser` command is available, and superuser creation is unrestricted.
* **`LANGFLOW_ENABLE_SUPERUSER_CLI=False` (recommended)**: Disables the `langflow superuser` command.
For security reasons, this is recommended to prevent unauthorized superuser creation, especially in production environments.

To disable the `langflow superuser` command, you must set `LANGFLOW_ENABLE_SUPERUSER_CLI=False` in your Langflow `.env` file, and then [start Langflow with your `.env` file](#start-langflow-with-a-specific-env-file).


================================================================================
filename: Develop/configuration-custom-database.mdx
================================================================================

---
title: Configure an external PostgreSQL database
slug: /configuration-custom-database
---

Langflow's default database is [SQLite](https://www.sqlite.org/docs.html), but you can configure Langflow to use PostgreSQL instead.

This guide walks you through setting up an external database for Langflow by replacing the default SQLite connection string `sqlite:///./langflow.db` with PostgreSQL, both in local and containerized environments.

In this configuration, all structured application data from Langflow, including flows, message history, and logs, is instead managed by PostgreSQL.
PostgreSQL is better suited for production environments due to its robust support for concurrent users, advanced data integrity features, and scalability.
Langflow can more efficiently handle multiple users and larger workloads by using PostgreSQL as the database.

## Prerequisites

- A [PostgreSQL](https://www.pgadmin.org/download/) database

## Connect Langflow to a local PostgreSQL database

1. If Langflow is running, stop Langflow with <kbd>Ctrl+C</kbd>.

2. Find your PostgreSQL database's connection string in the format `postgresql://user:password@host:port/dbname`.

    The hostname in your connection string depends on how you're running PostgreSQL:

    - If you're running PostgreSQL directly on your machine, use `localhost`.
    - If you're running PostgreSQL in Docker Compose, use the service name, such as `postgres`.
    - If you're running PostgreSQL in a separate Docker container with `docker run`, use the container's IP address or network alias.
    - If you're running a cloud-hosted PostgreSQL, your provider will share your connection string, which includes a username and password.

3. Edit or create a Langflow `.env` file:

    ```
    touch .env
    ```

    You can use the [`.env.example`](https://github.com/langflow-ai/langflow/blob/main/.env.example) file in the Langflow repository as a template for your own `.env` file.

4. In your `.env` file, set `LANGFLOW_DATABASE_URL` to your PostgreSQL connection string:

    ```text
    LANGFLOW_DATABASE_URL="postgresql://user:password@localhost:5432/dbname"
    ```

    Langflow uses [SQLAlchemy](https://www.sqlalchemy.org/) with the [psycopg](https://www.psycopg.org/) driver to pass SSL parameters directly to the PostgreSQL connection. The following SSL modes are supported:

    - **`sslmode=require`**: Requires SSL connection but doesn't verify server certificate. This option is the least secure, but acceptable for most use cases.
        ```bash
        LANGFLOW_DATABASE_URL="postgresql://user:password@localhost:5432/dbname?sslmode=require"
        ```

    - **`sslmode=verify-ca`**: Requires SSL and verifies the server certificate against the Certificate Authority (CA). Add the certificate paths to your connection string:
        ```bash
        LANGFLOW_DATABASE_URL="postgresql://user@localhost:5432/dbname?sslmode=verify-ca&sslcert=/path/to/client.crt&sslkey=/path/to/client.key&sslrootcert=/path/to/ca.crt"
        ```

    - **`sslmode=verify-full`**: Requires SSL, verifies the server certificate, and checks the request hostname against the certificate hostname. The `db.example.com` hostname in this example must match the server certificate's CN. This option is the most secure.
        ```bash
        LANGFLOW_DATABASE_URL="postgresql://user@db.example.com:5432/dbname?sslmode=verify-full&sslcert=/path/to/client.crt&sslkey=/path/to/client.key&sslrootcert=/path/to/ca.crt"
        ```

        Do not use the Langflow environment variables [`LANGFLOW_SSL_CERT_FILE`](/environment-variables#server) and [`LANGFLOW_SSL_KEY_FILE`](/environment-variables#server) for your PostgreSQL certificates: these variables are for enabling HTTPS on the Langflow server, not for PostgreSQL database connections.

        For more on managing SSL certificates in PostgreSQL, see the [PostgreSQL documentation](https://www.postgresql.org/docs/9.1/ssl-tcp.html).

5. Save your changes, and then start Langflow with your `.env` file:

    ```bash
    uv run langflow run --env-file .env
    ```

    For optional connection pooling and timeout settings, see [Configure external memory](/memory#configure-external-memory).

6. In Langflow, run any flow to create traffic.

7. Inspect your PostgreSQL database's tables and activity to verify that new tables and traffic were created after you ran a flow.

## Deploy Langflow and PostgreSQL containers with docker-compose.yml

Launching Langflow and PostgreSQL containers in the same Docker network ensures proper connectivity between services.
For an example, see the [`docker-compose.yml`](https://github.com/langflow-ai/langflow/blob/main/docker_example/docker-compose.yml) file in the Langflow repository.

The configuration in the example `docker-compose.yml` also sets up persistent volumes for both Langflow and PostgreSQL data.
Persistent volumes map directories inside of containers to storage on the host machine, so data persists through container restarts.

Docker Compose creates an isolated network for all services defined in `docker-compose.yml`. This ensures that the services can communicate with each other using their service names as hostnames, such as `postgres` in the database URL.
In contrast, if you run PostgreSQL separately with `docker run`, it launches in a different network than the Langflow container, and this prevents Langflow from connecting to PostgreSQL using the service name.

To start the Langflow and PostgreSQL services with the example Docker Compose file, navigate to the `langflow/docker_example` directory, and then run `docker-compose up`.
If you're using a different `docker-compose.yml` file, run the `docker-compose up` command from the same directory as your `docker-compose.yml` file.

## Deploy multiple Langflow instances with a shared PostgreSQL database

To configure multiple Langflow instances that share the same PostgreSQL database, modify your `docker-compose.yml` file to include multiple Langflow services.

This example populates the values in `docker-compose.yml` with values from your Langflow `.env` file.
This approach means you only have to manage deployment variables in one file, instead of copying values across multiple files.

1. Update your `.env` file with values for your PostgreSQL database:

    ```text
    POSTGRES_USER=langflow
    POSTGRES_PASSWORD=your_secure_password
    POSTGRES_DB=langflow
    POSTGRES_HOST=postgres
    POSTGRES_PORT=5432
    LANGFLOW_CONFIG_DIR=app/langflow
    LANGFLOW_PORT_1=7860
    LANGFLOW_PORT_2=7861
    LANGFLOW_HOST=0.0.0.0
    ```

    For optional connection pooling and timeout settings, see [Configure external memory](/memory#configure-external-memory).

2. Reference these variables in your `docker-compose.yml`.
For example:

    ```yaml
    services:
      postgres:
        image: postgres:16
        environment:
          - POSTGRES_USER=${POSTGRES_USER}
          - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
          - POSTGRES_DB=${POSTGRES_DB}
        ports:
          - "${POSTGRES_PORT}:5432"
        volumes:
          - langflow-postgres:/var/lib/postgresql/data

      langflow-1:
        image: langflowai/langflow:latest
        pull_policy: always
        ports:
          - "${LANGFLOW_PORT_1}:7860"
        depends_on:
          - postgres
        environment:
          - LANGFLOW_DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}
          - LANGFLOW_CONFIG_DIR=${LANGFLOW_CONFIG_DIR}
          - LANGFLOW_HOST=${LANGFLOW_HOST}
          - PORT=7860
        volumes:
          - langflow-data-1:/app/langflow

      langflow-2:
        image: langflowai/langflow:latest
        pull_policy: always
        ports:
          - "${LANGFLOW_PORT_2}:7860"
        depends_on:
          - postgres
        environment:
          - LANGFLOW_DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}
          - LANGFLOW_CONFIG_DIR=${LANGFLOW_CONFIG_DIR}
          - LANGFLOW_HOST=${LANGFLOW_HOST}
          - PORT=7860
        volumes:
          - langflow-data-2:/app/langflow

    volumes:
      langflow-postgres:
      langflow-data-1:
      langflow-data-2:
    ```

3. Deploy the file with `docker-compose up`.
You can access the first Langflow instance at `http://localhost:7860`, and the second Langflow instance at `http://localhost:7861`.

4. To confirm both instances are using the same database, run the `docker exec` command to start `psql` in your PostgreSQL container.
Your container name may vary.

    ```bash
    docker exec -it docker-test-postgres-1 psql -U langflow -d langflow
    ```

5. Query the database for active connections:

    ```sql
    langflow=# SELECT * FROM pg_stat_activity WHERE datname = 'langflow';
    ```

6. Examine the query results for multiple connections with different `client_addr` values, for example `172.21.0.3` and `172.21.0.4`.
Since each Langflow instance runs in its own container on the Docker network, using different incoming IP addresses confirms that both instances are actively connected to the PostgreSQL database.

7. To quit `psql`, type `quit`.

## See also

* [Langflow database guide for enterprise DBAs](/enterprise-database-guide)
* [Memory management options](/memory)
* [Logs](/logging)


================================================================================
filename: Develop/configuration-global-variables.mdx
================================================================================

---
title: Global variables
slug: /configuration-global-variables
---

import Icon from "@site/src/components/icon";
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

Use global variables to store and reuse credentials and generic values across all of your flows.
Global variables are typically used by components in flow, and you can use them in any field with the <Icon name="Globe" aria-hidden="true"/> global variable icon.

In contrast, [environment variables](/environment-variables), like `LANGFLOW_PORT` or `LANGFLOW_LOG_LEVEL`, are generally for broader settings that configure how Langflow runs.
However, Langflow can also source global variables from environment variables.

Langflow stores global variables in its internal database, and it encrypts the values using a secret key.

## Create a global variable

To create a new global variable, follow these steps.

1. In the Langflow header, click your profile icon, and then select **Settings**.
2. Click **Global Variables**.

3. Click **Add New**.

4. In the **Create Variable** dialog, enter a name for your variable in the **Variable Name** field.

5. Optional: Select a **Type** for your global variable. The available types are **Generic** (default) and **Credential**.

   Langflow encrypts both **Generic** and **Credential** type global variables.
   However, **Generic** variables aren't masked in the visual editor, whereas **Credential** variables are masked.
   **Session ID** fields don't accept **Credential** (masked) variables.

6. Enter the **Value** for your global variable.

7. Optional: Use the **Apply To Fields** menu to select one or more fields that you want Langflow to automatically apply your global variable to. For example, if you select **OpenAI API Key**, Langflow automatically applies the variable to any **OpenAI API Key** field.

8. Click **Save Variable**.

You can now select your global variable from any text input field that displays the <Icon name="Globe" aria-hidden="true"/> **Globe** icon.

## Edit a global variable

1. In the Langflow header, click your profile icon, and then select **Settings**.

2. Click **Global Variables**.

3. Click on the global variable you want to edit.

4. In the **Update Variable** dialog, you can edit the following fields: **Variable Name**, **Value**, and **Apply To Fields**.

5. Click **Update Variable**.

## Delete a global variable

Deleting a global variable permanently deletes the value from the database.
Flows that reference the deleted global variable will fail.

1. In the Langflow header, click your profile icon, and then select **Settings**.

2. Click **Global Variables**.

3. Click the checkbox next to the global variable that you want to delete.

4. Click <Icon name="Trash2" aria-hidden="true"/> **Delete**.

The global variable is deleted from the database.

## Add custom global variables from the environment {#add-custom-global-variables-from-the-environment}

Langflow can source custom global variables from your runtime environment.
For information about how Langflow detects and applies environment variables, see [Langflow environment variables](/environment-variables).

Langflow automatically generates global variables based on [`constants.py`](https://github.com/langflow-ai/langflow/blob/main/src/lfx/src/lfx/services/settings/constants.py) if it detects any matching environment variables.
For example, if you set `OPENAI_API_KEY` in your runtime environment, Langflow automatically generates a global variable using that value.

You can declare additional variables in `LANGFLOW_VARIABLES_TO_GET_FROM_ENVIRONMENT`.
For example, `LANGFLOW_VARIABLES_TO_GET_FROM_ENVIRONMENT=WATSONX_PROJECT_ID,WATSONX_API_KEY` creates global variables named `WATSONX_PROJECT_ID` and `WATSONX_API_KEY` in Langflow's database.
Then, you can use these variables wherever they are needed in your component settings.

<Tabs>
<TabItem value="local" label="Local" default>

If you installed Langflow locally, set `LANGFLOW_VARIABLES_TO_GET_FROM_ENVIRONMENT` in your Langflow `.env` file:

1. Create or edit your Langflow `.env` file.

2. Add the `LANGFLOW_VARIABLES_TO_GET_FROM_ENVIRONMENT` environment variable as follows:

   You can specify the variables either as a comma-separated string with no spaces, or as a JSON list:

   ```text
   # Option 1: Comma-separated string (no spaces)
   LANGFLOW_VARIABLES_TO_GET_FROM_ENVIRONMENT=VARIABLE1,VARIABLE2

   # Option 2: JSON list format
   LANGFLOW_VARIABLES_TO_GET_FROM_ENVIRONMENT=["VARIABLE1", "VARIABLE2"]
   ```

   Replace `VARIABLE1,VARIABLE2` with your additional variables that you want Langflow to source from the environment, such as `CUSTOM_API_KEY,INTERNAL_SERVICE_URL` or `["CUSTOM_API_KEY", "INTERNAL_SERVICE_URL"]`.

3. Save and close the file.

4. Start Langflow with the `.env` file:

   ```bash
   uv run langflow run --env-file .env
   ```

   Alternatively, you can set environment variables directly in the command line:

   ```bash
   VARIABLE1="VALUE1" VARIABLE2="VALUE2" uv run langflow run --env-file .env
   ```

   The command-line variables override matching variables in the `.env` file.
   Expose your environment variables to Langflow in a manner that best suits your own environment.

5. Confirm that Langflow successfully sourced the global variables from the environment:

   1. In the Langflow header, click your profile icon, and then select **Settings**.

   2. Click **Global Variables**, and then make sure that your environment variables appear in the **Global Variables** list.

</TabItem>
<TabItem value="docker" label="Docker">

If you're using Docker, there are two ways that you can set `LANGFLOW_VARIABLES_TO_GET_FROM_ENVIRONMENT`:

* On the command line:

   ```bash
   docker run -it --rm \
      -p 7860:7860 \
      -e LANGFLOW_VARIABLES_TO_GET_FROM_ENVIRONMENT="VARIABLE1,VARIABLE2" \
      -e VARIABLE1="VALUE1" \
      -e VARIABLE2="VALUE2" \
      langflowai/langflow:latest
   ```

* In your `.env` file:

   ```bash
   docker run -it --rm \
      -p 7860:7860 \
      --env-file .env \
      -e VARIABLE1="VALUE1" \
      -e VARIABLE2="VALUE2" \
      langflowai/langflow:latest
   ```

The list in `LANGFLOW_VARIABLES_TO_GET_FROM_ENVIRONMENT` includes only the variable names.
You must ensure that these environment variables are defined in your Docker environment, such as with `-e` or otherwise.

</TabItem>
</Tabs>

After starting Langflow, go to your Langflow **Settings** to confirm that the variables were created.

Only the **Name** and **Value** are taken from the environment.
You can edit the variables in your Langflow **Settings** if you want to configure additional options, such as the **Apply To Fields** option.

Global variables sourced from the environment are assigned the **Credential** type, which masks the values in the visual editor.
However, Langflow automatically encrypts _all_ global variables stored in the database.

## Disallow global variables from the environment

If you want to explicitly prevent Langflow from sourcing global variables from the environment, set `LANGFLOW_STORE_ENVIRONMENT_VARIABLES=False` in your `.env` file.

## Use environment variables for missing global variables {#use-environment-variables-for-missing-global-variables}

If you want to automatically set fallback values for your global variables to environment variables, set `LANGFLOW_FALLBACK_TO_ENV_VAR=True` in your `.env` file.
When this setting is enabled, if a global variable isn't found, Langflow attempts to use an environment variable with the same name as a backup.

For example, assume you have the following Langflow `.env` configuration, and your flow has a component that expects a `WATSONX_API_KEY` global variable:

```text
LANGFLOW_FALLBACK_TO_ENV_VAR=True
WATSONX_PROJECT_ID=your_project_id
WATSONX_API_KEY=your_api_key
```

When you run the flow, if there is no global variable named `WATSONX_API_KEY`, Langflow looks for an environment variable named `WATSONX_API_KEY`.
In this example, Langflow uses the `WATSONX_API_KEY` value from the `.env` to run the flow.


================================================================================
filename: Develop/contributing-telemetry.mdx
================================================================================

---
title: Telemetry
slug: /contributing-telemetry
---

Langflow uses anonymous telemetry to collect statistics about feature usage and performance.
The Langflow team uses this data to identify popular features and areas that need improvement based on actual usage patterns.
This helps prioritize development efforts on the most impactful changes and popular features.

## Privacy

The Langflow team respects your privacy, and the team is committed to protecting your data.

Langflow telemetry doesn't collect any personal information or sensitive data.
All telemetry data is anonymized and used solely for improving Langflow.

## Opt out of telemetry

To opt out of telemetry, set `DO_NOT_TRACK=True` in your [Langflow environment variables](/environment-variables) before starting Langflow. This disables telemetry data collection.

## Data that Langflow collects

Langflow telemetry collects data on flow runs, your environment, and component usage.

### Run

This telemetry event is sent every time a flow is executed.

- **IsWebhook**: Indicates whether the operation was triggered with a webhook.
- **Seconds**: Duration in seconds for how long the operation lasted, providing insights into performance.
- **Success**: Boolean value indicating whether the operation was successful, helping identify potential errors or issues.
- **ErrorMessage**: Provides error message details if the operation was unsuccessful, aiding in troubleshooting and enhancements.

### Shutdown

This telemetry event captures information about application lifecycle and runtime duration.

- **TimeRunning**: Total runtime before shutdown, which is useful for understanding the application lifecycle and optimizing uptime.

### Version

This telemetry event is sent once when the telemetry service starts.

- **Version**: The specific version of Langflow used, which helps in tracking feature adoption and compatibility.
- **Platform**: Operating system of the host machine, which helps determine the most popular platforms for development and testing efforts.
- **Python**: The version of Python used, assisting in maintaining compatibility and support for various Python versions.
- **Arch**: Architecture of the system, such as x86 or ARM, which helps prioritize hardware optimization and testing in the Langflow codebase.
- **AutoLogin**: Indicates whether the auto-login feature is enabled, reflecting user preference settings.
- **CacheType**: Type of caching mechanism used, which impacts performance and efficiency.
- **BackendOnly**: Boolean indicating whether Langflow is running in backend-only mode, useful for understanding deployment configurations.
- **Desktop**: Indicates whether Langflow is running in desktop mode (Langflow Desktop), helping to understand usage patterns across different deployment types.

### Playground

This telemetry event monitors performance and usage patterns in the **Playground** environment.

- **Seconds**: Duration in seconds for **Playground** execution, offering insights into performance during testing or experimental stages.
- **ComponentCount**: Number of components used in the **Playground**, which helps understand complexity and usage patterns.
- **Success**: Success status of the **Playground** operation, aiding in identifying the stability of experimental features.

### Component

This telemetry event is sent for each component execution.

- **Name**: Identifies the component, providing data on which components are most utilized or prone to issues.
- **Seconds**: Time taken by the component to execute, offering performance metrics.
- **Success**: Whether the component operated successfully, which helps in quality control.
- **ErrorMessage**: Details of any errors encountered, crucial for debugging and improvement.

### Exception

This telemetry event is sent when an unhandled exception is captured by Langflow's lifecycle or global exception handler.

- **Type**: The exception class name, such as `ValueError`.
- **Message**: The exception message that was raised.
- **Context**: Additional contextual information related to where the exception occurred, such as route, component, or operation details, when available.
- **StackTraceHash**: A hash of the stack trace used to group similar exceptions for easier analysis.


================================================================================
filename: Develop/data-types.mdx
================================================================================

---
title: Langflow data types
slug: /data-types
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import Icon from "@site/src/components/icon";

Langflow components are designed to accept and produce specific types of inputs and outputs.
Input and output data types define the structure and flow of information between components.
Understanding these structures helps you build applications that provide valid input and correctly anticipate the output format.

[Component ports](/concepts-components#component-ports) represent the data types that each component can send and receive.
Some data types are self-evident from the fields they are attached to; for example, a **System Message** field accepts [message data](#message).
[Port colors](/concepts-components#port-colors) also indicate the port's data type.
For example **Data** ports, represented by <Icon name="Circle" size="16" aria-label="Red data port" style={{ color: '#dc2626', fill: '#dc2626' }} />, either accept or emit [structured data objects](#data).

When building flows, connect output ports to input ports of the same type (color) to transfer that type of data between two components.

:::tip
* In the [workspace](/concepts-overview#workspace), hover over a port to see connection details for that port.
Click a port to <Icon name="Search" aria-hidden="true" /> **Search** for compatible components.

* If two components have incompatible data types, you can use a processing component like the [**Type Convert** component](/components-processing#type-convert) to convert the data between components.
:::

## Data

**Data** ports <Icon name="Circle" size="16" aria-label="Red data port" style={{ color: '#dc2626', fill: '#dc2626' }} /> accept or produce the `Data` type, which is a structured data object, like a JSON payload that you might send to an API.
This data type is used to pass key-value pairs between components, such as user profiles, settings, or other structured information.

`Data` objects include a primary text field, indicated by a `text_key`, and additional metadata.

### Schema and attributes

The schema is defined in [`data.py`](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/schema/data.py).

The following attributes are available:

- `data`: A `Data` object stores key-value pairs within the `.data` attribute. This is the `Data` object's core dictionary. Each key is a field name, and the values can be any supported data type.
- `text_key`: The key in `data` that is considered the primary text value.
- `default_value`: Fallback if `text_key` is missing. The default `text_key` is `"text"`.

```python
data_obj = Data(
    text_key="text",
    data={
        "text": "Hello world",
        "name": "Charlie",
        "age": 28
    },
    default_value=""
)
```

`Data` objects can be serialized to JSON, created from JSON, or created from other dictionary data.
However, the resulting `Data` object is a structured object with validation and methods, not a plain dictionary.
For example, when serialized into JSON, the previous Python example becomes the following JSON object:

```json
{
  "text_key": "text",
  "data": {
    "text": "Hello world",
    "name": "Charlie",
    "age": 28
  },
  "default_value": ""
}
```

## DataFrame

**DataFrame** ports <Icon name="Circle" size="16" aria-label="Pink dataframe port" style={{ color: '#ec4899', fill:'#ec4899' }} /> accept or produce [pandas DataFrames](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), which are similar to the tabular CSV data.

Use the `DataFrame` type to work with data containing multiple rows or records.

### Schema and attributes

The schema is defined in [`dataframe.py`](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/schema/dataframe.py).

The following attributes are available:

- **Full pandas compatibility**: All pandas DataFrame methods and functionality are supported

- **Langflow integration**: Accepts lists of [`Data` objects](#data), dictionaries, or existing DataFrames.

- **Convenience methods**:
   - `to_data_list()`
   - `add_row()`
   - `add_rows()`
   - `to_lc_documents()`
   - `to_data()`
   - `to_message()`

- **Text key support**: Maintains `text_key` and `default_value` attributes for [`Data` object](#data) compatibility.

### DataFrame structure

A DataFrame has a tabular data structure with rows and columns.
Keys are columns, and each object in the array is a row.

```json
[
  {
    "name": "Charlie Lastname",
    "age": 28,
    "email": "charlie.lastname@example.com"
  },
  {
    "name": "Alexandra Example",
    "age": 34,
    "email": "alexandra@example.com"
  }
]
```

When represented as tabular data, the preceding DataFrame object is structured as follows:

```text
| name | age | email |
|------|-----|-------|
| Charlie Lastname | 28 | charlie.lastname@example.com |
| Alexandra Example | 34 | alexandra@example.com |
```

## Embeddings

**Embeddings** ports <Icon name="Circle" size="16" aria-label="Emerald embeddings port" style={{ color: '#10b981', fill: '#10b981' }} /> emit or ingest vector embeddings to support functions like similarity search.

The `Embeddings` data type is used specifically by components that either produce or consume vector embeddings, such as the [embedding model components](/components-embedding-models) and vector store components.

For example, embedding model components output `Embeddings` data that you can connect to an **Embedding** input port on a vector store component.

For information about the underlying Python classes that produce `Embeddings`, see the [LangChain Embedding models documentation](https://docs.langchain.com/oss/python/integrations/text_embedding).

## LanguageModel

The `LanguageModel` type is a specific data type that can be produced by language model components and accepted by components that use an LLM.

When you change a language model component's output type from **Model Response** to **Language Model**, the component's output port changes from a **Message** port to a **Language Model** port <Icon name="Circle" size="16" aria-label="Fuchsia language model port" style={{ color: '#c026d3', fill: '#c026d3' }} />.
Then, you connect the outgoing **Language Model** port to a **Language Model** input port on a compatible component, such as a **Smart Function** component.

For more information about using these components in flows and toggling `LanguageModel` output, see [Language model components](/components-models#language-model-output-types).

<details>
<summary>LanguageModel is an instance of LangChain ChatModel</summary>

Because Langflow is built on LangChain, `LanguageModel` is actually an instance of a [LangChain chat model](https://docs.langchain.com/oss/python/integrations/chat) that uses the configuration parameters set in the originating component.

Often, components produce an instance of an integrated chat model that is designed to support the specific model provider, such as [`ChatOpenAI`](https://docs.langchain.com/oss/python/integrations/chat/openai) or [`ChatAnthropic`](https://docs.langchain.com/oss/python/integrations/chat/anthropic).

You can inspect the [component code](/concepts-components#component-code) to see the specific `Chat` instance it produces.

</details>

## Memory

**Memory** ports <Icon name="Circle" size="16" aria-label="Orange memory port" style={{ color: '#f97316', fill: '#f97316' }} /> are used to integrate a **Message History** component with external chat memory storage.

For more information, see the [**Message History** component](/components-helpers#message-history).

## Message

**Message** ports <Icon name="Circle" size="16" aria-label="Indigo message port" style={{ color: '#4f46e5', fill: '#4f46e5' }} /> accept or produce `Message` data, which extends the [`Data` type](#data) with additional fields and methods for text input typically used in chat flows.

This data type is used by many components.

:::tip
Components that accept or produce `Message` data may not include all attributes in the incoming or outgoing `Message` data.
As long as the data is compatible with the `Message` schema, it can be valid.

When building flows, focus on the fields shown on each component in the workspace, rather than the data types passed between components.
The details of a particular data type are often only relevant when you are debugging a flow or component that isn't producing the expected output.

For example, a **Chat Input** component only requires the content of the **Input Text** (`input_value`) field.
The component then constructs a complete `Message` object before passing the data to other components in the flow.
:::

### Schema, structure, and attributes

The `Message` schema is defined in [`message.py`](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/schema/message.py).
Some `Message` attributes have their own schema definitions, such as [`content_block.py`](https://github.com/langflow-ai/langflow/blob/main/src/backend/base/langflow/schema/content_block.py).

`Message` data is structured as a JSON object.
For example:

```json
{
  "text": "Name: Charlie Lastname, Age: 28, Email: charlie.lastname@example.com",
  "sender": "User",
  "sender_name": "Charlie Lastname",
  "session_id": "some-session-id",
  "timestamp": "2024-06-01T12:00:00Z",
  "files": [],
  "content_blocks": [],
  "category": "message"
}
```

The attributes included in a specific `Message` object depend on the context, including the component type, flow activity, and whether the message is a query or response.
Some common attributes include the following:

- `text`: The main message content.
- `sender`: Identifies the originator of a chat message as either `User` or `Language Model`.
- `sender_name`: The display name for the sender. Defaults to `User` or `Language Model`.
- `session_id`: The chat [session identifier](/session-id).
- `flow_id`: The ID of the flow that the message is associated with. `flow_id` and `session_id` are the same if the flow doesn't use custom session IDs.
- `timestamp`: The UTC timestamp that the message was sent.
- `files`: A list of file paths or images included with the message
- `content_blocks`: Container for rich content input, such as text, media, or code. Also contains error message information if the LLM can't process the input.
- `category`: `"message"`, `"error"`, `"warning"`, or `"info"`.

Not all attributes are required, and some components accept message-compatible input, such as raw text input.
The strictness depends on the component.

### Message data in Input and Output components

In flows with [**Chat Input and Output** components](/components-io#chat-io), `Message` data provides a consistent structure for chat interactions, and it is ideal for chatbots, conversational analysis, and other use cases based on a dialogue with an LLM or agent.
In these flows, the **Playground** chat interface prints only the `Message` attributes that are relevant to the conversation, such as `text`, `files`, and error messages from `content_blocks`.
To see all `Message` attributes, inspect the message logs in the **Playground**.

In flows with [**Text Input and Output** components](/components-io#text-io), `Message` data is used to pass simple text strings without the chat-related metadata.
These components handle `Message` data as independent text strings, not as part of an ongoing conversation.
For this reason, a flow with only **Text Input and Output** components isn't compatible with the **Playground**.
For more information, see [Input and output components](/components-io).

When using the Langflow API, the response includes the `Message` object along with other response data from the flow run.
Langflow API responses can be extremely verbose, so your applications must include code to extract relevant data from the response to return to the user.
For an example, see the [Langflow quickstart](/get-started-quickstart).

Additionally, input sent to the input port of input/output components does _not_ need to be a complete `Message` object because the component constructs the `Message` object that is then passed to other components in the flow or returned as flow output.
In fact, some components shouldn't receive a complete `Message` object because some attributes, like `timestamp` should be added by the component for accuracy.

## Tool

**Tool** ports <Icon name="Circle" size="16" aria-label="Cyan tool port" style={{ color: '#06b6d4', fill: '#06b6d4' }} /> connect tools to an **Agent** component.

Tools can be other components where you enabled **Tool Mode**, they can be the dedicated **MCP Tools** component, or they can be other components that only support **Tool Mode**.
Multiple tools can be connected to the same **Agent** component at the same port.

Functionally, `Tool` data is a LangChain `StructuredTool` object that can be used in agent flows.

For more information, see [Configure tools for agents](/agents-tools) and [Use Langflow as an MCP client](/mcp-client).

## Unknown or multiple types

If a port can accept or produce multiple data types, it is represented by the gray port icon <Icon name="Circle" size="16" aria-label="Gray unknown port" style={{ color: '#9CA3AF', fill: '#9CA3AF' }} />.

Hover over the port to see the accepted or produced data types.

## View data types in flows

In Langflow, you can use <Icon name="TextSearch" aria-hidden="true" /> **Inspect output** to view the output of individual components.
This can help you learn about the different data type and debug problems with invalid or malformed inputs and output.

The following example shows how to inspect the output of a [**Type Convert** component](/components-processing#type-convert), which can convert data from one type to another:

1. Create a flow, and then connect a **Chat Input** component to a **Type Convert** component.

2. In the **Chat Input** component, enter some text for the type converter to process.

3. On the **Type Convert** component, click <Icon name="Play" aria-hidden="true"/> **Run component**, and then click <Icon name="TextSearch" aria-hidden="true" /> **Inspect output**.

    The default output is `Message` data, which is the same as the input coming from the **Chat Input** component.
    To see the `Message` data converted to `Data` or `DataFrame`, change the **Output Type** on the **Type Convert** component, and then rerun the component.

    <Tabs>
    <TabItem value="Message" label="Message" default>

    ```text
    Input text
    ```

    </TabItem>
    <TabItem value="Data" label="Data">

    ```json
    {
      "timestamp": "2025-07-15 20:56:20 UTC",
      "sender": "User",
      "sender_name": "User",
      "session_id": "a0c7e888-4fd6-4242-b8c8-e761ad690aeb",
      "text": "",
      "files": [],
      "error": false,
      "edit": false,
      "properties": {
        "text_color": "",
        "background_color": "",
        "edited": false,
        "source": {
          "id": null,
          "display_name": null,
          "source": null
        },
        "icon": "",
        "allow_markdown": false,
        "positive_feedback": null,
        "state": "complete",
        "targets": []
      },
      "category": "message",
      "content_blocks": [],
      "id": "9da72da2-efbb-4ccd-90ad-b32429b0418e",
      "flow_id": "a0c7e888-4fd6-4242-b8c8-e761ad690aeb",
      "duration": null
    }
    ```

    </TabItem>
    <TabItem value="DataFrame" label="DataFrame">

    ```text
    | Field | Value |
    |-------|-------|
    | timestamp | 2025-07-15 20:56:11 UTC |
    | sender | User |
    | sender_name | User |
    | session_id | a0c7e888-4fd6-4242-b8c8-e761ad690aeb |
    | text | (empty) |
    | files | [] |
    | error | False |
    | edit | False |
    | properties | text_color: '', background_color: '', edited: False, source: {id: None, display_name: None, source: None}, icon: '', allow_markdown: False, positive_feedback: None, state: 'complete', targets: [] |
    | category | message |
    | content_blocks | [] |
    | id | 341686eb-7a39-4b80-a90a-d8bf267815ef |
    | flow_id | a0c7e888-4fd6-4242-b8c8-e761ad690aeb |
    | duration | (empty) |
    ```

    </TabItem>
    </Tabs>

## See also

- [Processing components](/components-processing)
- [Custom components](/components-custom-components)
- [Pydantic Models](https://docs.pydantic.dev/latest/api/base_model/)
- [pandas.DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)


================================================================================
filename: Develop/enterprise-database-guide.mdx
================================================================================

---
title: Langflow database guide for enterprise DBAs
slug: /enterprise-database-guide
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

The Langflow database stores data that is essential for more Langflow operations, including startup, flow execution, user interactions, and administrative tasks.
The database supports both frontend (visual editor) and backend (API) operations, making its availability critical to Langflow's stability and functionality.
For details about the database schema, see [Memory management options](/memory).

This guide is designed for enterprise database administrators (DBAs) and operators responsible for deploying and managing Langflow in production environments.
It explains how to configure Langflow to use PostgreSQL, including high availability (HA) and active-active configurations, as well as best practices for monitoring, maintenance, and security.

## Configure Langflow with PostgreSQL

Langflow's default database is SQLite.
However, PostgreSQL is recommended for production deployments due to its scalability, performance, and robustness.

The following steps explain how to configure Langflow to use PostgreSQL for a standalone or containerized deployment.
For more information, see [Configure an external PostgreSQL database](/configuration-custom-database).

1. Set up PostgreSQL:

   1. Deploy a PostgreSQL instance (version 12 or higher recommended) using a local server, Docker, or a managed cloud service.
   2. Create a database for Langflow.
   3. Create a PostgreSQL user with appropriate, minimal permissions to manage and write to the database, such as CREATE, SELECT, INSERT, UPDATE, DELETE on your Langflow tables.

2. Obtain the connection string in the format `postgresql://user:password@host:port/dbname`, such as`postgresql://langflow:securepassword@postgres:5432/langflow`.

    For High Availability, use the virtual IP or proxy hostname instead of the direct database host.
    For more information, see [High Availability for PostgreSQL](#high-availability-for-postgresql).

3. Configure Langflow with the `.env` or `docker-compose.yml` files.

    <Tabs groupId="environment">
    <TabItem value=".env" label=".env" default>

    1. Create a `.env` file in the `langflow` directory:

        ```shell
        touch .env
        ```

    2. Add the connection string to the `.env` file:

        ```text
        LANGFLOW_DATABASE_URL="postgresql://langflow:securepassword@postgres:5432/langflow"
        ```

    For more environment variables, see the `.env.example` file in the Langflow repository.

    </TabItem>
    <TabItem value="docker-compose.yml" label="docker-compose.yml">

    Use the sample `docker-compose.yml` from the Langflow Repository.
    You can use the default values or customize them as needed.

    ```yaml
    version: '3'
    services:
      langflow:
        image: langflowai/langflow:latest
        ports:
          - "7860:7860"
        environment:
          - LANGFLOW_DATABASE_URL=postgresql://langflow:langflow@postgres:5432/langflow
      postgres:
        image: postgres:16
        ports:
          - "5432:5432"
        environment:
          - POSTGRES_USER=langflow
          - POSTGRES_PASSWORD=langflow
          - POSTGRES_DB=langflow
        volumes:
          - langflow-postgres:/var/lib/postgresql/data
    volumes:
      - langflow-postgres:
    ```

    </TabItem>
    </Tabs>


4. Start Langflow with your PostgreSQL connection:

    <Tabs groupId="environment">
      <TabItem value=".env" label=".env" default>

      ```shell
      uv run langflow run --env-file .env
      ```

      </TabItem>
      <TabItem value="docker-compose.yml" label="docker-compose.yml">

      Navigate to the directory containing `docker-compose.yml`, and then run `docker-compose up`.

      </TabItem>
    </Tabs>

5. Optional: Run migrations.

    Langflow uses migrations to manage its database schema.
    When you first connect to PostgreSQL, Langflow automatically runs migrations to create the necessary tables.

    Direct schema modification can cause conflicts with Langflow's built-in schema management.
    If you need to update the schema, you can manually run migrations with the Langflow CLI:

    1. Run `langflow migration` to preview the changes.

    2. Review the changes to ensure that it's safe to proceed with the migration.

    3. Run `langflow migration --fix` to run the migration and permanently apply the changes.

        This is a destructive operation that can delete data.
        For more information, see [`langflow migration`](/configuration-cli#langflow-migration).

6. To verify the configuration, create any flow using the Langflow visual editor or API, and then query your database to confirm the tables and activity are recorded there. The content of the flow doesn't matter; you only need to confirm that the flow is stored in your PostgreSQL database.
You can query the database in two ways:

    * Query the database container:

        ```
        docker exec -it <postgres-container> psql -U langflow -d langflow
        ```

    * Use SQL:

        ```
        SELECT * FROM pg_stat_activity WHERE datname = 'langflow';
        ```

### High Availability for PostgreSQL {#high-availability-for-postgresql}

To further improve performance, reliability, and scalability, use a High Availability (HA) or Active-Active HA PostgreSQL configuration.
This is recommended for production deployments to minimize downtime and ensure continuous operations if your database server fails, especially when multiple Langflow instances rely on the same database.

<Tabs>
<TabItem value="HA" label="High Availability (HA)" default>

1. Set up streaming replication:

    1. Configure one primary database for writes.
    2. Configure one or more replicas for reads and failover.

        Select either synchronous or asynchronous replication based on your latency and consistency requirements.

2. Implement automatic failover using one of the following options:

    * Use an HA orchestrator, distributed configuration store, and traffic router, such as [Patroni](https://patroni.readthedocs.io/en/latest/), etcd or [Consul](https://developer.hashicorp.com/consul), and [HAProxy](https://www.haproxy.org/).
    * Use [Pgpool-II](https://www.pgpool.net/docs/46/en/html/index.html) alone or add supporting services for more robust HA support.
    * Use managed services that provide built-in HA with automatic failover, such as AWS RDS or Google Cloud SQL.

3. Update your PostgreSQL connection string to point to the HA setup.
If you have a multi-instance deployment, make sure all of your Langflow instances connect to the same HA PostgreSQL database.

    The connection string you use depends on your HA configuration and services.

    * Use a virtual IP or DNS name that resolves to the current primary database, such as `postgresql://langflow:securepassword@db-proxy:5432/langflow?sslmode=require`.
    * Use the provided endpoint for a managed service, such as `langflow.cluster-xyz.us-east-1.rds.amazonaws.com`.

4. Optional: Implement load balancing for read-heavy workloads:

    1. Use a connection pooler like [PgBouncer](https://www.pgbouncer.org/) to distribute read queries across replicas.
    2. Configure Langflow to use a single connection string pointing to the primary PostgreSQL database or proxy.

</TabItem>
<TabItem value="AA" label="Active-Active HA">

To implement Active-Active HA, you must deploy multiple Langflow instances, use load balancing to distribute traffic across the instances, and ensure all instances connect to the same HA PostgreSQL database:

1. Deploy multiple Langflow instances using Kubernetes or Docker Swarm.

    You must configure your instances to use a shared PostgreSQL database.
    For more information, see [Best practices for Langflow on Kubernetes](/deployment-prod-best-practices).

2. Set up streaming replication:

    1. Configure one primary database for writes.
    2. Configure one or more replicas for reads and failover.

        Select either synchronous or asynchronous replication based on your latency and consistency requirements.

3. Implement automatic failover using one of the following options:

    * Use an HA orchestrator, distributed configuration store, and traffic router, such as [Patroni](https://patroni.readthedocs.io/en/latest/), etcd or [Consul](https://developer.hashicorp.com/consul), and [HAProxy](https://www.haproxy.org/).
    * Use [Pgpool-II](https://www.pgpool.net/docs/46/en/html/index.html) alone or add supporting services for more robust HA support.
    * Use managed services that provide built-in HA with automatic failover, such as AWS RDS or Google Cloud SQL.

4. Update your PostgreSQL connection string to point to the HA setup.
Make sure all of your Langflow instances connect to the same HA PostgreSQL database.

    The connection string you use depends on your HA configuration and services:

    * Use a virtual IP or DNS name that resolves to the current primary database, such as `postgresql://langflow:securepassword@db-proxy:5432/langflow?sslmode=require`.
    * Use the provided endpoint for a managed service, such as `langflow.cluster-xyz.us-east-1.rds.amazonaws.com`.

5. Use a load balancer to distribute requests across your instances.

The following example configuration is for a production deployment that has three `langflow-runtime` replicas, uses the Kubernetes load balancer service to distribute traffic to healthy pods, and uses the HA PostgreSQL database connection string.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: langflow-runtime
spec:
  replicas: 3
  selector:
    matchLabels:
      app: langflow-runtime
  template:
    metadata:
      labels:
        app: langflow-runtime
    spec:
      containers:
      - name: langflow
        image: langflowai/langflow:latest
        ports:
        - containerPort: 7860
        env:
        - name: LANGFLOW_DATABASE_URL
          value: "postgresql://langflow:securepassword@db-proxy:5432/langflow?sslmode=require"
---
apiVersion: v1
kind: Service
metadata:
  name: langflow-runtime
spec:
  selector:
    app: langflow-runtime
  ports:
  - port: 80
    targetPort: 7860
  type: LoadBalancer
```

</TabItem>
</Tabs>

After implementing HA or Active-Active HA, monitor failover events and ensure replicas are in sync.
Langflow, through [SQLAlchemy](https://docs.sqlalchemy.org/en/20/), supports reconnection attempts if `LANGFLOW_DATABASE_CONNECTION_RETRY=True`, ensures recovery after failover, and reduces disruption once the database is back online.

Although PostgreSQL handles concurrent connections well, you must still monitor for contention, deadlocks, or other performance degradation during high load.

## Impact of database failure

If the PostgreSQL database becomes unavailable, the following Langflow functions will fail:

* **Flow Retrieval**: Cannot load new or existing flows from the database.
* **Flow Saving**: Unable to save new flows or updates to existing flows.
* **User Authentication**: Login and user management functions fail.
* **Project Collection Access**: Cannot access or share community/custom project collections.
* **Configuration Retrieval**: Unable to load application settings.
* **Configuration Updates**: Changes to settings cannot be saved.
* **Execution Log Access**: Cannot retrieve historical flow execution logs.
* **Log Writing**: New execution or system activity logs cannot be recorded.
* **Multi-User Collaboration**: Sharing flows or projects across users fails.
* **API Flow Loading**: API requests to load new flows (non-cached) fail.

Flows already loaded in memory may continue to function with cached configurations.
However, any operation requiring database access fails until the database is restored.
For example, a cached flow might run, but it won't record logs or message history to the database.

To minimize the possibility and impact of database failure, use [HA configurations](#high-availability-for-postgresql) and record backups regularly.
For example, you can use `pg_dump` to create logical backups or set up continuous archiving with write-ahead logs (WAL) for point-in-time recovery.
Test restoration procedures regularly to ensure your team understands how to execute them in a disaster recovery scenario.

## Database monitoring

Monitor your PostgreSQL database to ensure optimal performance and reliability:

* Use tools like pgAdmin, Prometheus with PostgreSQL exporter, or cloud-based monitoring for PostgreSQL.
* Track performance metrics such as CPU, memory, and disk I/O usage.
* Monitor replica health, availability, lag, and synchronization.
For example, use `pg_stat_activity` to monitor connection counts and contention.
* Set up alerts and notifications for high latency, failover events, or replication issues.
* Enable PostgreSQL logging, such as `log_connections` and `log_statements`, to track access and changes.

## See also

* [Configure an external PostgreSQL database](/configuration-custom-database)
* [Langflow architecture on Kubernetes](/deployment-architecture)
* [Deploy the Langflow production environment on Kubernetes](/deployment-kubernetes-prod)


================================================================================
filename: Develop/environment-variables.mdx
================================================================================

---
title: Environment variables
slug: /environment-variables
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import Link from '@docusaurus/Link';

In general, environment variables, like `LANGFLOW_PORT` or `LANGFLOW_LOG_LEVEL`, configure how Langflow runs.
These are broad settings that apply to your entire Langflow deployment.

In contrast, global variables are user-defined values stored in Langflow's database for use in flows, such as `OPENAI_API_KEY`.
Langflow can also source global variables from environment variables.
For more information, see [Langflow global variables](/configuration-global-variables).

## Configure environment variables for Langflow OSS

Langflow recognizes [supported environment variables](#supported-variables) from the following sources:

- Environment variables set in your terminal.
- Environment variables imported from a `.env` file when starting Langflow.
- Environment variables set with the [Langflow CLI](./configuration-cli), including the `--env-file` option and direct options, such as `--port`.

You can choose to use one or more of these sources.

### Precedence {#precedence}

If the same environment variable is set in multiple places, the following hierarchy applies:

1. Langflow CLI options override all other sources.
2. The `.env` file overrides system environment variables.
3. System environment variables are used only if not set elsewhere.
When running a Langflow Docker image, the `-e` flag can be used to set additional system environment variables.

For example:

* If you set `LANGFLOW_PORT=8080` in your system environment and `LANGFLOW_PORT=7860` in `.env`, Langflow uses `7860` from `.env`.
* If you use the Langflow CLI to run `langflow run --env-file .env --port 9000`, and you set `LANGFLOW_PORT=7860` in `.env`, then Langflow uses `9000` from the CLI option.

### Set environment variables in your terminal {#configure-variables-terminal}

Run the following commands to set environment variables for your current terminal session:

<Tabs>
<TabItem value="linux-macos" label="Linux or macOS" default>

```bash
export VARIABLE_NAME='VALUE'
```

</TabItem>
<TabItem value="windows" label="Windows">

```
set VARIABLE_NAME='VALUE'
```

</TabItem>
<TabItem value="docker" label="Docker">

```bash
docker run -it --rm \
    -p 7860:7860 \
    -e VARIABLE_NAME='VALUE' \
    langflowai/langflow:latest
```

</TabItem>
</Tabs>

When you start Langflow, it looks for environment variables that you've set in your terminal.
If it detects a supported environment variable, then it automatically adopts the specified value, subject to [precedence rules](#precedence).

### Import environment variables from a .env file {#configure-variables-env-file}

1. If Langflow is running, quit Langflow.

2. Create a `.env` file, and then open it in your preferred editor.

3. Define [Langflow environment variables](#supported-variables) in the `.env` file.

    <details>
    <summary>Example: .env</summary>

    ```text
    DO_NOT_TRACK=True
    LANGFLOW_AUTO_LOGIN=False
    LANGFLOW_AUTO_SAVING=True
    LANGFLOW_AUTO_SAVING_INTERVAL=1000
    LANGFLOW_BACKEND_ONLY=False
    LANGFLOW_BUNDLE_URLS=["https://github.com/user/repo/commit/hash"]
    LANGFLOW_CACHE_TYPE=async
    LANGFLOW_COMPONENTS_PATH=/path/to/components/
    LANGFLOW_CONFIG_DIR=/path/to/config/
    LANGFLOW_DATABASE_URL=postgresql://user:password@localhost:5432/langflow
    LANGFLOW_DEV=False
    LANGFLOW_FALLBACK_TO_ENV_VAR=False
    LANGFLOW_HEALTH_CHECK_MAX_RETRIES=5
    LANGFLOW_HOST=localhost
    LANGFLOW_LANGCHAIN_CACHE=InMemoryCache
    LANGFLOW_MAX_FILE_SIZE_UPLOAD=10000
    LANGFLOW_MAX_ITEMS_LENGTH=100
    LANGFLOW_MAX_TEXT_LENGTH=1000
    LANGFLOW_LOG_LEVEL=error
    LANGFLOW_OPEN_BROWSER=False
    LANGFLOW_PORT=7860
    LANGFLOW_REMOVE_API_KEYS=False
    LANGFLOW_SAVE_DB_IN_CONFIG_DIR=True
    LANGFLOW_SECRET_KEY=somesecretkey
    LANGFLOW_STORE_ENVIRONMENT_VARIABLES=True
    LANGFLOW_SUPERUSER=adminuser
    LANGFLOW_SUPERUSER_PASSWORD=adminpass
    LANGFLOW_WORKER_TIMEOUT=60000
    LANGFLOW_WORKERS=3
    ```

    For additional examples, see [`.env.example`](https://github.com/langflow-ai/langflow/blob/main/.env.example) in the Langflow repository.

    </details>

4. Save and close `.env`.

5. Start Langflow with your `.env` file:

    <Tabs>
    <TabItem value="local" label="Local" default>

    ```bash
    uv run langflow run --env-file .env
    ```

    </TabItem>
    <TabItem value="docker" label="Docker">

    ```bash
    docker run -it --rm \
        -p 7860:7860 \
        --env-file .env \
        langflowai/langflow:latest
    ```

    </TabItem>
    </Tabs>

    If your `.env` file isn't in the same directory, provide the path to your `.env` file.

On startup, Langflow imports the environment variables from your `.env` file, as well as any others that you set in your terminal, and then adopts their specified values.

### Configure environment variables for development

The following examples show how to configure Langflow using environment variables in different development scenarios.

<Tabs>
<TabItem value="env" label=".env file" default>

The `.env` file is a text file that contains key-value pairs of environment variables.

Create or edit a `.env` file in the root directory of your application or Langflow environment, and then add your configuration variables to the file:

<details>
<summary>Example: .env</summary>

```text title=".env"
DO_NOT_TRACK=True
LANGFLOW_AUTO_LOGIN=False
LANGFLOW_AUTO_SAVING=True
LANGFLOW_AUTO_SAVING_INTERVAL=1000
LANGFLOW_BACKEND_ONLY=False
LANGFLOW_BUNDLE_URLS=["https://github.com/user/repo/commit/hash"]
LANGFLOW_CACHE_TYPE=async
LANGFLOW_COMPONENTS_PATH=/path/to/components/
LANGFLOW_CONFIG_DIR=/path/to/config/
LANGFLOW_DATABASE_URL=postgresql://user:password@localhost:5432/langflow
LANGFLOW_DEV=False
LANGFLOW_FALLBACK_TO_ENV_VAR=False
LANGFLOW_HEALTH_CHECK_MAX_RETRIES=5
LANGFLOW_HOST=localhost
LANGFLOW_LANGCHAIN_CACHE=InMemoryCache
LANGFLOW_MAX_FILE_SIZE_UPLOAD=10000
LANGFLOW_MAX_ITEMS_LENGTH=100
LANGFLOW_MAX_TEXT_LENGTH=1000
LANGFLOW_LOG_LEVEL=error
LANGFLOW_OPEN_BROWSER=False
LANGFLOW_PORT=7860
LANGFLOW_REMOVE_API_KEYS=False
LANGFLOW_SAVE_DB_IN_CONFIG_DIR=True
LANGFLOW_SECRET_KEY=somesecretkey
LANGFLOW_STORE_ENVIRONMENT_VARIABLES=True
LANGFLOW_SUPERUSER=adminuser
LANGFLOW_SUPERUSER_PASSWORD=adminpass
LANGFLOW_WORKER_TIMEOUT=60000
LANGFLOW_WORKERS=3
```

</details>

</TabItem>
<TabItem value="systemd" label="Systemd service">

A systemd service configuration file configures Linux system services.

To add environment variables, create or edit a service configuration file and add an `override.conf` file. This file allows you to override the default environment variables for the service.

<details>
<summary>Example: override.conf</summary>

```ini title="override.conf"
[Service]
Environment="DO_NOT_TRACK=true"
Environment="LANGFLOW_AUTO_LOGIN=false"
Environment="LANGFLOW_AUTO_SAVING=true"
Environment="LANGFLOW_AUTO_SAVING_INTERVAL=1000"
Environment="LANGFLOW_BACKEND_ONLY=false"
Environment="LANGFLOW_BUNDLE_URLS=[\"https://github.com/user/repo/commit/hash\"]"
Environment="LANGFLOW_CACHE_TYPE=async"
Environment="LANGFLOW_COMPONENTS_PATH=/path/to/components/"
Environment="LANGFLOW_CONFIG_DIR=/path/to/config"
Environment="LANGFLOW_DATABASE_URL=postgresql://user:password@localhost:5432/langflow"
Environment="LANGFLOW_DEV=false"
Environment="LANGFLOW_FALLBACK_TO_ENV_VAR=false"
Environment="LANGFLOW_HEALTH_CHECK_MAX_RETRIES=5"
Environment="LANGFLOW_HOST=localhost"
Environment="LANGFLOW_LANGCHAIN_CACHE=InMemoryCache"
Environment="LANGFLOW_MAX_FILE_SIZE_UPLOAD=10000"
Environment="LANGFLOW_MAX_ITEMS_LENGTH=100"
Environment="LANGFLOW_MAX_TEXT_LENGTH=1000"
Environment="LANGFLOW_LOG_ENV=container_json"
Environment="LANGFLOW_LOG_FILE=logs/langflow.log"
Environment="LANGFLOW_LOG_LEVEL=error"
Environment="LANGFLOW_OPEN_BROWSER=false"
Environment="LANGFLOW_PORT=7860"
Environment="LANGFLOW_REMOVE_API_KEYS=false"
Environment="LANGFLOW_SAVE_DB_IN_CONFIG_DIR=true"
Environment="LANGFLOW_SECRET_KEY=somesecretkey"
Environment="LANGFLOW_STORE_ENVIRONMENT_VARIABLES=true"
Environment="LANGFLOW_SUPERUSER=adminuser"
Environment="LANGFLOW_SUPERUSER_PASSWORD=adminpass"
Environment="LANGFLOW_WORKER_TIMEOUT=60000"
Environment="LANGFLOW_WORKERS=3"
```

</details>

For more information on systemd, see the [Red Hat documentation](https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/9/html/using_systemd_unit_files_to_customize_and_optimize_your_system/assembly_working-with-systemd-unit-files_working-with-systemd).

</TabItem>
<TabItem value="vscode" label="VSCode tasks.json">

The `tasks.json` file located in `.vscode/tasks.json` is a configuration file for development environments using Visual Studio Code.

Create or edit the `.vscode/tasks.json` file in your project root.

<details>
<summary>Example: .vscode/tasks.json</summary>

```json title=".vscode/tasks.json"
{
    "version": "2.0.0",
    "options": {
        "env": {
            "DO_NOT_TRACK": "true",
            "LANGFLOW_AUTO_LOGIN": "false",
            "LANGFLOW_AUTO_SAVING": "true",
            "LANGFLOW_AUTO_SAVING_INTERVAL": "1000",
            "LANGFLOW_BACKEND_ONLY": "false",
            "LANGFLOW_BUNDLE_URLS": "[\"https://github.com/user/repo/commit/hash\"]",
            "LANGFLOW_CACHE_TYPE": "async",
            "LANGFLOW_COMPONENTS_PATH": "D:/path/to/components/",
            "LANGFLOW_CONFIG_DIR": "D:/path/to/config/",
            "LANGFLOW_DATABASE_URL": "postgresql://postgres:password@localhost:5432/langflow",
            "LANGFLOW_DEV": "false",
            "LANGFLOW_FALLBACK_TO_ENV_VAR": "false",
            "LANGFLOW_HEALTH_CHECK_MAX_RETRIES": "5",
            "LANGFLOW_HOST": "localhost",
            "LANGFLOW_LANGCHAIN_CACHE": "InMemoryCache",
            "LANGFLOW_MAX_FILE_SIZE_UPLOAD": "10000",
            "LANGFLOW_MAX_ITEMS_LENGTH": "100",
            "LANGFLOW_MAX_TEXT_LENGTH": "1000",
            "LANGFLOW_LOG_ENV": "container_csv",
            "LANGFLOW_LOG_FILE": "langflow.log",
            "LANGFLOW_LOG_LEVEL": "error",
            "LANGFLOW_OPEN_BROWSER": "false",
            "LANGFLOW_PORT": "7860",
            "LANGFLOW_REMOVE_API_KEYS": "true",
            "LANGFLOW_SAVE_DB_IN_CONFIG_DIR": "false",
            "LANGFLOW_SECRET_KEY": "somesecretkey",
            "LANGFLOW_STORE_ENVIRONMENT_VARIABLES": "true",
            "LANGFLOW_SUPERUSER": "adminuser",
            "LANGFLOW_SUPERUSER_PASSWORD": "adminpass",
            "LANGFLOW_WORKER_TIMEOUT": "60000",
            "LANGFLOW_WORKERS": "3"
        }
    },
    "tasks": [
        {
            "label": "langflow backend",
            "type": "shell",
            "command": ". ./langflownightly/Scripts/activate && langflow run",
            "isBackground": true,
            "problemMatcher": []
        }
    ]
}
```

</details>

To run Langflow using the above VSCode `tasks.json` file, in the VSCode command palette, select **Tasks: Run Task** > **langflow backend**.

</TabItem>
</Tabs>

## Set environment variables for Langflow Desktop

Environment variables set in your terminal aren't automatically available to GUI-based applications like Langflow Desktop when you launch them from the Windows or macOS GUI.

For Windows, this means any GUI-based app launched from the Start menu, desktop shortcuts, or Windows Explorer.

For macOS, this means any GUI-based app launched from Finder, Spotlight, Launchpad, or the Dock.

To set environment variables for Langflow Desktop, you need to use specific commands or files, depending on your OS.

<Tabs>
<TabItem value="macos" label="macOS" default>

Langflow Desktop for macOS cannot automatically use variables set in your terminal, such as those in`.zshrc` or `.bash_profile`, when launched from the macOS GUI.

To make environment variables available to GUI apps on macOS, you need to use `launchctl` with a `plist` file:

1. Create the `LaunchAgents` directory if it doesn't exist:

    ```bash
    mkdir -p ~/Library/LaunchAgents
    ```

2. In the `LaunchAgents` directory, create a `.plist` file called `dev.langflow.env`.

3. Add the following content to `dev.langflow.env.plist`, and then add, change, or remove Langflow environment variables as needed for your configuration.

    This example sets multiple environmental variables for all GUI apps launched from the macOS GUI.

    ```xml
     <?xml version="1.0" encoding="UTF-8"?>
     <!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
     <plist version="1.0">
     <dict>
       <key>Label</key>
       <string>dev.langflow.env</string>
       <key>ProgramArguments</key>
       <array>
         <string>/bin/sh</string>
         <string>-c</string>
          <string>
            launchctl setenv LANGFLOW_CONFIG_DIR /Users/your_user/custom/config &&
            launchctl setenv LANGFLOW_PORT 7860 &&
            launchctl setenv LANGFLOW_HOST localhost &&
            launchctl setenv ARIZE_API_KEY ak-...
          </string>
       </array>
       <key>RunAtLoad</key>
       <true/>
     </dict>
     </plist>
    ```
4. Load the file with `launchctl`:

    ```bash
    launchctl load ~/Library/LaunchAgents/dev.langflow.env.plist
    ```

</TabItem>
<TabItem value="windows1" label="Window System Properties">

Langflow Desktop for Windows cannot automatically use variables set in your terminal, such as those defined with `set` in `cmd` or `$env:VAR=...` in PowerShell, when launched from the Windows GUI.

To make environment variables available to the Langflow Desktop app, you must set them at the user or system level using the **System Properties** interface or the Terminal.

To set environment variables using the System Properties interface, do the following:

1. Press <kbd>Win + R</kbd>, enter `SystemPropertiesAdvanced`, and then press <kbd>Enter</kbd>.
2. Click **Environment Variables**.
3. Under **User variables**, click **New**.

    :::tip
    To apply the setting to all users, select **System variables**.
    :::

4. Enter the name of the Langflow variable you want to set, such as `LANGFLOW_CONFIG_DIR`, and the desired value, such as `C:\Users\your_user\.langflow_config`.
5. Click **OK** to save the variable.
6. Repeat until you have set all necessary Langflow environment variables.
7. Launch or restart Langflow Desktop to apply the environment variables.

</TabItem>
<TabItem value="windows2" label="Powershell">

To define environment variables for Windows using PowerShell, do the following:

1. Enter the name of the Langflow variable you want to set, such as `LANGFLOW_CONFIG_DIR`, and the desired value, such as `C:\Users\your_user\.langflow_config`.

    To set an environment variable for the current user:
    ```powershell
    [System.Environment]::SetEnvironmentVariable("LANGFLOW_CONFIG_DIR", "C:\Users\your_user\.langflow_config", "User")
    ```

   To set an environment variable for all users (you must have Administrator priveleges):
   ```powershell
   [System.Environment]::SetEnvironmentVariable("LANGFLOW_CONFIG_DIR", "C:\Langflow\Config", "Machine")
   ```

2. Repeat until you have set all necessary Langflow environment variables.
3. Launch or restart Langflow Desktop to apply the environment variables.

</TabItem>
</Tabs>

## Supported environment variables {#supported-variables}

The following sections provide information about specific Langflow environment variables.

### Authentication and security

See [API keys and authentication](/api-keys-and-authentication).

### Global variables

For information about the relationship between Langflow global variables and environment variables, as well as environment variables that control handling of global variables, see [Global variables](/configuration-global-variables).

### Logs {#logging}

See [Configure log options](/logging#log-storage).

### MCP servers {#mcp}

See [Use Langflow as an MCP server](/mcp-server).

### Monitoring and metrics

For environment variables for specific monitoring service providers, see the Langflow monitoring integration guides, such as [Langfuse](/integrations-langfuse) and [Best practices for Langflow on Kubernetes](/deployment-prod-best-practices).

### Server

The following environment variables set base Langflow server configuration, such as where the server is hosted, required files for SSL encryption, and the deployment type (frontend and backend, backend-only, development mode).

| Variable | Format | Default | Description |
|----------|--------|---------|-------------|
| `LANGFLOW_HOST` | String | `localhost` | The host on which the Langflow server will run. |
| `LANGFLOW_PORT` | Integer | `7860` | The port on which the Langflow server runs. The server automatically selects a free port if the specified port is in use. |
| `LANGFLOW_BACKEND_ONLY` | Boolean | `False` | Run only the Langflow backend service (no frontend). |
| `LANGFLOW_DEV` | Boolean | `False` | Whether to run Langflow in development mode (may contain bugs). |
| `LANGFLOW_OPEN_BROWSER` | Boolean | `False` | Open the system web browser on startup. |
| `LANGFLOW_HEALTH_CHECK_MAX_RETRIES` | Integer | `5` | Set the maximum number of retries for Langflow's server status health checks. |
| `LANGFLOW_WORKERS` | Integer | `1` | Number of worker processes. |
| `LANGFLOW_WORKER_TIMEOUT` | Integer | `300` | Worker timeout in seconds. |
| `LANGFLOW_SSL_CERT_FILE` | String | Not set | Path to the SSL certificate file for enabling HTTPS on the Langflow web server. This is separate from [database SSL connections](/configuration-custom-database#connect-langflow-to-a-local-postgresql-database). |
| `LANGFLOW_SSL_KEY_FILE` | String | Not set | Path to the SSL key file for enabling HTTPS on the Langflow web server. This is separate from [database SSL connections](/configuration-custom-database#connect-langflow-to-a-local-postgresql-database). |
| `LANGFLOW_DEACTIVATE_TRACING` | Boolean | `False` | Deactivate tracing functionality. |
| `LANGFLOW_CELERY_ENABLED` | Boolean | `False` | Enable Celery for distributed task processing. |

For more information about deploying Langflow servers, see [Langflow deployment overview](/deployment-overview).

### Storage

See [Memory management options](/memory) and [Manage files](/concepts-file-management).

### Telemetry

See [Telemetry](/contributing-telemetry).

### Visual editor and Playground behavior

| Variable | Format | Default | Description |
|----------|--------|---------|-------------|
| `LANGFLOW_AUTO_SAVING` | Boolean | `True` | Whether to automatically save flows. |
| `LANGFLOW_AUTO_SAVING_INTERVAL` | Integer | `1000` | Set the auto-save interval in milliseconds if `LANGFLOW_AUTO_SAVING=True`. |
| `LANGFLOW_BUNDLE_URLS` | List[String] | `[]` | A list of URLs from which to load custom bundles. Supports GitHub URLs. If `LANGFLOW_AUTO_LOGIN=True`, flows from these bundles are loaded into the database. |
| `LANGFLOW_COMPONENTS_PATH` | String | Not set | Path to a directory containing custom components. Typically used if you have local custom components or you are building a Docker image with custom components. |
| `LANGFLOW_LOAD_FLOWS_PATH` | String | Not set | Path to a directory containing flow JSON files to be loaded on startup. Typically used when creating a Docker image with prepackaged flows. Requires `LANGFLOW_AUTO_LOGIN=True`. |
| `LANGFLOW_CREATE_STARTER_PROJECTS` | Boolean | `True` | Whether to create templates during initialization. If `false`, Langflow doesn't create templates, and `LANGFLOW_UPDATE_STARTER_PROJECTS` is treated as `false`. |
| `LANGFLOW_UPDATE_STARTER_PROJECTS` | Boolean | `True` | Whether to update templates with the latest component versions when initializing after an upgrade. |
| `LANGFLOW_LAZY_LOAD_COMPONENTS` | Boolean | `False` | If `true`, Langflow only partially loads components at startup and fully loads them on demand. This significantly reduces startup time but can cause a slight delay when a component is first used. |
| `LANGFLOW_EVENT_DELIVERY` | String | `streaming` | How to deliver build events to the frontend: `polling`, `streaming` or `direct`. |
| `LANGFLOW_FRONTEND_PATH` | String | `./frontend` | Path to the frontend directory containing build files. For development purposes only when you need to serve specific frontend code. |
| `LANGFLOW_MAX_ITEMS_LENGTH` | Integer | `100` | Maximum number of items to store and display in the visual editor. Lists longer than this will be truncated when displayed in the visual editor. Doesn't affect outputs or data passed between components. |
| `LANGFLOW_MAX_TEXT_LENGTH` | Integer | `1000` | Maximum number of characters to store and display in the visual editor. Responses longer than this will be truncated when displayed in the visual editor. Doesn't truncate outputs or responses passed between components. |
| `LANGFLOW_MAX_TRANSACTIONS_TO_KEEP` | Integer | `3000` | Maximum number of flow transaction events to keep in the database. |
| `LANGFLOW_MAX_VERTEX_BUILDS_TO_KEEP` | Integer | `3000` | Maximum number of vertex builds to keep in the database. Relates to [Playground](/concepts-playground) functionality. |
| `LANGFLOW_MAX_VERTEX_BUILDS_PER_VERTEX` | Integer | `2` | Maximum number of builds to keep per vertex. Older builds are deleted. Relates to [Playground](/concepts-playground) functionality. |
| `LANGFLOW_PUBLIC_FLOW_CLEANUP_INTERVAL` | Integer | `3600` | The interval in seconds at which data for [shared Playground](/concepts-playground#share-a-flows-playground) flows are cleaned up. Default: 3600 seconds (1 hour). Minimum: 600 seconds (10 minutes). |
| `LANGFLOW_PUBLIC_FLOW_EXPIRATION` | Integer | `86400` | The time in seconds after which a [shared Playground](/concepts-playground#share-a-flows-playground) flow is considered expired and eligible for cleanup. Default: 86400 seconds (24 hours). Minimum: 600 seconds (10 minutes). |


================================================================================
filename: Develop/install-custom-dependencies.mdx
================================================================================

---
title: Install custom dependencies
slug: /install-custom-dependencies
---

Langflow provides optional dependency groups and support for custom dependencies to extend Langflow functionality. This guide covers how to add dependencies for different Langflow installations, including Langflow Desktop and Langflow OSS.

The Langflow codebase uses two `pyproject.toml` files to manage dependencies, with one for `base` and one for `main`:

* The `main` package is managed by the root level `pyproject.toml`, and it includes end-user features and main application code, such as Langchain and OpenAI.
* The `base` package is managed at `src/backend/base/pyproject.toml`, and it includes core infrastructure, such as the FastAPI web framework.

## Install custom dependencies in Langflow Desktop {#langflow-desktop}

To add dependencies to Langflow Desktop, add an entry for the package to the application's `requirements.txt` file:

    * On macOS, the file is located at `/Users/USER/.langflow/data/requirements.txt`.
    * On Windows, the file is located at `C:\Users\USER\AppData\Roaming\com.Langflow\data\requirements.txt`.

Add each dependency to `requirements.txt` on its own line in the format `DEPENDENCY==VERSION`, such as `matplotlib==3.10.0`.

Restart Langflow Desktop to install the dependencies.

If you need to change or uninstall custom dependencies, edit the `requirements.txt` file, and then restart Langflow Desktop.

## Install custom dependencies in Langflow OSS

To install your own custom dependencies in your Langflow environment, add them with your package manager.

If you're working within a cloned Langflow repository, add dependencies with `uv add` because there is already a `pyproject.toml` file for uv to reference:

```bash
uv add DEPENDENCY
```

### Install optional dependency groups

Langflow OSS provides optional dependency groups that extend its functionality.

These dependencies are listed in the [pyproject.toml](https://github.com/langflow-ai/langflow/blob/main/pyproject.toml#L194) file under `[project.optional-dependencies]`.

Install dependency groups using pip's `[extras]` syntax. For example, to install Langflow with the `postgresql` dependency group, enter the following command:

```bash
uv pip install "langflow[postgresql]"
```

To install multiple extras, use commas to separate each dependency group:

```bash
uv pip install "langflow[local,postgresql]"
```

### Use a virtual environment to test custom dependencies

When testing locally, use a virtual environment to isolate your dependencies and prevent conflicts with other Python projects.

For example, if you want to experiment with `matplotlib` with Langflow:

```bash
# Create and activate a virtual environment
uv venv YOUR_LANGFLOW_VENV
source YOUR_LANGFLOW_VENV/bin/activate

# Install langflow and your additional dependency
uv pip install langflow matplotlib
```

If you're working within a cloned Langflow repository, add dependencies with `uv add` to reference the existing `pyproject.toml` files:

```bash
uv add matplotlib
```

The `uv add` commands automatically update the `uv.lock` file in the appropriate location.

## Add dependencies to the Langflow codebase

When contributing to the Langflow codebase, you might need to add dependencies to Langflow.

Langflow uses a workspace with two packages, each with different types of dependencies.

To add a dependency to the `main` package, run `uv add DEPENDENCY` from the project root.
For example:

```bash
uv add matplotlib
```

Dependencies can be added to the `main` package as regular dependencies at `[project.dependencies]` or optional dependencies at `[project.optional-dependencies]`.

To add a dependency to the `base` package, navigate to `src/backend/base` and run:
```bash
cd src/backend/base && uv add DEPENDENCY
```

To add a development dependency for testing, linting, or debugging, navigate to `src/backend/base` and run:
```bash
cd src/backend/base && uv add --group dev DEPENDENCY
```

Dependencies can be added to the `base` package as regular dependencies at `[project.dependencies]`, development dependencies at `[dependency-groups.dev]`, or optional dependencies at `[project.optional-dependencies]`.

You can optionally use `make add` instead of `uv add`:

```bash
# Equivalent to: uv add matplotlib
make add main="matplotlib"

# Equivalent to: cd src/backend/base && uv add --group dev matplotlib
make add devel="matplotlib"

# Equivalent to: cd src/backend/base && uv add matplotlib
make add base="matplotlib"
```

Alternatively, you can add these dependencies manually to the appropriate `pyproject.toml` file:

```
[project]
dependencies = [
    "matplotlib>=3.8.0"
]
```

Or as an optional dependency in the main package:

```
[project.optional-dependencies]
plotting = [
    "matplotlib>=3.8.0",
]
```

Or as a development dependency in the base package:

```
[dependency-groups]
dev = [
    "matplotlib>=3.8.0",
]
```

## See also

* [Containerize a Langflow application](/develop-application)
* [Create custom Python components](/components-custom-components)


================================================================================
filename: Develop/integrations-arize.mdx
================================================================================

---
title: Arize
slug: /integrations-arize
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

Arize is a tool built on [OpenTelemetry](https://opentelemetry.io/) and [OpenInference](https://docs.arize.com/phoenix/reference/open-inference) for monitoring and optimizing LLM applications.

To enable Arize tracing, set the required Arize environment variables in your Langflow deployment.
Arize begins monitoring and collecting telemetry data from your LLM applications automatically.

:::tip
Instructions for integrating Langflow and Arize are also available in the Arize documentation:

* [Langflow tracing with Arize Platform](https://arize.com/docs/ax/integrations/frameworks-and-platforms/langflow/langflow-tracing)
* [Langflow tracing with Arize Phoenix](https://arize.com/docs/phoenix/integrations/langflow/langflow-tracing)
:::

## Prerequisites

* If you are using the [standard Arize platform](https://docs.arize.com/arize), you need an **Arize Space ID** and **Arize API Key**.
* If you are using the open-source [Arize Phoenix platform](https://docs.arize.com/phoenix), you need an **Arize Phoenix API key**.

## Connect Arize to Langflow

<Tabs>
<TabItem value="platform" label="Arize Platform" default>

1. In your [Arize dashboard](https://app.arize.com/), copy your **Space ID** and [**API Key (Ingestion Service Account Key)**](https://arize.com/docs/ax/security-and-settings/api-keys).

2. In the root of your Langflow application, edit your existing Langflow `.env` file or create a new one.

3. Add `ARIZE_SPACE_ID` and `ARIZE_API_KEY` environment variables:

    ```bash
    ARIZE_SPACE_ID=SPACE_ID
    ARIZE_API_KEY=API_KEY
    ```

    Replace `SPACE_ID` and `API_KEY` with the values you copied from the Arize platform.

    You don't need to specify the Arize project name if you're using the standard Arize platform.

4. Start your Langflow application with your `.env` file:

    ```bash
    uv run langflow run --env-file .env
    ```

</TabItem>
<TabItem value="phoenix" label="Arize Phoenix">

1. In your [Arize Phoenix dashboard](https://app.phoenix.arize.com/), copy your **API Key**.

2. In the root of your Langflow application, edit your existing Langflow `.env` file or create a new one.

3. Add a `PHOENIX_API_KEY` environment variable:

    ```bash
    PHOENIX_API_KEY=API_KEY
    ```

    Replace `API_KEY` with the Arize Phoenix API key that you copied from the Arize Phoenix platform.

4. Start your Langflow application with your `.env` file:

    ```bash
    uv run langflow run --env-file .env
    ```

</TabItem>
</Tabs>

## Run a flow and view metrics in Arize

1. In Langflow, run a flow that has an LLM-driven component, such as an **Agent** component or any language model component.
You must chat with the flow or trigger the LLM to produce traffic for Arize to trace.

    For example, you can create a flow with the **Simple Agent** template, add your OpenAI API key to the **Agent** component, and then click **Playground** to chat with the flow and generate traffic.

2. In Arize, open your project dashboard, and then wait for Arize to process the data.
This can take a few minutes.

3. To view metrics for your flows, go to the **LLM Tracing** tab.

    Each Langflow execution generates two traces in Arize:

    * The `AgentExecutor` trace is the Arize trace of LangChain's `AgentExecutor`.
    * The `UUID` trace is the trace of the Langflow components.

4. To view traces, go to the **Traces** tab.

    A _trace_ is the complete journey of a request, made of multiple _spans_.

5. To view spans, go to the **Spans** tab.

    A _span_ is a single operation within a trace.
    For example, a _span_ could be a single API call to OpenAI or a single function call to a custom tool.

    For information about tracing metrics in Arize, see the [Arize LLM tracing documentation](https://docs.arize.com/arize/llm-tracing/tracing).

6. To add a span to a [dataset](https://docs.arize.com/arize/llm-datasets-and-experiments/datasets-and-experiments), click **Add to Dataset**.

    All metrics on the **LLM Tracing** tab can be added to datasets.

7. To view a dataset, click the **Datasets** tab, and then select your dataset.


================================================================================
filename: Develop/integrations-instana-traceloop.mdx
================================================================================

---
title: Traceloop
slug: /integrations-instana-traceloop
description: Instrument Langflow with the Traceloop SDK, and export traces and metrics to Instana using OpenTelemetry.
---

Traceloop SDK is a lightweight instrumentation toolkit designed for LLM applications.
It enables developers to automatically capture and export traces, metrics, and key observability signals from their LLM-powered workflows.

When combined with Instana, the exported telemetry data from Traceloop provides end-to-end visibility, allowing users to visualize traces, analyze performance bottlenecks, and ensure reliable operation of LLM-driven applications.

This guide demonstrates how to integrate the Instana observability platform with your Langflow application using the Traceloop SDK so you can monitor and analyze LLM performance.

## Prerequisites

- Create a [Traceloop API key](https://app.traceloop.com/settings/api-key)
- Create an [Instana endpoint and Instana key](https://www.ibm.com/docs/en/instana-observability/1.0.302)
- [Install Langflow](/get-started-installation)

## Configure environment variables

1. In the root folder of your Langflow application, edit your existing Langflow `.env` file or create a new one.

2. Enter the following environment variables, and then replace the placeholders with the values for your deployment or requirements:

   ```text
   TRACELOOP_API_KEY=tl_dummy_1234567890abcdef1234567890abcdef
   TRACELOOP_BASE_URL=https://otlp-magenta-saas.instana.rocks:4318
   TRACELOOP_HEADERS="x-instana-key=INSTANA_KEY"
   OTEL_EXPORTER_OTLP_INSECURE=false
   TRACELOOP_METRICS_ENDPOINT=HOST:8000
   TRACELOOP_METRICS_ENABLED=true
   OTEL_METRIC_EXPORT_INTERVAL=10000
   ```

   Set the necessary values for each environment variable:

   - **`TRACELOOP_API_KEY`**: A Traceloop API key to authenticate your application with Traceloop's monitoring service.
   You can get this from your Traceloop account dashboard.

      This integration uses the Traceloop SDK for instrumentation, which requires a Traceloop API key to initialize properly.
      If you don't have a Traceloop API key, you can proceed with the placeholder API key in the preceding example.

   - **`TRACELOOP_BASE_URL`**: The Instana endpoint URL for telemetry data collection, which is your Instana backend endpoint, such as `https://otlp-magenta-saas.instana.rocks:4318`.
   You can get this from your Instana configuration or by contacting your Instana administrator.

   - **`TRACELOOP_HEADERS`**: Authentication headers for Instana data collection. Set this to `"x-instana-key=INSTANA_KEY"`, replacing `INSTANA_KEY` with the Instana key from your Instana setup.

   - **`OTEL_EXPORTER_OTLP_INSECURE`**: Security setting for OpenTelemetry Protocol connections. Set to `false` for secure HTTPS/TLS connections. This is recommended for production Instana SaaS endpoints. Set to `true` for insecure HTTP connections during local development.

   - **`TRACELOOP_METRICS_ENDPOINT`**: Separate metrics endpoint configuration, in the form of `OTEL_DC_LLM_HOST:8000`. Typically set to `host.docker.internal:8000` for Docker environments. Adjust the host and port based on your deployment setup.

   - **`TRACELOOP_METRICS_ENABLED`**: Boolean to enable metrics collection. Set to `true` to activate metrics gathering.

   - **`OTEL_METRIC_EXPORT_INTERVAL`**: Interval in milliseconds for metrics export. Set to `10000` for 10-second export intervals, or adjust based on your monitoring requirements.

3. Make sure the OpenTelemetry Data Collector (OTel DC) is running and correctly configured.
Open your Collector's `config.yaml` file, enter the following configuration, and then replace the placeholder values with the values from your data collector setup or requirements:

   ```yaml
   llm.application: "LLM_DC"
   instances:
      -  otel.agentless.mode: true
         # Example endpoint: https://otlp-magenta-saas.instana.rocks:4318
         otel.backend.url: "INSTANA_ENDPOINT"
         otel.backend.using.http: false
         callback.interval: 10
         otel.service.name: "DC1"
         otel.service.port: 8000
         currency: "USD"
   ```

   This configuration enables the OTel Collector to operate in agentless mode and route telemetry data to your Instana backend with proper service identification and collection intervals for effective monitoring integration.

## Start Langflow with Traceloop environment variables

Launch your Langflow application with your `.env` file:

```bash
uv run langflow run --env-file .env
```

Traceloop automatically begins monitoring and collecting telemetry data from your LLM applications.

## Verify the integration

To verify that observability is working correctly:

1. Run a flow in Langflow to generate traffic.
2. To view traces in Instana, open Instana and click **Applications**.
3. In **Services**, search for `Langflow`.
4. Click **Langflow** to view and analyze the associated calls.

   ![Instana Traces dashboard](/img/instana-traces-dashboard.png)

5. To view metrics in Instana, open Instana and click **Infrastructure**.
6. In **Analyze Infrastructure**, click **Otel LLMonitor**.
7. To view your Metrics dashboard, click `LLM:DC1@your_machine_name.local`.

   ![Instana Metrics dashboard](/img/instana-metrics-dashboard.png)

## See also

* [Traceloop documentation](https://www.traceloop.com/docs/introduction)
* [Instana setup documentation](https://www.ibm.com/docs/en/instana-observability/1.0.300?topic=started-instana-setup)
* [Otel DC setup documentation](https://www.ibm.com/docs/en/instana-observability/1.0.300?topic=started-install-otel-data-collector-llm-odcl)


================================================================================
filename: Develop/integrations-langfuse.mdx
================================================================================

---
title: Langfuse
slug: /integrations-langfuse
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

[Langfuse](https://langfuse.com) is an open-source platform for LLM observability. It provides tracing and monitoring capabilities for AI applications, helping developers debug, analyze, and optimize their AI systems. Langfuse integrates with various tools and frameworks, including workflow builders and runtimes like Langflow.

This guide explains how to configure Langflow to collect [tracing](https://langfuse.com/docs/tracing) data about your flow executions and automatically send the data to Langfuse.

<iframe width="760" height="415" src="https://www.youtube.com/embed/SA9gGbzwNGU?si=eDKvdtvhb3fJCSbl" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

## Prerequisites

- An account in a [Langfuse Cloud](https://cloud.langfuse.com) or [Langfuse self-hosted](https://langfuse.com/self-hosting) instance
- A [running Langflow server](/get-started-installation) with a [flow](/concepts-flows) that you want to trace

:::tip
If you need a flow to test the Langfuse integration, see the [Langflow quickstart](/get-started-quickstart).
:::

## Set Langfuse credentials as environment variables {#langfuse-credentials}

1. Create a set of [Langfuse API keys](https://langfuse.com/faq/all/where-are-langfuse-api-keys).

2. Copy the following API key information:

    - Secret Key
    - Public Key
    - Host URL

3. Set your Langfuse project credentials as environment variables in the same environment where you run Langflow.

    In the following examples, replace `SECRET_KEY`, `PUBLIC_KEY`, and `HOST_URL` with your API key details from Langfuse.

    <Tabs>
    <TabItem value="linux-macos" label="Linux or macOS" default>

    These commands set the environment variables in a Linux or macOS terminal session:

    ```
    export LANGFUSE_SECRET_KEY=SECRET_KEY
    export LANGFUSE_PUBLIC_KEY=PUBLIC_KEY
    export LANGFUSE_HOST=HOST_URL
    ```

    </TabItem>
    <TabItem value="windows" label="Windows">

    These commands set the environment variables in a Windows command prompt session:

    ```
    set LANGFUSE_SECRET_KEY=SECRET_KEY
    set LANGFUSE_PUBLIC_KEY=PUBLIC_KEY
    set LANGFUSE_HOST=HOST_URL
    ```

    </TabItem>
    </Tabs>

## Start Langflow and view traces in Langfuse

1. Start Langflow in the same environment where you set the Langfuse environment variables:

    ```bash
    uv run langflow run
    ```

2. Run a flow.

    Langflow automatically collects and sends tracing data about the flow execution to Langfuse.

3. View the collected data in your [Langfuse dashboard](https://langfuse.com/docs/analytics/overview).

    Langfuse also provides a [public live trace example dashboard](https://cloud.langfuse.com/project/cm0nywmaa005c3ol2msoisiho/traces/f016ae6d-4527-43f5-93ba-9d78388cd3d9).

## Disable Langfuse tracing

To disable the Langfuse integration, remove the [Langfuse environment variables](#langfuse-credentials), and then restart Langflow.

## Run Langfuse and Langflow with Docker Compose

As an alternative to the previous setup, particularly for self-hosted Langfuse, you can run both services with Docker Compose.

1. Create a set of [Langfuse API keys](https://langfuse.com/faq/all/where-are-langfuse-api-keys).

2. Copy the following API key information:

    - Secret Key
    - Public Key
    - Host URL

3. Add your Langflow credentials to your Langflow `docker-compose.yml` file in the `environment` section.

    The following example is based on the [example `docker-compose.yml`](https://github.com/langflow-ai/langflow/blob/main/docker_example/docker-compose.yml).

    ```yml
    services:
      langflow:
        image: langflowai/langflow:latest # or another version tag on https://hub.docker.com/r/langflowai/langflow
        pull_policy: always               # set to 'always' when using 'latest' image
        ports:
          - "7860:7860"
        depends_on:
          - postgres
        environment:
          - LANGFLOW_DATABASE_URL=postgresql://langflow:langflow@postgres:5432/langflow
          # This variable defines where the logs, file storage, monitor data and secret keys are stored.
          - LANGFLOW_CONFIG_DIR=app/langflow
          - LANGFUSE_SECRET_KEY=sk-...
          - LANGFUSE_PUBLIC_KEY=pk-...
          - LANGFUSE_HOST=https://us.cloud.langfuse.com
        volumes:
          - langflow-data:/app/langflow

      postgres:
        image: postgres:16
        environment:
          POSTGRES_USER: langflow
          POSTGRES_PASSWORD: langflow
          POSTGRES_DB: langflow
        ports:
          - "5432:5432"
        volumes:
          - langflow-postgres:/var/lib/postgresql/data

    volumes:
      langflow-postgres:
      langflow-data:
    ```

4. Start the Docker container:

    ```text
    docker-compose up
    ```

5. To confirm Langfuse is connected to your Langflow container, run the following command:

    ```sh
    docker compose exec langflow python -c "import requests, os; addr = os.environ.get('LANGFUSE_HOST'); print(addr); res = requests.get(addr, timeout=5); print(res.status_code)"
    ```

    If there is an error, make sure you have set the `LANGFUSE_HOST` environment variable in your terminal session.

    Output similar to the following indicates success:

    ```text
    https://us.cloud.langfuse.com
    200
    ```

## See also

* [Langfuse GitHub repository](https://github.com/langfuse/langfuse)


================================================================================
filename: Develop/integrations-langsmith.mdx
================================================================================

---
title: LangSmith
slug: /integrations-langsmith
---

LangSmith is a full-lifecycle DevOps service from LangChain that provides monitoring and observability. To integrate with Langflow, add your LangChain API key and configuration as Langflow environment variables, and then start Langflow.

1. Obtain your LangChain API key from [https://smith.langchain.com](https://smith.langchain.com/)
2. Set the following environment variables in your Langflow `.env` file, replacing `LANGCHAIN_API_KEY` and `LANGSMITH_PROJECT_NAME` with your own values:

    ```text
    LANGSMITH_TRACING=True
    LANGSMITH_ENDPOINT=https://api.smith.langchain.com/
    LANGSMITH_API_KEY=LANGCHAIN_API_KEY
    LANGSMITH_PROJECT=LANGSMITH_PROJECT_NAME
    ```

    Alternatively, you can export the environment variables in your terminal instead of adding them to the `.env` file:

    ```bash
    export LANGSMITH_TRACING=True && export LANGSMITH_ENDPOINT="https://api.smith.langchain.com/" && export LANGSMITH_API_KEY="LANGCHAIN_API_KEY" && export LANGSMITH_PROJECT="LANGSMITH_PROJECT_NAME"
    ```

3. Restart Langflow with your modified `.env` file or from the terminal where you set your environment variables:

    ```bash
    langflow run --env-file .env
    ```

    If you set the environment variables in your terminal, you can omit `--env-file`.
    However, Langflow can source environment variables from `.env` _and_ your terminal.
    For more information, see [Langflow environment variables](/environment-variables).

4. Run a flow in Langflow to generate some activity.

5. View the LangSmith dashboard for monitoring and observability.

    ![LangSmith dashboard](/img/langsmith-dashboard.png)


================================================================================
filename: Develop/integrations-langwatch.mdx
================================================================================

---
title: LangWatch
slug: /integrations-langwatch
---

[LangWatch](https://app.langwatch.ai/) is an all-in-one LLMOps platform for monitoring, observability, analytics, evaluations and alerting for getting user insights and improve your LLM workflows.

## Integrate LangWatch observability

To integrate with Langflow, add your LangWatch API key as a Langflow environment variable:

1. Get a LangWatch API key from your LangWatch account.

2. Add the key to your Langflow `.env` file:

    ```shell
    LANGWATCH_API_KEY="API_KEY_STRING"
    ```

    Alternatively, you can set the environment variable in your terminal session:

    ```shell
    export LANGWATCH_API_KEY="API_KEY_STRING"
    ```

3. Restart Langflow with your `.env` file, if you modified the Langflow `.env`:

    ```
    langflow run --env-file .env
    ```

4. Run a flow.

5. View the LangWatch dashboard for monitoring and observability.

![LangWatch dashboard](/img/langwatch-dashboard.png)

## Use the LangWatch Evaluator

In your flows, you can use the **LangWatch Evaluator** component to use LangWatch's evaluation endpoints to assess a model's performance.
This component is available in the **LangWatch** [bundle](/components-bundle-components).


================================================================================
filename: Develop/integrations-opik.mdx
================================================================================

---
title: Opik
slug: /integrations-opik
---

[Opik](https://www.comet.com/site/products/opik/) is an open-source platform designed for evaluating, testing, and monitoring large language model (LLM) applications. Developed by Comet, it aims to facilitate more intuitive collaboration, testing, and monitoring of LLM-based applications.

You can configure Langflow to collect [tracing](https://www.comet.com/docs/opik/tracing/log_traces) data about your flow executions and automatically send the data to Opik.

## Prerequisites

- An [Open-Source Opik server or an Opik Cloud account](https://www.comet.com/docs/opik/faq#what-is-the-difference-between-opik-cloud-and-the-open-source-opik-platform-)
- A [running Langflow server](/get-started-installation) with a [flow](/concepts-flows) that you want to trace

:::tip
If you need a flow to test the Opik integration, see the [Langflow quickstart](/get-started-quickstart).
:::

## Integrate Opik with Langflow

1. If you use Opik Cloud, get an [Opik API key](https://www.comet.com/docs/opik/faq#where-can-i-find-my-opik-api-key-).

    An API key isn't required with an Open-Source Opik server.

2. Call the `opik configure` CLI to save your Opik configuration in the same environment where you run Langflow:

    ```bash
    opik configure
    ```

    For self-hosted Opik, you can also use the following Opik CLI command:

    ```bash
    opik configure --use_local
    ```

    For more information, see the [Opik SDK configuration documentation](https://www.comet.com/docs/opik/tracing/sdk_configuration).

3. Start Langflow in the same terminal or environment where you set the environment variables:

    ```bash
    uv run langflow run
    ```

4. In Langflow, run a flow to produce activity for Opik to trace.

5. Navigate to your Opik project dashboard and view the collected tracing data.

## Disable the Opik integration

To disable the Opik integration, remove the environment variables you set with `opik configure`, and then restart Langflow.


================================================================================
filename: Develop/logging.mdx
================================================================================

---
title: Logs
slug: /logging
---

import Icon from "@site/src/components/icon";
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

Langflow produces logs for individual flows and the Langflow application itself using the [structlog](https://www.structlog.org) library for logging.

The default, primary logfile is named `langflow.log`.

Log files are stored in JSON format with structured metadata.

## Log storage

Langflow logs are stored in the config directory specified in the `LANGFLOW_CONFIG_DIR` environment variable.
The default config directory location depends on your operating system and installation method:

- **Langflow Desktop**:

    - **macOS**: `/Users/<username>/Library/Logs/com.LangflowDesktop`
    - **Windows**: `C:\Users\<username>\AppData\Local\com.LangflowDesktop\logs`

- **OSS Langflow**:

    - **macOS with `uv pip install`**: `/Users/<username>/Library/Caches/langflow`
    - **Linux with `uv pip install`**: `/home/<username>/.cache/langflow`
    - **Windows/WSL with `uv pip install`**: `C:\Users\<username>\AppData\Local\langflow\langflow\Cache`
    - **macOS/Windows/Linux/WSL with `git clone`**: `<path_to_clone>/src/backend/base/langflow/`

To customize log storage locations and behaviors, set the following [Langflow environment variables](/environment-variables) in your Langflow `.env` file, and then start Langflow with `uv run langflow run --env-file .env`:

| Variable | Format | Default | Description |
|----------|--------|---------|-------------|
| `LANGFLOW_CONFIG_DIR` | String | Varies | Set the Langflow configuration directory where files and logs are stored. Default path depends on your installation, as described in the preceding list. |
| `LANGFLOW_LOG_LEVEL` | String | `ERROR` | Sets the log level as one of `DEBUG`, `ERROR`, `INFO`, `WARNING`, and `CRITICAL`. For example, `LANGFLOW_LOG_LEVEL=DEBUG`. |
| `LANGFLOW_LOG_FILE` | String | Not set | Sets the log file storage location if you want to use a non-default location. For example, `LANGFLOW_LOG_FILE=path/to/logfile.log`. If not set, logs are written to stdout. |
| `LANGFLOW_LOG_ENV` | String | `default` | This variable is the primary log format controller. `container`: JSON format for Docker/structured logging. `container_csv`: Key-value format for data analysis. `default` or unset: Uses `LANGFLOW_PRETTY_LOGS` to determine format. |
| `LANGFLOW_PRETTY_LOGS` | Boolean | `True` | This variable controls log output format when `LANGFLOW_LOG_ENV=default` or unset. When `true`, uses structlog's [ConsoleRenderer](https://www.structlog.org/en/stable/console-output.html). When `false`, outputs logs in JSON format.  |
| `LANGFLOW_LOG_FORMAT` | String | Not set | Switch between key-value format and console format. Set to `key_value` for key-value format or `console` to use structlog's [ConsoleRenderer](https://www.structlog.org/en/stable/console-output.html). This variable only works when `LANGFLOW_LOG_ENV=default` and `LANGFLOW_PRETTY_LOGS=true`. |
| `LANGFLOW_LOG_ROTATION` | String | `1 day` | Controls when the log file is rotated, either based on time or file size. For time-based rotation, set to `1 day`, `12 hours`, or `1 week`. For size-based rotation, set to `10 MB` or `1 GB`. To disable rotation, set to `None`. If disabled, log files grow without limit. |
| `LANGFLOW_ENABLE_LOG_RETRIEVAL` | Boolean | `False` | Enables retrieval of logs from your Langflow instance with [Logs endpoints](/api-logs). |
| `LANGFLOW_LOG_RETRIEVER_BUFFER_SIZE` | Integer | `10000` | Set the buffer size for log retrieval if `LANGFLOW_ENABLE_LOG_RETRIEVAL=True`. Must be greater than `0` for log retrieval to function. |

## View logs in real-time

To monitor Langflow logs as they are generated, you can follow the log file:

1. Change to your [Langflow config directory](#log-storage):

    <Tabs groupId="cd-command">
    <TabItem value="macOS" label="macOS" default>

    ```bash
    cd /Users/**USERNAME**/Library/Caches/langflow
    ```

    </TabItem>
    <TabItem value="Windows" label="Windows">

    ```cmd
    cd C:\Users\**USERNAME**\AppData\Local\com.LangflowDesktop\logs
    ```

    </TabItem>
    </Tabs>

2. Tail the main log file:

    <Tabs groupId="tail-command">
    <TabItem value="macOS" label="macOS" default>

    ```bash
    tail -f langflow.log
    ```

    </TabItem>
    <TabItem value="Windows" label="Windows">

    ```cmd
    Get-Content -Wait -Path langflow.log
    ```

    </TabItem>
    </Tabs>

    If you don't see new log entries, check that Langflow is running, and perform some actions to generate logs events. You can also check the terminal where you started Langflow to see if logs are being printed there.

## Flow and component logs

After you run a flow, you can inspect the logs for the each component and flow run.
For example, you can inspect `Message` objects ingested and generated by [Input and Output components](/components-io).

### View flow logs

In the visual editor, click **Logs** to view logs for the entire flow:

![Logs pane](/img/logs.png)

Then, click the cells in the **inputs** and **outputs** columns to inspect the `Message` objects.
For example, the following `Message` data could be the output from a **Chat Input** component:

```text
    "messages": [
    {
        "message": "What's the recommended way to install Docker on Mac M1?",
        "sender": "User",
        "sender_name": "User",
        "session_id": "Session Apr 21, 17:37:04",
        "stream_url": null,
        "component_id": "ChatInput-4WKag",
        "files": [],
        "type": "text"
    }
    ],
```

In the case of Input/Output components, the original input might not be structured as a `Message` object.
For example, a language model component can pass a raw text response to a **Chat Output** component that is then transformed into a `Message` object.

You can find `.log` files for flows at your Langflow installation's log storage location.
For filepaths, see [Log storage](#log-storage).

### View chat logs

In the **Playground**, you can inspect the chat history for each chat session.
For more information, see [View chat history](/concepts-playground#view-chat-history).

### View output from a single component

When debugging issues with the format or content of a flow's output, it can help to inspect each component's output to determine where data is being lost or malformed.

To view the output produced by a single component during the most recent run, click <Icon name="TextSearch" aria-hidden="true"/> **Inspect output** on the component in the visual editor.

## Access Langflow Desktop logs {#desktop-logs}

If you encounter issues with Langflow Desktop, you might need to access startup logs for debugging.
Follow the steps for your operating system.

<Tabs>
<TabItem value="macos" label="macOS">

1. Open Terminal and run:
   ```bash
   cd ~/Library/Logs/com.LangflowDesktop
   ```

2. To open the folder and view the log files, run the command:
   ```bash
   open .
   ```

3. Locate the `langflow.log` file.

</TabItem>
<TabItem value="windows" label="Windows">

1. Open the Command Prompt (CMD), and then run the following command:

   ```cmd
   cd %LOCALAPPDATA%\com.LangflowDesktop\cache
   ```

2. Open the folder and view the log files:

   ```cmd
   start .
   ```

3. Locate the `langflow.log` file.

</TabItem>
</Tabs>

You can use the log file to investigate the issue on your own, add context to a [GitHub Issue](/contributing-github-issues), or send it to [support](/luna-for-langflow) for debugging assistance.

The log file is only created when Langflow Desktop runs. If you don't see a log file, try starting Langflow Desktop first, then check for the log file.

## See also

* [Logs endpoints](/api-logs)
* [Memory management options](/memory)
* [Configure an external PostgreSQL database](/configuration-custom-database)


================================================================================
filename: Develop/memory.mdx
================================================================================

---
title: Memory management options
slug: /memory
---

Langflow provides flexible memory management options for storage and retrieval of data relevant to your flows and your Langflow server.
This includes essential Langflow database tables, file management, and caching, as well as chat memory.

## Storage options and paths

Langflow supports both local memory and external memory options.

Langflow's default storage option is a [SQLite](https://www.sqlite.org/) database.
The default storage path depends on your operating system and installation method:

- **Langflow Desktop**:
    - **macOS**: `/Users/<username>/.langflow/data/database.db`
    - **Windows**: `C:\Users\<name>\AppData\Roaming\com.LangflowDesktop\data\database.db`
- **Langflow OSS**
    - **macOS/Windows/Linux/WSL with `uv pip install`**: `<path_to_venv>/lib/python3.12/site-packages/langflow/langflow.db` (Python version can vary. Database isn't shared between virtual environments because it is tied to the venv path.)
    - **macOS/Windows/Linux/WSL with `git clone`**: `<path_to_clone>/src/backend/base/langflow/langflow.db`

Langflow offers a few alternatives to the default database path:

* **Config directory**: Set `LANGFLOW_SAVE_DB_IN_CONFIG_DIR=True` to store the database in your Langflow config directory as set in [`LANGFLOW_CONFIG_DIR`](/logging).

* **External PostgreSQL database**: You can use an external PostgreSQL database for all of your Langflow storage.
For more information, see [Configure external memory](#configure-external-memory)

    External storage can be useful if you want to preserve the data after uninstalling Langflow or to share the same database between multiple virtual environments.

* **Separate chat memory**: You can selectively use external storage for chat memory only, separate from other Langflow storage.
For more information, see [Store chat memory](#store-chat-memory).

* **No database**: To disable all database operations and run a no-op session, set `LANGFLOW_USE_NOOP_DATABASE=True` in your [Langflow environment variables](/environment-variables).
This is useful for testing when you don't want to persist any data.

## Langflow database tables

The following tables are stored in `langflow.db`:

â€¢ **ApiKey**: Manages Langflow API authentication keys. Component API keys are stored in the **Variables** table. For more information, see [API keys and authentication](/api-keys-and-authentication).

â€¢ **File**: Stores metadata for files uploaded to Langflow's file management system, including file names, paths, sizes, and storage providers. For more information, see [Manage files](/concepts-file-management).

â€¢ **Flow**: Contains flow definitions, including nodes, edges, and components, stored as JSON or database records. For more information, see [Build flows](/concepts-flows).

    :::tip
    To automatically remove API keys and tokens from flow data before saving a flow to the database, set `LANGFLOW_REMOVE_API_KEYS=True` in your [Langflow environment variables](/environment-variables).
    When `true`, any field marked as a password field that _also_ has `api`, `key`, or `token` in its name is set to `null` before the flow is saved.
    This helps prevent credentials from being stored in the database.
    :::

â€¢ **Folder**: Provides a structure for flow storage, including single-user folders and shared folders accessed by multiple users. For more information, see [Manage flows in projects](/concepts-flows#projects).

â€¢ **Message**: Stores chat messages and interactions that occur between components. For more information, see [Message objects](/data-types#message) and [Store chat memory](#store-chat-memory).

â€¢ **Transactions**: Records execution history and results of flow runs. This information is used for [logging](/logging).

â€¢ **User**: Stores user account information including credentials, permissions, profiles, and user management settings. For more information, see [API keys and authentication](/api-keys-and-authentication).

â€¢ **Variables**: Stores global encrypted values and credentials. For more information, see [Global variables](/configuration-global-variables) and [Component API keys](/api-keys-and-authentication#component-api-keys).

â€¢ **VertexBuild**: Tracks the build status of individual nodes within flows. For more information, see [Test flows in the Playground](/concepts-playground).

For more information, see the database models in the [source code](https://github.com/langflow-ai/langflow/tree/main/src/backend/base/langflow/services/database/models).

## Configure external memory {#configure-external-memory}

To replace the default Langflow SQLite database with another database, set the `LANGFLOW_DATABASE_URL` environment variable to your database URL, and then start Langflow with your `.env` file.
For more information and examples, see [Configure an external PostgreSQL database](/configuration-custom-database).

```text
LANGFLOW_DATABASE_URL=postgresql://user:password@localhost:5432/langflow
```

To fine-tune your database connection pool and timeout settings, you can set the following additional environment variables:

* `LANGFLOW_DATABASE_CONNECTION_RETRY`: Whether to retry lost connections to your Langflow database. If `true`, Langflow tries to connect to the database again if the connection fails. Default: `false`.

* `LANGFLOW_DB_CONNECT_TIMEOUT`: The number of seconds to wait before giving up on a lock to be released or establishing a connection to the database. This may be separate from the `pool_timeout` in `LANGFLOW_DB_CONNECTION_SETTINGS`. Default: 30.

* `LANGFLOW_DB_CONNECTION_SETTINGS`: A JSON dictionary containing the following database connection pool settings:

    - `pool_size`: The base number of connections to keep open in the connection pool. Default: 20.
    - `max_overflow`: Maximum number of connections that can be created in excess of `pool_size` if needed. Default: 30.
    - `pool_timeout`: Number of seconds to wait for a connection from the pool before timing out. Default: 30.
    - `pool_pre_ping`: If `true`, the pool tests connections for liveness upon each checkout. Default: `true`.
    - `pool_recycle`: Number of seconds after which a connection is automatically recycled. Default: 1800 (30 minutes).
    - `echo`: If `true`, SQL queries are logged for debugging purposes. Default: `false`.

    For example:

    ```text
    LANGFLOW_DB_CONNECTION_SETTINGS='{"pool_size": 20, "max_overflow": 30, "pool_timeout": 30, "pool_pre_ping": true, "pool_recycle": 1800, "echo": false}'
    ```

    Don't use the deprecated environment variables `LANGFLOW_DB_POOL_SIZE` or `LANGFLOW_DB_MAX_OVERFLOW`.
    Instead, use `pool_size` and `max_overflow` in `LANGFLOW_DB_CONNECTION_SETTINGS`.

## Configure cache memory

The default Langflow caching behavior is an asynchronous, in-memory cache:

```text
LANGFLOW_LANGCHAIN_CACHE=InMemoryCache
LANGFLOW_CACHE_TYPE=async
```

Langflow officially supports only the default asynchronous, in-memory cache, which is suitable for most use cases.
Other cache options, such as Redis, are experimental and can change without notice.
If you want to use a non-default cache setting, you can use the following environment variables:

| Variable | Type | Default | Description |
|----------|------|---------|-------------|
| `LANGFLOW_CACHE_TYPE` | String | `async` | Set the cache type for Langflow's internal caching system. Possible values: `async`, `redis`, `memory`, `disk`. If you set the type to `redis`, then you must also set the `LANGFLOW_REDIS_*` environment variables. |
| `LANGFLOW_LANGCHAIN_CACHE` | String | `InMemoryCache` | Set the cache storage type for the LangChain caching system (a Langflow dependency), either `InMemoryCache` or `SQLiteCache`. |
| `LANGFLOW_REDIS_HOST` | String | `localhost` | Redis server hostname if `LANGFLOW_CACHE_TYPE=redis`. |
| `LANGFLOW_REDIS_PORT` | Integer | `6379` | Redis server port if `LANGFLOW_CACHE_TYPE=redis`. |
| `LANGFLOW_REDIS_DB` | Integer | `0` | Redis database number if `LANGFLOW_CACHE_TYPE=redis`. |
| `LANGFLOW_REDIS_CACHE_EXPIRE` | Integer | `3600` | Cache expiration time in seconds if `LANGFLOW_CACHE_TYPE=redis`. |
| `LANGFLOW_REDIS_PASSWORD` | String | Not set | Optional password for Redis authentication if `LANGFLOW_CACHE_TYPE=redis`. |

## Store chat memory

Chat-based flows with a **Chat Input** or **Chat Output** component produce chat history that is stored in the Langflow `messages` table.
At minimum, this serves as a chat log, but it isn't functionally the same as chat memory that provides historical context to an LLM.

To store and retrieve chat memories in flows, you can use a **Message History** component or the **Agent** component's built-in chat memory.

<details>
<summary>How does chat memory work?</summary>

Chat memory is a cache for an LLM or agent to preserve past conversations to retain and reference that context in future interactions.
For example, if a user has already told the LLM their name, the LLM can retrieve that information from chat memory rather than asking the user to repeat themselves in future conversations or messages.

Chat memory is distinct from vector store memory because it is built specifically for storing and retrieving chat messages from databases.

Components that support chat memory (such as the **Agent** and **Message History** components) provide access to their respective databases _as memory_.
Retrieval as memory is an important distinction for LLMs and agents because this storage and retrieval mechanism is specifically designed to recall context from past conversations.
Unlike vector stores, which are designed for semantic search and retrieval of text chunks, chat memory is designed to store and retrieve chat messages in a way that is optimized for conversation history.

</details>

### Session ID and chat memory

Chat history and memories are grouped by [session ID (`session_id`)](/session-id).

The default session ID is the flow ID, which means that all chat messages for a flow are stored under the same session ID as one large chat session.

For better segregation of chat memory, especially in flows used by multiple users, consider using custom session IDs.
For example, if you use user IDs as session IDs, then each user's chat history is stored separately, isolating the context of their chats from other users' chats.

### Chat memory options

Where and how chat memory is stored depends on the components used in your flow:

* **Agent component**: This component has built-in chat memory that is enabled by default.
This memory allows the agent to retrieve and reference messages from previous conversations associated with the same session ID.
All messages are stored in [Langflow storage](#storage-options-and-paths), and the component provides minimal memory configuration options, such as the number of messages to retrieve.

    The **Agent** component's built-in chat memory is sufficient for most use cases.

    If you want to use external chat memory storage, retrieve memories outside the context of a chat, or use chat memory with a language model component (not an agent), you must use the **Message History** component (with or without a third-party chat memory component).

* **Message History component**: By default, this component stores and retrieves memories from Langflow storage, unless you attach a third-party chat memory component. It provides a few more options for sorting and filtering memories, although most of these options are also built-in to the **Agent** component as configurable or fixed parameters.

    You can use the **Message History** component with or without a language model or agent.
    For example, if you need to retrieve data from memories outside of chat, you can use the **Message History** component to fetch that data directly from your chat memory database without feeding it into a chat.

* **Third-party chat memory components**: Use one of these components only if you need to store or retrieve chat memories from a dedicated external chat memory database.
Typically, this is necessary only if you have specific storage needs that aren't met by Langflow storage.
For example, if you want to manage chat memory data by directly working with the database, or if you want to use a different database than the default Langflow storage.

For more information and examples, see [**Message History** component](/components-helpers#message-history) and [Agent memory](/agents#agent-memory).

## See also

* [Langflow file management](/concepts-file-management)
* [Langflow logs](/logging)
* [Langflow environment variables](/environment-variables)


================================================================================
filename: Develop/session-id.mdx
================================================================================

---
title: Use session ID to manage communication between components
slug: /session-id
---

Session ID is a unique identifier for client/server connections. A single session equals the duration of a client's connection to a server.

In the Langflow **Playground**, current sessions are listed on the left side of the pane.

Langflow uses session IDs to track different chat interactions within flows. This allows multiple chat sessions to exist in a single flow. Messages are stored in the database with session IDs as a reference.

This differentiation between users per session is helpful in managing client/server connections, but is also important in maintaining separate conversational contexts within a single flow. LLMs rely on past interactions to generate responses to queries, and if these conversations aren't separated, the responses becomes less useful, or even confused.

## Customize session ID

Custom session IDs can be set as part of the payload in API calls, or as advanced settings in individual components. The API session ID value takes precedence. If no session ID is specified, the flow ID is assigned.

If you set a custom session ID in a payload, all downstream components use the upstream component's session ID value.
Replace `LANGFLOW_SERVER_ADDRESS`, `FLOW_ID`, and `LANGFLOW_API_KEY` with the values from your Langflow deployment.
```
curl --request POST \
  --url "http://LANGFLOW_SERVER_ADDRESS/api/v1/run/FLOW_ID" \
  --header "Content-Type: application/json" \
  --header "x-api-key: $LANGFLOW_API_KEY" \
  --data '{
  "input_value": "Hello",
  "output_type": "chat",
  "input_type": "chat",
  "session_id": "my_custom_session_value"
}'
```

The `my_custom_session_value` value is used in components that accept it, and the stored messages from this flow are stored in `langflow.db` with their respective `session_id` values.

## Retrieval of messages from memory by session ID

To retrieve messages from local Langflow memory, add a [**Message History** component](/components-helpers#message-history) to your flow.
The component accepts `sessionID` as a filter parameter, and uses the session ID value from upstream automatically to retrieve message history by session ID from storage.

Messages can be retrieved by `session_id` from the Langflow API at `GET /v1/monitor/messages`. For more information, see [Monitor endpoints](https://docs.langflow.org/api-monitor).

For an example of session ID in action, see [Use Session IDs in Langflow](https://www.youtube.com/watch?v=nJiF_eF21MY).


================================================================================
filename: Flows/concepts-flows-import.mdx
================================================================================

---
title: Import and export flows
slug: /concepts-flows-import
---

import Icon from "@site/src/components/icon";

You can export flows to transfer them between Langflow instances, share them with others, or create backups.

## Export a flow

There are three ways to export flows:

* **Export from projects**: On the [**Projects** page](/concepts-flows#projects), find the flow you want to export, click <Icon name="Ellipsis" aria-hidden="true" /> **More**, and then select **Export**. To export all flows in a project, click <Icon name="Ellipsis" aria-hidden="true" /> **Options** on the **Projects** list, and then select **Download**.

* **Export by sharing**: When editing a flow, click **Share**, and then click **Export**.

* **Export with the Langflow API**: To export one flow, use the [`/flows/download`](/api-flows#export-flows) endpoint.
To export all flows in a project, use the [`/projects/download`](/api-projects#export-a-project) endpoint.

Exported flows are downloaded to your local machine as JSON files named `FLOW_NAME.json`.
If you export an entire project, the JSON files are packaged in a zip archive.
For more information, see [Langflow JSON file contents](#langflow-json-file-contents).

### Save with my API keys

When exporting from the **Projects** page or **Share** menu, you can select **Save with my API keys** to export the flow _and_ any defined API key variables.
Non-API key variables are included in the export regardless of the **Save with my API keys** setting.

:::warning
If you enter the literal key into a component's API key field, then **Save with my API keys** exports the literal key value.

If your key is stored in a Langflow global variable, **Save with my API keys** exports only the variable name.
:::

When you or another user import the flow to another Langflow instance, that instance must have Langflow global variables with the same names and valid values in order to run the flow successfully.
If any variables are missing or invalid, those variables must be created or edited after importing the flow.

## Import a flow

You can import Langflow JSON files from your local machine in the following ways:

* **Import to projects**: On the **Projects** page, click <Icon name="Upload" aria-hidden="true"/> **Upload a flow**, and then select the Langflow JSON file to import.
* **Import anywhere**: Drag and drop Langflow JSON files from your file explorer into your Langflow window to import a flow from any Langflow page.
* **Import with the Langflow API**: To import one Langflow JSON file, use the [`/flows/upload/`](/api-flows#import-flows) endpoint.
To import a zip archive of Langflow JSON files, use the [`/projects/upload`](/api-projects#import-a-project) endpoint.

### Run an imported flow

Once imported, your flow is ready to use.
If the flow contains any global variables, make sure your Langflow instance has global variables with the same names and valid values.
For more information, see [Save with my API keys](/concepts-flows-import#save-with-my-api-keys).

## Langflow JSON file contents

An exported flow is downloaded to your local machine as a JSON file named `FLOW_NAME.json`.

Langflow JSON files contain the following:

* [Nodes](#nodes) and [edges](#edges) that describe components and connections in the flow.
* [Additional metadata](#additional-metadata-and-project-information) that describes the flow and the project it belongs to.

For example Langflow JSON files, you can examine any of the https://github.com/langflow-ai/langflow/tree/main/src/backend/base/langflow/initial_setup/starter_projects[templates in the Langflow repository], or you can create a flow from a template in Langflow, export it, and then open the exported JSON file in a text editor.

### Nodes

Nodes represent the components that make up the flow.
For example, this object represents a **Chat Input** component:

```json
{
  "data": {
    "description": "Get chat inputs from the Playground.",
    "display_name": "Chat Input",
    "id": "ChatInput-jFwUm",
    "node": {
      "base_classes": ["Message"],
      "description": "Get chat inputs from the Playground.",
      "display_name": "Chat Input",
      "icon": "MessagesSquare",
      "template": {
        "input_value": {
          "display_name": "Text",
          "info": "Message to be passed as input.",
          "value": "Hello"
        },
        "sender": {
          "value": "User",
          "options": ["Machine", "User"]
        },
        "sender_name": {
          "value": "User"
        },
        "should_store_message": {
          "value": true
        }
      }
    },
    "type": "ChatInput"
  },
  "position": {
    "x": 689.5720422421635,
    "y": 765.155834131403
  }
}
```

Each node has a unique identifier in the format of `NODE_NAME-UUID`, such as `ChatInput-jFwUm`.

Entrypoint nodes, such as the `ChatInput` node, are the first node executed when running a flow.

### Edges

Edges represent the connections between nodes.

The following example represents the edge (or connection) between the `ChatInput` node and the `OpenAIModel` node:

```json
{
  "className": "",
  "data": {
    "sourceHandle": {
      "dataType": "ChatInput",
      "id": "ChatInput-jFwUm",
      "name": "message",
      "output_types": ["Message"]
    },
    "targetHandle": {
      "fieldName": "input_value",
      "id": "OpenAIModel-OcXkl",
      "inputTypes": ["Message"],
      "type": "str"
    }
  },
  "id": "reactflow__edge-ChatInput-jFwUm{Å“dataTypeÅ“:Å“ChatInputÅ“,Å“idÅ“:Å“ChatInput-jFwUmÅ“,Å“nameÅ“:Å“messageÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}-OpenAIModel-OcXkl{Å“fieldNameÅ“:Å“input_valueÅ“,Å“idÅ“:Å“OpenAIModel-OcXklÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}",
  "source": "ChatInput-jFwUm",
  "sourceHandle": "{Å“dataTypeÅ“: Å“ChatInputÅ“, Å“idÅ“: Å“ChatInput-jFwUmÅ“, Å“nameÅ“: Å“messageÅ“, Å“output_typesÅ“: [Å“MessageÅ“]}",
  "target": "OpenAIModel-OcXkl",
  "targetHandle": "{Å“fieldNameÅ“: Å“input_valueÅ“, Å“idÅ“: Å“OpenAIModel-OcXklÅ“, Å“inputTypesÅ“: [Å“MessageÅ“], Å“typeÅ“: Å“strÅ“}"
}
```

This edge shows that the `ChatInput` component outputs a `Message` type to the `target` node, which is the `OpenAIModel` node.
The `OpenAIModel` component accepts the `Message` type at the `input_value` field.

### Additional metadata and project information

Additional information about the flow is stored in the root `data` object:

* Metadata and project information including the name, description, and `last_tested_version` of the flow.
For example:

    ```json
    {
      "name": "Basic Prompting",
      "description": "Perform basic prompting with an OpenAI model.",
      "tags": ["chatbots"],
      "id": "1511c230-d446-43a7-bfc3-539e69ce05b8",
      "last_tested_version": "1.0.19.post2",
      "gradient": "2",
      "icon": "Braces"
    }
    ```

* Visual information about the flow defining the position of the viewport when you open the flow in the workspace:

    ```json
    "viewport": {
      "x": -37.61270157375441,
      "y": -155.91266341888854,
      "zoom": 0.7575251406952855
    }
    ```

* <Icon name="StickyNote" aria-hidden="true"/> **Notes** are comments that help explain the flow's purpose, configuration details, and any other information relevant to users who might be editing the flow.
They can contain text, links, code snippets, and other information.
They are encoded in Markdown format and stored as `node` objects.

    ```json
    {
      "id": "undefined-kVLkG",
      "node": {
        "description": "## ðŸ“– README\nPerform basic prompting with an OpenAI model.\n\n#### Quick Start\n- Add your **OpenAI API key** to the **OpenAI Model**\n- Open the **Playground** to chat with your bot.\n..."
      }
    }
    ```

## See also

* [Build flows](/concepts-flows)
* [Share and embed flows](/concepts-publish)


================================================================================
filename: Flows/concepts-flows.mdx
================================================================================

---
title: Build flows
slug: /concepts-flows
---

import Icon from "@site/src/components/icon";

A _flow_ is a functional representation of an application workflow.
Flows receive input, process it, and produce output.

Flows consist of _components_ that represent individual steps in your application's workflow.

![Basic Prompting flow in the workspace](/img/workspace-basic-prompting.png)

Langflow flows are fully serializable and can be saved and loaded from the file system where Langflow is installed.

:::tip
To try building and running a flow in a few minutes, see the [Langflow quickstart](/get-started-quickstart).
:::

## Create a flow

From the [**Projects** page](#projects), there are four ways to create a flow:

* **Create a blank flow**: Select a project, click **New Flow**, and then click **Blank Flow**.

* **Create a flow from a template**: Select a project, click **New Flow**, and then click the template that you want to use.

    <details>
    <summary>What are templates?</summary>

    Templates are pre-built flows that you can use as a starting point for your own flow.
    They range from basic flows with a few components to complex flows with many components and sub-flows.

    For example, the **Basic Prompting** template demonstrates a small flow that passes both chat input and pre-defined instructions (as a prompt) to an LLM.
    In contrast, the **Vector Store RAG** template consists of two sub-flows that demonstrate how to create a Retrieval Augmented Generation (RAG) chatbot. One sub-flow populates the vector store with contextually relevant data and embeddings, and the other sub-flow queries the vector store for similar data to answer user questions.

    You can also [contribute templates](/contributing-templates) to the Langflow codebase.

    </details>

* **Duplicate an existing flow**: Locate the flow you want to copy, click <Icon name="Ellipsis" aria-hidden="true" /> **More**, and then select **Duplicate**.

* **Import a flow**: See [Import and export flows](/concepts-flows-import).

You can also create a flow with the [Langflow API](/api-flows), but the Langflow team recommends using the [visual editor](/concepts-overview) until you are familiar with flow creation.

### Add components

Flows consist of [components](/concepts-components), which are nodes that you configure and connect in the [workspace](/concepts-overview#workspace).
Each component performs a specific task, like serving an AI model or connecting a data source.

Drag and drop components from the <Icon name="Component" aria-hidden="true" /> **Core components** and <Icon name="Blocks" aria-hidden="true" /> **Bundles** menus to add them to your flow.
Then, configure the component settings and connect the components together.

![Chat Input and Output connected to a Language Model component](/img/connect-component.png)

Each component has configuration settings and options. Some of these are common to all components, and some are unique to specific components.

To form a cohesive flow, you connect components by _edges_ or _ports_, which have a specific data type they receive or send.
For example, message ports send text strings between components.

For more information about component configuration, including port types and underlying component code, see [Components overview](/concepts-components).

### Run a flow

After you build a prototype flow, you can test it in the [**Playground**](/concepts-playground).
When you're ready to use Langflow for application development, learn how to [trigger flows with the Langflow API](/concepts-publish), explore more advanced configuration options like [custom dependencies](/install-custom-dependencies), and, eventually, [containerize your Langflow application](/develop-application).

When you're ready to go to production or deploy a Langflow MCP server for access over the public internet, see [Langflow deployment overview](/deployment-overview).

#### Flow graphs

When a flow runs, Langflow builds a Directed Acyclic Graph (DAG) object from the nodes (components) and edges (connections), and the nodes are sorted to determine the order of execution.

The graph build calls each component's `def_build` function to validate and prepare the nodes.
This graph is then processed in dependency order.
Each node is built and executed sequentially, with results from each built node being passed to nodes that are dependent on that node's results.

## Manage flows in projects {#projects}

The **Projects** page is where you arrive when you launch Langflow.
From here, you can manage flows and your projects' [MCP servers](/mcp-server).

Langflow projects are like folders that you can use to organize related flows.
The default project is **Starter Project**, and your flows are stored here unless you create another project.
To create a project, click <Icon name="Plus" aria-hidden="true"/> **Create new project**.

![Projects page with multiple flows in a project](/img/my-projects.png)

:::tip
To get back to the **Projects** page after editing a flow, click the project name or Langflow icon in the Langflow header.
:::

### Edit flow details

1. On the **Projects** page, locate the flow you want to edit.
2. Click <Icon name="Ellipsis" aria-hidden="true" /> **More**, and then select **Edit details**.
3. Edit the **Name** and **Description**, and then click **Save**.

### Lock a flow

To prevent changes to a flow, you can lock it:

1. On the **Projects** page, locate the flow you want to lock.
2. Click <Icon name="Ellipsis" aria-hidden="true" /> **More**, and then select **Edit details**.
3. Enable **Lock Flow**, and then click **Save**.

Repeat these steps to unlock the flow by disabling **Lock Flow**.

When editing a flow, the **Lock Status** indicates whether the flow is <Icon name="Lock" aria-hidden="true" /> **Locked** or <Icon name="LockOpen" aria-hidden="true" /> **Unlocked**.
You cannot change the lock status while editing the flow.

### Move a flow

To move a flow from one project to another, do the following:

1. On the **Projects** page, locate the flow you want to move.
2. Click and drag the flow from the list of flows to the target project name in the list of projects.

### Delete a flow

1. On the **Projects** page, locate the flow you want to delete.
2. Click <Icon name="Ellipsis" aria-hidden="true" /> **More**, and then select **Delete**.

## Flow storage and logs

By default, flows and flow execution data are stored in the Langflow database, and flow logs are stored with other Langflow logs in the Langflow config directory.
For more information, see [Memory management options](/memory) and [Logging](/logging).

## See also

* [Share and embed flows](/concepts-publish)
* [Import and export flows](/concepts-flows-import)
* [Langflow environment variables](/environment-variables)


================================================================================
filename: Flows/concepts-overview.mdx
================================================================================

---
title: Use the visual editor
slug: /concepts-overview
---

import Icon from "@site/src/components/icon";

You use Langflow's visual editor to create, test, and share flows, which are functional representations of application workflows.
Flows consist of components that represent individual steps in your application's workflow.

Langflow's drag-and-drop interface allows you to create complex AI workflows without writing extensive code.
You can connect different resources, including prompts, large language models (LLMs), data sources, agents, MCP servers, and other tools and integrations.

:::tip
To try building and running a flow in a few minutes, see the [Langflow quickstart](/get-started-quickstart).
:::

## Workspace

When building a [flow](/concepts-flows), you primarily interact with the workspace.
This is where you add [components](/concepts-components), configure them, and attach them together.

![Empty Langflow workspace](/img/workspace.png)

From the workspace, you can also access the [**Playground**](#playground), [**Share** menu](#share-menu), and [**Logs**](/logging).

### Workspace gestures and interactions

Use these shortcuts, gestures, and functionality to navigate the workspace:

- **Pan horizontally and vertically**: Click and drag an empty area of the workspace.

- **Rearrange components**: Click and drag the components anywhere on the workspace.

    To change the programmatic relationship between components, you must manipulate the component _edges_ or _ports_. For more information, see [Components overview](/concepts-components).

    To enable guide lines, click <Icon name="CircleQuestionMark" aria-hidden="true" /> **Help**, and then toggle **Enable smart guides**.

    If you can't edit any components, make sure the flow is [unlocked](/concepts-flows#lock-a-flow).

- **Zoom**: Scroll on the mouse or trackpad, or click <Icon name="ChevronUp" aria-hidden="true" /> **Canvas controls** next to the zoom percentage more zoom options: **Zoom In**, **Zoom Out**, **Zoom To 100%**, and **Zoom To Fit**.

- **Add notes and comments**: Click <Icon name="StickyNote" aria-hidden="true"/> **Add Note**.

- **Keyboard shortcuts**: To view available shortcuts, click <Icon name="CircleQuestionMark" aria-hidden="true" /> **Help**, and then select **Shortcuts**.

## Playground

If your flow has a **Chat Input** component, you can use the **Playground** to run your flow, chat with your flow, view inputs and outputs, and modify the LLM's memories to tune the flow's responses in real time.

To try this for yourself, create a flow based on the **Basic Prompting** template, and then click <Icon name="Play" aria-hidden="true"/> **Playground** when editing the flow in the workspace.

![Playground](/img/playground.png)

If you have an **Agent** component in your flow, the **Playground** displays its tool calls and outputs so you can monitor the agent's tool use and understand the reasoning behind its responses.
To try an agent flow in the **Playground**, use the **Simple Agent** template or the [Langflow quickstart](/get-started-quickstart).

![Playground with agent response](/img/playground-with-agent.png)

For more information, see [Test flows in the Playground](/concepts-playground).

## Share {#share-menu}

The **Share** menu provides the following options for integrating your flow into external applications:

* [**API access**](/concepts-publish#api-access): Integrate your flow into your applications with automatically-generated Python, JavaScript, and curl code snippets.
* [**Export**](/concepts-flows-import#export-a-flow): Export your flow to your local machine as a JSON file.
* [**MCP Server**](/mcp-server): Expose your flow as a tool for MCP-compatible clients.
* [**Embed into site**](/concepts-publish#embedded-chat-widget): Embed your flow in HTML, React, or Angular applications.
* [**Shareable Playground**](/concepts-playground#share-a-flows-playground): Share your **Playground** interface with another user.

    This is specifically for sharing the **Playground** experience; it isn't for running a flow in a production application.

    The **Sharable Playground** isn't available for Langflow Desktop.

## See also

* [Manage files in Langflow](/concepts-file-management)
* [Global variables](/configuration-global-variables)
* [API keys and authentication](/api-keys-and-authentication)


================================================================================
filename: Flows/concepts-playground.mdx
================================================================================

---
title: Test flows in the Playground
slug: /concepts-playground
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import Icon from "@site/src/components/icon";

Langflow's **Playground** is a dynamic interface you can use to test your LLM-based flows in real-time.

You can test how a flow responds to different inputs, review and modify memories, and monitor flow output and logic.
For example, you can make sure agent flows use the appropriate tools to respond to different inputs.

The **Playground** allows you to quickly iterate over your flow's logic and behavior, making it easier to prototype and refine your applications.

## Run a flow in the Playground

To run a flow in the **Playground**, open the flow, and then click <Icon name="Play" aria-hidden="true"/> **Playground**.
Then, if your flow has a [**Chat Input** component](/components-io), enter a prompt or [use voice mode](/concepts-voice-mode) to trigger the flow and start a chat session.

:::tip
If there is no message input field in the **Playground**, make sure your flow has a **Chat Input** component that is connected, directly or indirectly, to the **Input** port of a **Language Model** or **Agent** component.

Because the **Playground** is designed for flows that use an LLM in a query-and-response format, such as chatbots and agents, a flow must have **Chat Input**, **Language Model**/**Agent**, and **Chat Output** components to be fully supported by the **Playground** chat interface

For flows that require another type of input, such as a webhook event, file upload, or text input, you can [use the Langflow API to trigger the flow](/api-flows-run), and then open the **Playground** to review the LLM activity for the flow run, if applicable.
:::

![Playground window](/img/playground.png)

For technical details about how the **Playground** works, see [Monitor endpoints](/api-monitor).

### Review agent logic

If your flow has an **Agent** component, the **Playground** prints the tools used by the agent and the output from each tool.
This helps you monitor the agent's tool use and understand the logic behind the agent's responses.
For example, the following agent used a connected `fetch_content` tool to perform a web search:

![Playground with agent response](/img/playground-with-agent.png)

### View chat history {#view-chat-history}

In the **Playground**, you can view message logs for each of your flow's chat sessions, including timestamps, content, and senders.

In the **Playground** sidebar, find the chat session you want to review, click <Icon name="Ellipsis" aria-hidden="true"/> **Options**, and then select **Message Logs**.

![Playground logs](/img/messages-logs.png)

Message logs break apart the [`Message` data](/data-types#message) for each chat message.
Click any cell in the message logs to view the full contents of that cell.

### Modify memories in the Playground

To help debug and test your flows, you can edit or delete individual messages in [message logs](#view-chat-history).
For example, you might want to delete messages that you sent while testing a component that is no longer part of your flow.

You can also delete entire chat sessions from the sidebar: click <Icon name="Ellipsis" aria-hidden="true"/> **Options**, and then select **Delete**.

Modifying memories influences the behavior of the chatbot responses if you continue the chat session or if you preserve memories over multiple chat sessions.

**Editing message logs edits Langflow's internal `messages` table, which is the default chat memory storage.**
For more information about managing sessions and chat memory in Langflow, see [Use custom session IDs](#session-ids) and [Memory management options](/memory).

## Set custom session IDs {#session-ids}

Chat sessions are identified by session ID (`session_id`), which is a unique identifier for a flow run.

The default session ID is the flow ID, which means that all chat messages for a flow are stored under the same session ID as one enormous chat session.

If you need to preserve chat context over multiple flow runs or differentiate chat sessions when debugging flows, you can set a custom `session_id`.

Custom session IDs are helpful for multiple reasons:

* Separate chat sessions in situations where one flow has multiple chat sessions, such as a chatbot that can have multiple simultaneous user interactions.
* Preserve memory when continuing a chat session across multiple flow runs or when passing context from one flow to another.
* Differentiate activity from multiple users within the same flow.
* Identify your own chat sessions when debugging and testing flows.

You can set custom session IDs in the visual editor and programmatically.

<Tabs>
<TabItem value="visual" label="Visual editor" default>

In your [input and output components](/components-io), use the **Session ID** field:

1. Click the component where you want to set a custom session ID.
2. In the [component's header menu](/concepts-components#component-menus), click <Icon name="SlidersHorizontal" aria-hidden="true"/> **Controls**.
3. Enable **Session ID**.
4. Click **Close**.
5. Enter a custom session ID.
If the field is empty, the flow uses the default session ID.
6. Open the **Playground** to start a chat under your custom session ID.

Make sure to change the **Session ID** when you want to start a new chat session or continue an earlier chat session with a different session ID.

</TabItem>
<TabItem value="api" label="Langflow API">

When you trigger a flow with the Langflow API, include the `session_id` parameter in the request payload.
For example:

```bash
curl -X POST "http://$LANGFLOW_SERVER_ADDRESS/api/v1/run/$FLOW_ID" \
-H "Content-Type: application/json" \
-H "x-api-key: $LANGFLOW_API_KEY" \
-d '{
    "session_id": "CUSTOM_SESSION_ID",
    "input_value": "message",
    "input_type": "chat",
    "output_type": "chat"
}'
```

This command starts a new chat sessions with the specified `session_id` or it retrieves an existing session with that ID, if one exists.

</TabItem>
</Tabs>

:::tip
In a production environment, consider using a variable for the session ID rather than a hardcoded value.

For example, if you want to preserve context for authenticated users, user ID could be a useful variable for the session ID.
Alternatively, if you want every chat to be unique, you might want to automatically generate a UUID for each session ID.
:::

For more information, see [Use session ID to manage communication between components](/session-id).

## Share a flow's Playground

:::warning
The **Shareable Playground** is for testing purposes only.
The **Playground** isn't meant for embedding flows in applications. For information about running flows in applications or websites, see [Trigger flows with the Langflow API](/concepts-publish).

The **Shareable Playground** isn't available in Langflow Desktop.
:::

The **Shareable Playground** option exposes the **Playground** for a single flow at the `/public_flow/$FLOW_ID` endpoint.

After you [deploy a public Langflow server](/deployment-overview), you can share this public URL with another user to allow them to access the specified flow's **Playground** only.
The user can interact with the flow's chat input and output and view the results without installing Langflow or generating a Langflow API key.

To share a flow's **Playground** with another user, do the following:

1. In Langflow, open the flow you want share.
2. In the [workspace](/concepts-overview#workspace), click **Share**, and then enable **Shareable Playground**.
3. Click **Shareable Playground** again to open the **Playground** window.
This window's URL is the flow's **Shareable Playground** address, such as `https://3f7c-73-64-93-151.ngrok-free.app/playground/d764c4b8-5cec-4c0f-9de0-4b419b11901a`.
4. Send the URL to another user to give them access to the flow's **Playground**.

## See also

- [Upload images](/concepts-file-management#upload-images)
- [Use voice mode](/concepts-voice-mode)
- [Trigger flows with the Langflow API](/concepts-publish)
- [Session ID](/session-id)


================================================================================
filename: Flows/concepts-publish.mdx
================================================================================

---
title: Trigger flows with the Langflow API
slug: /concepts-publish
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

After you build a flow, you probably want to run it within an application, such as a chatbot within a mobile app or website.

Langflow provides several ways to run flows from external applications:

* [Trigger flows with the Langflow API](#api-access)
* [Add an embedded chat widget to a website](#embedded-chat-widget)
* [Serve flows through a Langflow MCP server](#serve-flows-through-a-langflow-mcp-server)
* [Run flows with the OpenAI Responses compatible endpoint](#openai-responses-api)

Although you can use these options with an isolated, local Langflow instance, they are typically more valuable when you have [deployed a Langflow server](/deployment-overview) or [packaged Langflow as a dependency of an application](/develop-application).

## Use the Langflow API to run flows {#api-access}

The Langflow API is the primary way to access your flows and Langflow servers programmatically.

:::tip Try it
For an example of a script that calls the Langflow API, see the [Quickstart](/get-started-quickstart).
:::

### Generate API code snippets

To help you embed Langflow API requests in your scripts, Langflow automatically generates Python, JavaScript, and curl code snippets for your flows.
To get these code snippets, do the following:

1. In Langflow, open the flow that you want to embed in your application.
2. Click **Share**, and then select **API access**.

    These code snippets call the `/v1/run/$FLOW_ID` endpoint, and they automatically populate minimum values, like the Langflow server URL, flow ID, headers, and request parameters.

    :::tip Windows
    The paths generated by the API access pane assume a *nix environment.
    If you use Microsoft Windows or WSL, you might need to adjust the filepaths given in the code snippets.
    :::

    ![API access pane](/img/api-pane.png)

3. Optional: Click [**Input Schema**](#input-schema) to modify component parameters in the code snippets without changing the flow itself.

4. Copy the snippet for the language that you want to use.

5. Run the snippet as is, or use the snippet in the context of a larger script.

For more information and examples of other Langflow API endpoints, see [Get started with the Langflow API](/api-reference-api-examples).

### Langflow API authentication

In Langflow versions 1.5 and later, most API endpoints require authentication with a Langflow API key.

Code snippets generated in the **API access** pane include a script that checks for a `LANGFLOW_API_KEY` environment variable set in the local terminal session.
This script doesn't check for Langflow API keys set anywhere besides the local terminal session.

For this script to work, you must set a `LANGFLOW_API_KEY` variable in the terminal session where you intend to run the code snippet, such as `export LANGFLOW_API_KEY="sk..."`.

Alternatively, you can edit the code snippet to include an `x-api-key` header and ensure that the request can authenticate to the Langflow API.

For more information, see [API keys and authentication](/api-keys-and-authentication) and [Get started with the Langflow API](/api-reference-api-examples).

### Input Schema (tweaks) {#input-schema}

Tweaks are one-time overrides that modify component parameters at runtime, rather than permanently modifying the flow itself.
For an example of tweaks in a script, see the [Quickstart](/get-started-quickstart).

:::tip
Tweaks make your flows more dynamic and reusable.

You can create one flow and use it for multiple applications by passing application-specific tweaks in each application's Langflow API requests.
:::

In the **API access** pane, click **Input Schema** to add `tweaks` to the request payload in a flow's code snippets.

Changes to a flow's **Input Schema** are saved exclusively as tweaks for that flow's **API access** code snippets.
These tweaks don't change the flow parameters set in the [workspace](/concepts-overview#workspace), and they don't apply to other flows.

Adding tweaks through the **Input Schema** can help you troubleshoot formatting issues with tweaks that you manually added to Langflow API requests.

For example, the following curl command includes a tweak that disables the **Store Messages** setting in a flow's **Chat Input** component:

```bash
curl --request POST \
  --url "http://LANGFLOW_SERVER_ADDRESS/api/v1/run/FLOW_ID" \
  --header "Content-Type: application/json" \
  --header "x-api-key: LANGFLOW_API_KEY" \
  --data '{
  "input_value": "Text to input to the flow",
  "output_type": "chat",
  "input_type": "chat",
  "tweaks": {
    "ChatInput-4WKag": {
      "should_store_message": false
    }
  }
}'
```

### Use a flow ID alias

If you want your requests to use an alias instead of the actual flow ID, you can rename the flow's `/v1/run/$FLOW_ID` endpoint:

1. In Langflow, open the flow, click **Share**, and then select **API access**.
2. Click **Input Schema**.
3. In the **Endpoint Name** field, enter an alias for your flow's ID, such as a memorable, human-readable name.

    The name can contain only letters, numbers, hyphens, and underscores, such as `flow-customer-database-agent`.

4. To save the change, close the **Input Schema** pane.

The automatically generated code snippets now use your new endpoint name instead of the original flow ID, such as `url = "http://localhost:7868/api/v1/run/flow-customer-database-agent"`.

## Embed a flow into a website {#embedded-chat-widget}

For each flow, Langflow provides a code snippet that you can insert into the `<body>` of your website's HTML to interact with your flow through an embedded chat widget.

:::warning Required components
The chat widget only supports flows that have **Chat Input** and **Chat Output** components, which are required for the chat experience.
**Text Input** and **Text Output** components can send and receive messages, but they don't include ongoing LLM chat context.

Attempting to chat with a flow that doesn't have [**Chat Input** component](/components-io) will trigger the flow, but the response only indicates that the input was empty.
:::

### Get a langflow-chat snippet

To get a flow's embedded chat widget code snippet, do the following:

1. In Langflow, open the flow you want to embed.
2. Click **Share**, and then select **Embed into site**.
3. Copy the code snippet and use it in the `<body>` of your website's HTML.
For more information, see [Embed the chat widget with React, Angular, or HTML](#embed-the-chat-widget).
4. Add the `api_key` prop to ensure the widget has permission to run the flow, as explained in [Configure the langflow-chat web component](#configure-the-langflow-chat-web-component).

The chat widget is implemented as a web component called `langflow-chat` that is loaded from a CDN. For more information, see the [langflow-embedded-chat repository](https://github.com/langflow-ai/langflow-embedded-chat).

For example, the following HTML embeds a chat widget for a **Basic Prompting** template flow hosted on a Langflow server deployed on ngrok:

```html
<html>
  <head>
    <script src="https://cdn.jsdelivr.net/gh/langflow-ai/langflow-embedded-chat@main/dist/build/static/js/bundle.min.js"></script>
  </head>
  <body>
    <langflow-chat
      host_url="https://c822-73-64-93-151.ngrok-free.app"
      flow_id="dcbed533-859f-4b99-b1f5-16fce884f28f"
      api_key="$LANGFLOW_API_KEY"
    ></langflow-chat>
  </body>
</html>
```

When this code is deployed to a live site, it renders as a responsive chatbot.
If a user interacts with the chatbot, the input triggers the specified flow, and then the chatbot returns the output from the flow run.

![Default chat widget](/img/chat-widget-default.png)

:::tip Try it
Use the [Langflow embedded chat CodeSandbox](https://codesandbox.io/p/sandbox/langflow-embedded-chat-example-dv9zpx) for an interactive live demo of the embedded chat widget that uses your own flow.
For more information, see the [langflow-embedded-chat README](https://github.com/langflow-ai/langflow-embedded-chat?tab=readme-ov-file#live-example).
:::

### Embed the chat widget with React, Angular, or HTML {#embed-the-chat-widget}

The following examples show how to use embedded chat widget in React, Angular, and plain HTML.

<Tabs>
<TabItem value="react" label="React" default>

To use the chat widget in your React application, create a component that loads the widget script and renders the chat interface:

1. Declare your web component, and then encapsulate it in a React component:

    ```javascript
    //Declaration of langflow-chat web component
    declare global {
    namespace JSX {
        interface IntrinsicElements {
        "langflow-chat": any;
        }
    }
    }

    //Definition for langflow-chat React component
    export default function ChatWidget({ className }) {
    return (
        <div className={className}>
        <langflow-chat
            host_url="https://c822-73-64-93-151.ngrok-free.app"
            flow_id="dcbed533-859f-4b99-b1f5-16fce884f28f"
            api_key="$LANGFLOW_API_KEY"
        ></langflow-chat>
        </div>
    );
    }
    ```

2. Place the component anywhere in your code to render the chat widget.

    In the following example, the React widget component is located at `docs/src/components/ChatWidget/index.tsx`, and `index.tsx` includes a script to load the chat widget code from CDN, along with the declaration and definition from the previous step:

    ```javascript
    import React, { useEffect } from 'react';

    // Component to load the chat widget script
    const ChatScriptLoader = () => {
    useEffect(() => {
        if (!document.querySelector('script[src*="langflow-embedded-chat"]')) {
        const script = document.createElement('script');
        script.src = 'https://cdn.jsdelivr.net/gh/langflow-ai/langflow-embedded-chat@main/dist/build/static/js/bundle.min.js';
        script.async = true;
        document.body.appendChild(script);
        }
    }, []);

    return null;
    };

    //Declaration of langflow-chat web component
    declare global {
    namespace JSX {
        interface IntrinsicElements {
        "langflow-chat": any;
        }
    }
    }

    //Definition for langflow-chat React component
    export default function ChatWidget({ className }) {
    return (
        <div className={className}>
        <ChatScriptLoader />
        <langflow-chat
            host_url="https://c822-73-64-93-151.ngrok-free.app"
            flow_id="dcbed533-859f-4b99-b1f5-16fce884f28f"
            api_key="$LANGFLOW_API_KEY"
        ></langflow-chat>
        </div>
    );
    }
    ```

3. Import the `langflow-chat` React component to make it available for use on a page.
Modify the following import statement with your React component's name and path:

    ```jsx
    import ChatWidget from '@site/src/components/ChatWidget';
    ```

4. To display the widget, call your `langflow-chat` component in the desired location on the page.
Modify the following reference for your React component's name and the desired `className`:

   ```
   <ChatWidget className="my-chat-widget" />
   ```

</TabItem>
<TabItem value="angular" label="Angular">

To use the chat widget in your Angular application, create a component that loads the widget script and renders the chat interface.

In an Angular application, `langflow-chat` is a custom web component that you must explicitly allow in your site's `.components.ts`.
Therefore, to use the embedded chat widget, you must add `CUSTOM_ELEMENTS_SCHEMA` to your module's configuration, and then integrate the `<langflow-chat>` element.

Angular requires you to explicitly allow custom web components, like `langflow-chat`, in your site's  `components`.
Therefore, you must add the `<langflow-chat>` element to your Angular template and configure Angular to recognize it.
You must add `CUSTOM_ELEMENTS_SCHEMA` to your module's configuration to enable this.

1. In your Angular application, edit the `.module.ts` file where you want to add the `langflow-chat` web component.

2. At the top of `.module.ts`, import `CUSTOM_ELEMENTS_SCHEMA`:

    ```
    import { NgModule, CUSTOM_ELEMENTS_SCHEMA } from '@angular/core';
    ```

3. In the `@NgModule` decorator, add `CUSTOM_ELEMENTS_SCHEMA` to the `schemas` array:

    ```javascript
    import { NgModule, CUSTOM_ELEMENTS_SCHEMA } from '@angular/core';
    import { BrowserModule } from '@angular/platform-browser';
    import { AppComponent } from './app.component';

    @NgModule({
    declarations: [
        AppComponent
    ],
    imports: [
        BrowserModule
    ],
    schemas: [CUSTOM_ELEMENTS_SCHEMA],
    providers: [],
    bootstrap: [AppComponent]
    })
    export class AppModule { }
    ```

4. Edit the `.component.ts` file where you want to use the embedded chat widget.

5. In the `@Component` decorator, add the `<langflow-chat>` element to the `template` key:

    ```javascript
    import { Component } from '@angular/core';

    @Component({
    selector: 'app-root',
    template: `
        <div class="container">
        <h1>Langflow Chat Test</h1>
        <langflow-chat
            host_url="https://c822-73-64-93-151.ngrok-free.app"
            flow_id="dcbed533-859f-4b99-b1f5-16fce884f28f"
            api_key="$LANGFLOW_API_KEY"
        ></langflow-chat>
        </div>
    `,
    styles: [`
        .container {
        padding: 20px;
        text-align: center;
        }
    `]
    })
    export class AppComponent {
    title = 'Langflow Chat Test';
    }
    ```

</TabItem>
<TabItem value="html" label="HTML">

```html
<html lang="en">
<head>
<script src="https://cdn.jsdelivr.net/gh/langflow-ai/langflow-embedded-chat@v1.0.7/dist/build/static/js/bundle.min.js"></script>
</head>
<body>
<langflow-chat
    host_url="https://c822-73-64-93-151.ngrok-free.app"
    flow_id="dcbed533-859f-4b99-b1f5-16fce884f28f"
    api_key="$LANGFLOW_API_KEY"
  ></langflow-chat>
</body>
</html>
```

</TabItem>
</Tabs>

### Configure the langflow-chat web component {#configure-the-langflow-chat-web-component}

To use the embedded chat widget in your HTML, the `langflow-chat` web component must include the following minimum inputs (also known as _props_ in React):

* `host_url`: Your Langflow server URL. Must be `HTTPS`. Don't include a trailing slash (`/`).
* `flow_id`: The ID of the flow you want to embed.
* `api_key`: A [Langflow API key](/api-keys-and-authentication).
This prop is recommended to ensure the widget has permission to run the flow.

The minimum inputs are automatically populated in the [**Embed into site** code snippet](#get-a-langflow-chat-snippet) that is generated by Langflow.

You can use additional inputs (props) to modify the embedded chat widget.
For a list of all props, types, and descriptions, see the [langflow-embedded-chat README](https://github.com/langflow-ai/langflow-embedded-chat?tab=readme-ov-file#configuration).

<details>
<summary>Example: Langflow API key prop</summary>

The `api_key` prop stores a Langflow API key that the chat widget can use to authenticate the underlying Langflow API request.

The Langflow team recommends following industry best practices for handling sensitive credentials.
For example, securely store your API key, and then retrieve with an environment variable:

```html
<langflow-chat
    host_url="https://c822-73-64-93-151.ngrok-free.app"
    flow_id="dcbed533-859f-4b99-b1f5-16fce884f28f"
    api_key="$LANGFLOW_API_KEY"
></langflow-chat>
```

</details>

<details>
<summary>Example: Style props</summary>

There are many props you can use to customize the style and positioning of the embedded chat widget.
Many of these props are of type JSON, and they require specific formatting, depending on where you embed the `langflow-chat` web component.

In React and plain HTML, JSON props are expressed as JSON objects or stringified JSON, such as `\{"key":"value"\}`:

```html
<langflow-chat
    host_url="https://c822-73-64-93-151.ngrok-free.app"
    flow_id="dcbed533-859f-4b99-b1f5-16fce884f28f"
    api_key="$LANGFLOW_API_KEY"
    chat_window_style='{
        "backgroundColor": "#1a0d0d",
        "border": "4px solid #b30000",
        "borderRadius": "16px",
        "boxShadow": "0 8px 32px #b30000",
        "color": "#fff",
        "fontFamily": "Georgia, serif",
        "padding": "16px"
    }'
    window_title="Custom Styled Chat"
    height="600"
    width="400"
></langflow-chat>
```

For Angular applications, use [property binding syntax](https://angular.dev/guide/templates/binding#binding-dynamic-properties-and-attributes) to pass JSON props as JavaScript objects.
For example:

```javascript
import { Component } from '@angular/core';

@Component({
  selector: 'app-root',
  template: `
    <div class="container">
      <h1>Langflow Chat Test</h1>
      <langflow-chat
        host_url="https://c822-73-64-93-151.ngrok-free.app"
        flow_id="dcbed533-859f-4b99-b1f5-16fce884f28f"
        api_key="$LANGFLOW_API_KEY"
        [chat_window_style]='{"backgroundColor": "#ffffff"}'
        [bot_message_style]='{"color": "#000000"}'
        [user_message_style]='{"color": "#000000"}'
        height="600"
        width="400"
        chat_position="bottom-right"
      ></langflow-chat>
    </div>
  `,
  styles: [`
    .container {
      padding: 20px;
      text-align: center;
    }
  `]
})
export class AppComponent {
  title = 'Langflow Chat Test';
}
```

</details>

<details>
<summary>Example: Session ID prop</summary>

The following example adds a custom [session ID](/session-id) to help identify flow runs started by the embedded chat widget:

```html
<langflow-chat
    host_url="https://c822-73-64-93-151.ngrok-free.app"
    flow_id="dcbed533-859f-4b99-b1f5-16fce884f28f"
    api_key="$LANGFLOW_API_KEY"
    session_id="$SESSION_ID"
></langflow-chat>
```

</details>

<details>
<summary>Example: Tweaks prop</summary>

Use the `tweaks` prop to modify flow parameters at runtime.
The available keys for the `tweaks` object depend on the flow you are serving through the embedded chat widget.

In React and plain HTML, `tweaks` are declared as a JSON object, similar to how you would pass them to a Langflow API endpoint like [`/v1/run/$FLOW_ID`](/api-flows-run#run-flow).
For example:

```html
<langflow-chat
    host_url="https://c822-73-64-93-151.ngrok-free.app"
    flow_id="dcbed533-859f-4b99-b1f5-16fce884f28f"
    api_key="$LANGFLOW_API_KEY"
    tweaks='{
        "model_name": "llama-3.1-8b-instant"
    }'
></langflow-chat>
```

For Angular applications, use [property binding syntax](https://angular.dev/guide/templates/binding#binding-dynamic-properties-and-attributes) to pass JSON props as JavaScript objects.
For example:

```javascript
import { Component } from '@angular/core';

@Component({
  selector: 'app-root',
  template: `
    <div class="container">
      <h1>Langflow Chat Test</h1>
      <langflow-chat
        host_url="https://c822-73-64-93-151.ngrok-free.app"
        flow_id="dcbed533-859f-4b99-b1f5-16fce884f28f"
        api_key="$LANGFLOW_API_KEY"
        [tweaks]='{"model_name": "llama-3.1-8b-instant"}'
      ></langflow-chat>
    </div>
  `,
  styles: [`
    .container {
      padding: 20px;
      text-align: center;
    }
  `]
})
export class AppComponent {
  title = 'Langflow Chat Test';
}
```

</details>

## Serve flows through a Langflow MCP server

Each [Langflow project](/concepts-flows#projects) has an MCP server that exposes the project's flows as [tools](https://modelcontextprotocol.io/docs/concepts/tools) that [MCP clients](https://modelcontextprotocol.io/clients) can use to generate responses.

In addition to serving flows through Langflow MCP servers, you can use Langflow as an MCP client to access any MCP server, including your Langflow MCP servers.

Interactions with Langflow MCP servers happen through the Langflow API's `/mcp` endpoints.

For more information, see [Use Langflow as an MCP server](/mcp-server) and [Use Langflow as an MCP client](/mcp-client).

## Run flows with the OpenAI Responses compatible endpoint {#openai-responses-api}

Langflow includes an OpenAI Responses API-compatible endpoint at `/api/v1/responses` that allows you to use existing OpenAI client libraries and code with minimal modifications.

For more information, see [OpenAI Responses API](/api-openai-responses).

## See also

* [Import and export flows](/concepts-flows-import)
* [Files endpoints](/api-files)
* [Test flows in the Playground](/concepts-playground)


================================================================================
filename: Flows/webhook.mdx
================================================================================

---
title: Trigger flows with webhooks
slug: /webhook
---

import Icon from "@site/src/components/icon";

You can use the **Webhook** component to start a flow run in response to an external event.

With the **Webhook** component, a flow can receive data directly from external sources. Then, the flow can parse the data and pass it to other components in the flow to initiate other actions, such as calling APIs, writing to databases, and chatting with LLMs.
If the input isn't valid JSON, the **Webhook** component wraps it in a `payload` object so that it can be accepted as input to trigger the flow.

The **Webhook** component provides a versatile entrypoint that can make your flows more event-driven and integrated with your entire stack of applications and services.
For example:

* Use an LLM to analyze the sentiment and content of customer feedback or survey responses.
* Receive notifications from a monitoring system, and then trigger automated responses based on alert type and severity.
* Integrate with e-commerce platforms to process orders and update inventory.

## Configure the Webhook component

To use the **Webhook** component in a flow, do the following:

1. In Langflow, open the flow where you want to use the **Webhook** component.

2. Add a [**Webhook** component](/components-data#webhook) and a [**Parser** component](/components-processing#parser) to your flow.

    These two components are commonly paired together because the **Parser** component extracts relevant data from the raw payload received by the **Webhook** component.

3. Connect the **Webhook** component's **Data** output to the **Parser** component's **Data** input.

4. In the **Parser** component's **Template** field, enter a template to parse the raw payload into structured text.

    In the template, use variables for payload keys in the same way you would define variables in a [**Prompt Template** component](/components-prompts).

    For example, assume that you expect your **Webhook** component to receive the following JSON data:

    ```json
    {
      "id": "",
      "name": "",
      "email": ""
    }
    ```

    Then, you can [use curly braces](/components-prompts#define-variables-in-prompts) to reference the JSON keys anywhere in your parser template:

    ```text
    ID: {id} - Name: {name} - Email: {email}
    ```

5. Connect the **Parser** component's **Parsed Text** output to the next logical component in your flow, such as a **Chat Input** component.

    If you want to test only the **Webhook** and **Parser** components, you can connect the **Parsed Text** output directly to a **Chat Output** component's **Text** input. Then, you can see the parsed data in the **Playground** after you run the flow.

6. From the **Webhook** component's **Endpoint** field, copy the API endpoint that you will use to send data to the **Webhook** component and trigger the flow.

    Alternatively, to get a complete `POST /v1/webhook/$FLOW_ID` code snippet, open the flow's [**API access** pane](/concepts-publish#api-access), and then click the **Webhook curl** tab.
    You can also modify the default curl command in the **Webhook** component's **curl** field.
    If this field isn't visible by default, click the **Webhook** component, and then click <Icon name="SlidersHorizontal" aria-hidden="true"/> **Controls** in the [component's header menu](/concepts-components#component-menus).

7. Send a POST request with `data` to the flow's `webhook` endpoint to trigger the flow.

    The following example sends a payload containing `id`, `name`, and `email` strings:

    ```bash
    curl -X POST "http://localhost:7860/api/v1/webhook/FLOW_ID" \
        -H "Content-Type: application/json" \
        -H "x-api-key: LANGFLOW_API_KEY" \
        -d '{"id": "12345", "name": "alex", "email": "alex@email.com"}'
    ```

    A successful response indicates that Langflow started the flow.
    The response doesn't include the output for the entire flow, only an indication that the flow started.

    ```json
    {
      "message": "Task started in the background",
      "status": "in progress"
    }
    ```

8. To view the flow's most recent parsed payload, click the **Parser** component, and then click <Icon name="TextSearch" aria-hidden="true"/> **Inspect output**.
For the preceding example, the parsed payload would be a string like `ID: 12345 - Name: alex - Email: alex@email.com`.

## Trigger flows with Composio webhooks

Typically, you won't manually trigger the **Webhook** component.
To learn about triggering flows with payloads from external applications, see the video tutorial [How to Use Webhooks in Langflow](https://www.youtube.com/watch?v=IC1CAtzFRE0).

## Troubleshoot flows with Webhook components

Use the following information to help address common issues that can occur with the **Webhook** component.

### Validate data received by the Webhook component

To troubleshoot a flow with a **Webhook** component and verify that the component is receiving data, you can create a small flow that outputs only the parsed payload:

1. Create a flow with **Webhook**, **Parser**, and **Chat Output** components.
2. Connect the **Webhook** component's **Data** output to the **Parser** component's **Data** input.
3. Connect the **Parser** component's **Parsed Text** output to the **Chat Output** component's **Text** input.
4. Edit the **Parser** component to set **Mode** to **Stringify**.

    This mode passes the data received by the **Webhook** component as a string that is printed by the **Chat Output** component.

5. Click **Share**, select **API access**, and then copy the **Webhook curl** code snippet.
6. Optional: Edit the `data` in the code snippet if you want to pass a different payload.
7. Send the POST request to trigger the flow.
8. Click **Playground** to verify that the **Chat Output** component printed the JSON data from your POST request.

### Parser component build failure

The **Parser** component can fail to build if it doesn't receive data from the **Webhook** component or if there is a problem with the incoming data.

If this occurs, try changing the **Parser** component's **Mode** to **Stringify** so that the component outputs the parsed payload as a single string.
Then, you can examine the string output and troubleshoot your parsing template, or work with the parsed data in string form.

## See also

- [Get started with the Langflow API](/api-reference-api-examples)
- [**Webhook** component](/components-data#webhook)
- [Flow trigger endpoints](/api-flows-run)


================================================================================
filename: Get-Started/about-langflow.mdx
================================================================================

---
title: What is Langflow?
slug: /
---

Langflow is an open-source, Python-based, customizable framework for building AI applications.
It supports important AI functionality like agents and the Model Context Protocol (MCP), and it doesn't require you to use specific large language models (LLMs) or vector stores.

The visual editor simplifies prototyping of application workflows, enabling developers to quickly turn their ideas into powerful, real-world solutions.

:::tip Try it
Build and run your first flow in minutes: [Install Langflow](/get-started-installation), and then try the [Quickstart](/get-started-quickstart).
:::

## Application development and prototyping

Langflow can help you develop a wide variety of AI applications, such as chatbots, document analysis systems, content generators, and agentic applications.

Langflow includes several pre-built templates that are ready to use or customize to your needs.

### Create flows in minutes

The primary purpose of Langflow is to create and serve flows, which are functional representations of application workflows.

To [build a flow](/concepts-flows), you connect and configure component nodes. Each component is a single step in the workflow.

With Langflow's [visual editor](/concepts-overview), you can drag and drop components to quickly build and test a functional AI application workflow.
For example, you could build a chatbot flow for an e-commerce store that uses an LLM and a product data store to allow customers to ask questions about the store's products.

![Basic Prompting flow in Langflow](/img/workspace-basic-prompting.png)

### Test flows in real-time

You can use the [**Playground**](/concepts-playground) to test flows without having to build your entire application stack.
You can interact with your flows and get real-time feedback about flow logic and response generation.

You can also run individual components to test dependencies in isolation.

### Run and serve flows

You can use your flows as prototypes for more formal application development, or you can use the Langflow API to embed your flows into your application code.

For more extensive development, you can build Langflow as a dependency or deploy a Langflow server to serve flows over the public internet.

For more information, see the following:

* [Trigger flows with the Langflow API](/concepts-publish)
* [Containerize a Langflow application](/develop-application)

## Endless modifications and integrations

Langflow provides [components](/concepts-components) that support many services, tools, and functionality that are required for AI applications.

Some components are generalized, such as inputs, outputs, and data stores.
Others are specialized, such as agents, language models, and embedding providers.

All components offer parameters that you can set to fixed or variable values. You can also use tweaks to temporarily override flow settings at runtime.

### Agent and MCP support

In addition to building agent flows with Langflow, you can leverage Langflow's built-in agent and MCP features:

* [Use Langflow Agents](/agents)
* [Use components and flows as agent tools](/agents-tools)
* [Use Langflow as an MCP server](/mcp-server)
* [Use Langflow as an MCP client](/mcp-client)

### Extensibility

In addition to the core components, Langflow supports custom components.

You can use custom components developed by others, and you can develop your own custom components for personal use or to share with other Langflow users.

For more information, see the following:

* [Contribute to Langflow](/contributing-how-to-contribute)
* [Create custom Python components](/components-custom-components)
* [Get help and request enhancements](/contributing-github-issues)

## Next steps

* [Install Langflow](/get-started-installation)
* [Quickstart](/get-started-quickstart)


================================================================================
filename: Get-Started/get-started-installation.mdx
================================================================================

---
title: Install Langflow
slug: /get-started-installation
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

Langflow can be installed in multiple ways:

* [**Langflow Desktop (Recommended)**](#install-and-run-langflow-desktop): Download and install the standalone desktop application for the least complicated setup experience.
This option includes dependency management and facilitated upgrades.

* [**Docker**](#install-and-run-langflow-docker): Pull and run the Langflow Docker image to start a Langflow container and run Langflow in isolation.

* [**Python package**](#install-and-run-the-langflow-oss-python-package): Install and run the Langflow OSS Python package.
This option offers more control over the environment, dependencies, and versioning.

* [**Install from source**](/contributing-how-to-contribute#install-langflow-from-source): Use this option if you want to contribute to the Langflow codebase or documentation.

## Install and run Langflow Desktop

Langflow Desktop is a desktop version of Langflow that simplifies dependency management and upgrades.
However, some features aren't available for Langflow Desktop, such as the **Shareable Playground** and **Voice Mode**.

<Tabs>
<TabItem value="macos" label="macOS" default>

Langflow Desktop requires macOS 13 or later.

1. Navigate to [Langflow Desktop](https://www.langflow.org/desktop).
2. Click **Download Langflow**, enter your contact information, and then click **Download**.
3. Mount and install the Langflow application.
4. When the installation completes, open the Langflow application, and then create your first flow with the [Quickstart](/get-started-quickstart).

</TabItem>
<TabItem value="Windows" label="Windows">

    :::warning
    If you are upgrading Langflow Desktop on Windows, **don't** use the in-app update feature to upgrade to Langflow version 1.6.0.
    For more information, see [Known issues for 1.6.0](/release-notes#windows-desktop-update-issue).
    :::

1. Navigate to [Langflow Desktop](https://www.langflow.org/desktop).
2. Click **Download Langflow**, enter your contact information, and then click **Download**.
3. Open the **File Explorer**, and then navigate to **Downloads**.
4. Double-click the downloaded `.msi` file, and then use the install wizard to install Langflow Desktop.

    :::tip
    Windows installations of Langflow Desktop require a C++ compiler that may not be present on your system. If you receive a `C++ Build Tools Required!` error, follow the on-screen prompt to install Microsoft C++ Build Tools, or [install Microsoft Visual Studio](https://visualstudio.microsoft.com/downloads/).
    :::

5. When the installation completes, open the Langflow application, and then create your first flow with the [Quickstart](/get-started-quickstart).

</TabItem>
</Tabs>

For upgrade information, see the [Release notes](/release-notes).

To manage dependencies in Langflow Desktop, see [Install custom dependencies in Langflow Desktop](/install-custom-dependencies#langflow-desktop).

## Install and run Langflow with Docker {#install-and-run-langflow-docker}

You can use the Langflow Docker image to start a Langflow container.
For more information, see [Deploy Langflow on Docker](/deployment-docker).

1. Install and start [Docker](https://docs.docker.com/).

2. Pull the latest [Langflow Docker image](https://hub.docker.com/r/langflowai/langflow) and start it:

    ```bash
    docker run -p 7860:7860 langflowai/langflow:latest
    ```

3. To access Langflow, navigate to `http://localhost:7860/`.

4. Create your first flow with the [Quickstart](/get-started-quickstart).

## Install and run the Langflow OSS Python package

1. Make sure you have the required dependencies and infrastructure:

    - [Python](https://www.python.org/downloads/release/python-3100/)
       - macOS and Linux: Version 3.10 to 3.13
       - Windows: Version 3.10 to 3.12
    - [uv](https://docs.astral.sh/uv/getting-started/installation/)
    - Sufficient infrastructure:
       - Minimum: Dual-core CPU and 2 GB RAM
       - Recommended: Multi-core CPU and at least 4 GB RAM

2. Create a virtual environment with [uv](https://docs.astral.sh/uv/pip/environments).

    <details>
    <summary>Need help with virtual environments?</summary>

    Virtual environments ensure Langflow is installed in an isolated, fresh environment.
    To create a new virtual environment, do the following.

        <Tabs>
        <TabItem value="linux-macos" label="Linux or macOS" default>

        1. Navigate to where you want your virtual environment to be created, and then create it with `uv`:

            ```shell
            uv venv VENV_NAME
            ```

            Replace `VENV_NAME` with a name for your virtual environment.

        2. Start the virtual environment:

            ```shell
            source VENV_NAME/bin/activate
            ```

            Your shell's prompt changes to display that you're currently working in a virtual environment:

            ```text
            (VENV_NAME) âžœ  langflow git:(main) âœ—
            ```

        3. To deactivate the virtual environment and return to your regular shell, type `deactivate`.

            When activated, the virtual environment temporarily modifies your `PATH` variable to prioritize packages installed within the virtual environment.
            To avoid conflicts with other projects, it's a good idea to deactivate your virtual environment when you're done working in it.

            To delete the virtual environment, type `rm -rf VENV_NAME`.
            This completely removes the virtual environment directory and its contents.

        </TabItem>
        <TabItem value="Windows" label="Windows">

        1. Navigate to where you want your virtual environment to be created, and create it with `uv`.

            ```shell
            uv venv VENV_NAME
            ```

            Replace `VENV_NAME` with a name for your virtual environment.

        2. Start the virtual environment:

            ```shell
            VENV_NAME\Scripts\activate
            ```

            Your shell's prompt changes to display that you're currently working in a virtual environment:

            ```text
            (VENV_NAME) PS C:/users/username/langflow-dir>
            ```

        3. To deactivate the virtual environment and return to your regular shell, type `deactivate`.

            When activated, the virtual environment temporarily modifies your `PATH` variable to prioritize packages installed within the virtual environment.
            To avoid conflicts with other projects, it's a good idea to deactivate your virtual environment when you're done working in it.

            To delete the virtual environment, type `Remove-Item VENV_NAME`.
            This completely removes the virtual environment directory and its contents.

        </TabItem>
        </Tabs>

    </details>

3. In your virtual environment, install Langflow:

    ```bash
    uv pip install langflow
    ```

    To install a specific version of the Langflow package, add the required version to the command, such as `uv pip install langflow==1.4.22`.

    <details>
    <summary>Reinstall or upgrade Langflow</summary>

    To reinstall Langflow and all of its dependencies, run `uv pip install langflow --force-reinstall`.

    To upgrade Langflow to the latest version, run `uv pip install langflow -U`.
    However, the Langflow team recommends taking steps to backup your existing installation before you upgrade Langflow.
    For more information, see [Prepare to upgrade](/release-notes#prepare-to-upgrade).

    </details>

4. Start Langflow:

    ```bash
    uv run langflow run
    ```

    It can take a few minutes for Langflow to start.

5. To confirm that a local Langflow instance is running, navigate to the default Langflow URL `http://127.0.0.1:7860`.

6. Create your first flow with the [Quickstart](/get-started-quickstart).

For upgrade information, see the [Release notes](/release-notes).

For information about optional dependency groups and support for custom dependencies to extend Langflow OSS functionality, see [Install custom dependencies](/install-custom-dependencies).

## Next steps

* [Quickstart](/get-started-quickstart): Build and run your first flow in minutes.
* [Build flows](/concepts-flows): Learn about building flows.
* [Troubleshoot Langflow](/troubleshoot): Get help with common Langflow install and startup issues.


================================================================================
filename: Get-Started/get-started-quickstart.mdx
================================================================================

---
title: Quickstart
slug: /get-started-quickstart
---

import Icon from "@site/src/components/icon";
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

Get started with Langflow by loading a template flow, running it, and then serving it at the `/run` API endpoint.

## Prerequisites

- [Install and start Langflow](/get-started-installation)
- Create an [OpenAI API key](https://platform.openai.com/api-keys)
- Create a [Langflow API key](/api-keys-and-authentication)

    <details>
    <summary>Create a Langflow API key</summary>

    A Langflow API key is a user-specific token you can use with Langflow.

    To create a Langflow API key, do the following:

    1. In Langflow, click your user icon, and then select **Settings**.
    2. Click **Langflow API Keys**, and then click <Icon name="Plus" aria-hidden="true"/> **Add New**.
    3. Name your key, and then click **Create API Key**.
    4. Copy the API key and store it securely.
    5. To use your Langflow API key in a request, set a `LANGFLOW_API_KEY` environment variable in your terminal, and then include an `x-api-key` header or query parameter with your request.
    For example:

        ```bash
        # Set variable
        export LANGFLOW_API_KEY="sk..."

        # Send request
        curl --request POST \
        --url "http://LANGFLOW_SERVER_ADDRESS/api/v1/run/FLOW_ID" \
        --header "Content-Type: application/json" \
        --header "x-api-key: $LANGFLOW_API_KEY" \
        --data '{
          "output_type": "chat",
          "input_type": "chat",
          "input_value": "Hello"
        }'
        ```

    </details>

## Run the Simple Agent template flow

1. In Langflow, click **New Flow**, and then select the **Simple Agent** template.

![Simple Agent template](/img/quickstart-simple-agent-flow.png)

The **Simple Agent** template consists of an [**Agent** component](/agents) connected to [**Chat Input** and **Chat Output** components](/components-io), a [**Calculator** component](/components-helpers#calculator), and a [**URL** component](/components-data#url). When you run this flow, you submit a query to the agent through the **Chat Input** component, the agent uses the **Calculator** and **URL** tools to generate a response, and then returns the response through the **Chat Output** component.

Many components can be tools for agents, including [Model Context Protocol (MCP) servers](/mcp-server). The agent decides which tools to call based on the context of a given query.

2. In the **Agent** component, enter your OpenAI API key directly or use a <Icon name="Globe" aria-hidden="true"/> [global variable](/configuration-global-variables).

    This example uses the **Agent** component's built-in OpenAI model.
    If you want to use a different provider, edit the model provider, model name, and credentials accordingly.
    If your preferred provider or model isn't listed, set **Model Provider** to **Connect other models**, and then connect any [language model component](/components-models#additional-language-models).

3. To run the flow, click <Icon name="Play" aria-hidden="true"/> **Playground**.

4. To test the **Calculator** tool, ask the agent a simple math question, such as `I want to add 4 and 4.`
To help you test and evaluate your flows, the **Playground** shows the agent's reasoning process as it analyzes the prompt, selects a tool, and then uses the tool to generate a response.
In this case, a math question causes the agent to select the **Calculator** tool and use an action like `evaluate_expression`.

![Playground with Agent tool](/img/quickstart-simple-agent-playground.png)

5. To test the **URL** tool, ask the agent about current events.
For this request, the agent selects the **URL** tool's `fetch_content` action, and then returns a summary of current news headlines.

6. When you are done testing the flow, click <Icon name="X" aria-hidden="true"/>**Close**.

:::tip Next steps
Now that you've run your first flow, try these next steps:

- Edit your **Simple Agent** flow by attaching different tools or adding more [components](/concepts-components) to the flow.
- [Build your own flows](/concepts-flows) from scratch or by modifying other template flows.
- Integrate flows into your applications, as explained in [Run your flows from external applications](#run-your-flows-from-external-applications).
:::

## Run your flows from external applications

Langflow is an IDE, but it's also a runtime you can call through the [Langflow API](/api-reference-api-examples) with Python, JavaScript, or HTTP.

When you start Langflow locally, you can send requests to the local Langflow server.
For production applications, you need to [deploy a stable Langflow instance](/deployment-overview) to handle API calls.

For example, you can use the `/run` endpoint to run a flow and get the result.

Langflow provides code snippets to help you get started with the Langflow API.

1. When editing a flow, click **Share**, and then click **API access**.

    The default code in the API access pane constructs a request with the Langflow server `url`, `headers`, and a `payload` of request data.
    The code snippets automatically include the `LANGFLOW_SERVER_ADDRESS` and `FLOW_ID` values for the flow, and a script to include your `LANGFLOW_API_KEY` if you've set it as an environment variable in your terminal session.
    Replace these values if you're using the code for a different server or flow.
    The default Langflow server address is `http://localhost:7860`.

    <Tabs>
    <TabItem value="Python" label="Python" default>

    ```python
    import requests

    url = "http://LANGFLOW_SERVER_ADDRESS/api/v1/run/FLOW_ID"  # The complete API endpoint URL for this flow

    # Request payload configuration
    payload = {
        "output_type": "chat",
        "input_type": "chat",
        "input_value": "hello world!"
    }

    # Request headers
    headers = {
        "Content-Type": "application/json",
        "x-api-key": "$LANGFLOW_API_KEY"
    }

    try:
        # Send API request
        response = requests.request("POST", url, json=payload, headers=headers)
        response.raise_for_status()  # Raise exception for bad status codes

        # Print response
        print(response.text)

    except requests.exceptions.RequestException as e:
        print(f"Error making API request: {e}")
    except ValueError as e:
        print(f"Error parsing response: {e}")
    ```

    </TabItem>
    <TabItem value="JavaScript" label="JavaScript">

    ```js
    const payload = {
        "output_type": "chat",
        "input_type": "chat",
        "input_value": "hello world!",
        "session_id": "user_1"
    };

    const options = {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'x-api-key': 'LANGFLOW_API_KEY'
        },
        body: JSON.stringify(payload)
    };

    fetch('http://LANGFLOW_SERVER_ADDRESS/api/v1/run/FLOW_ID', options)
        .then(response => response.json())
        .then(response => console.log(response))
        .catch(err => console.error(err));
    ```

    </TabItem>
    <TabItem value="curl" label="curl">

    ```bash
    curl --request POST \
         --url 'http://LANGFLOW_SERVER_ADDRESS/api/v1/run/FLOW_ID?stream=false' \
         --header 'Content-Type: application/json' \
         --header "x-api-key: LANGFLOW_API_KEY" \
         --data '{
          "output_type": "chat",
          "input_type": "chat",
          "input_value": "hello world!"
          }'

    # A 200 response confirms the call succeeded.
    ```

    </TabItem>
    </Tabs>

2. Copy the snippet, paste it in a script file, and then run the script to send the request.
If you are using the curl snippet, you can run the command directly in your terminal.

If the request is successful, the response includes many details about the flow run, including the session ID, inputs, outputs, components, durations, and more.
The following is an example of a response from running the **Simple Agent** template flow:

<details>
<summary>Result</summary>

```json
{
  "session_id": "29deb764-af3f-4d7d-94a0-47491ed241d6",
  "outputs": [
    {
      "inputs": {
        "input_value": "hello world!"
      },
      "outputs": [
        {
          "results": {
            "message": {
              "text_key": "text",
              "data": {
                "timestamp": "2025-06-16 19:58:23 UTC",
                "sender": "Machine",
                "sender_name": "AI",
                "session_id": "29deb764-af3f-4d7d-94a0-47491ed241d6",
                "text": "Hello world! ðŸŒ How can I assist you today?",
                "files": [],
                "error": false,
                "edit": false,
                "properties": {
                  "text_color": "",
                  "background_color": "",
                  "edited": false,
                  "source": {
                    "id": "Agent-ZOknz",
                    "display_name": "Agent",
                    "source": "gpt-4o-mini"
                  },
                  "icon": "bot",
                  "allow_markdown": false,
                  "positive_feedback": null,
                  "state": "complete",
                  "targets": []
                },
                "category": "message",
                "content_blocks": [
                  {
                    "title": "Agent Steps",
                    "contents": [
                      {
                        "type": "text",
                        "duration": 2,
                        "header": {
                          "title": "Input",
                          "icon": "MessageSquare"
                        },
                        "text": "**Input**: hello world!"
                      },
                      {
                        "type": "text",
                        "duration": 226,
                        "header": {
                          "title": "Output",
                          "icon": "MessageSquare"
                        },
                        "text": "Hello world! ðŸŒ How can I assist you today?"
                      }
                    ],
                    "allow_markdown": true,
                    "media_url": null
                  }
                ],
                "id": "f3d85d9a-261c-4325-b004-95a1bf5de7ca",
                "flow_id": "29deb764-af3f-4d7d-94a0-47491ed241d6",
                "duration": null
              },
              "default_value": "",
              "text": "Hello world! ðŸŒ How can I assist you today?",
              "sender": "Machine",
              "sender_name": "AI",
              "files": [],
              "session_id": "29deb764-af3f-4d7d-94a0-47491ed241d6",
              "timestamp": "2025-06-16T19:58:23+00:00",
              "flow_id": "29deb764-af3f-4d7d-94a0-47491ed241d6",
              "error": false,
              "edit": false,
              "properties": {
                "text_color": "",
                "background_color": "",
                "edited": false,
                "source": {
                  "id": "Agent-ZOknz",
                  "display_name": "Agent",
                  "source": "gpt-4o-mini"
                },
                "icon": "bot",
                "allow_markdown": false,
                "positive_feedback": null,
                "state": "complete",
                "targets": []
              },
              "category": "message",
              "content_blocks": [
                {
                  "title": "Agent Steps",
                  "contents": [
                    {
                      "type": "text",
                      "duration": 2,
                      "header": {
                        "title": "Input",
                        "icon": "MessageSquare"
                      },
                      "text": "**Input**: hello world!"
                    },
                    {
                      "type": "text",
                      "duration": 226,
                      "header": {
                        "title": "Output",
                        "icon": "MessageSquare"
                      },
                      "text": "Hello world! ðŸŒ How can I assist you today?"
                    }
                  ],
                  "allow_markdown": true,
                  "media_url": null
                }
              ],
              "duration": null
            }
          },
          "artifacts": {
            "message": "Hello world! ðŸŒ How can I assist you today?",
            "sender": "Machine",
            "sender_name": "AI",
            "files": [],
            "type": "object"
          },
          "outputs": {
            "message": {
              "message": "Hello world! ðŸŒ How can I assist you today?",
              "type": "text"
            }
          },
          "logs": {
            "message": []
          },
          "messages": [
            {
              "message": "Hello world! ðŸŒ How can I assist you today?",
              "sender": "Machine",
              "sender_name": "AI",
              "session_id": "29deb764-af3f-4d7d-94a0-47491ed241d6",
              "stream_url": null,
              "component_id": "ChatOutput-aF5lw",
              "files": [],
              "type": "text"
            }
          ],
          "timedelta": null,
          "duration": null,
          "component_display_name": "Chat Output",
          "component_id": "ChatOutput-aF5lw",
          "used_frozen_result": false
        }
      ]
    }
  ]
}
```

</details>

In a production application, you probably want to select parts of this response to return to the user, store in logs, and so on. The next steps demonstrate how you can extract data from a Langflow API response to use in your application.

### Extract data from the response

The following example builds on the API pane's example code to create a question-and-answer chat in your terminal that stores the agent's previous answer.

1. Incorporate your **Simple Agent** flow's `/run` snippet into the following script.
This script runs a question-and-answer chat in your terminal and stores the agent's previous answer so you can compare them.

    <Tabs>
    <TabItem value="Python" label="Python" default>

    ```python
    import requests
    import json

    url = "http://LANGFLOW_SERVER_ADDRESS/api/v1/run/FLOW_ID"

    def ask_agent(question):
        payload = {
            "output_type": "chat",
            "input_type": "chat",
            "input_value": question,
        }

        headers = {
        "Content-Type": "application/json",
        "x-api-key": "LANGFLOW_API_KEY"
        }

        try:
            response = requests.post(url, json=payload, headers=headers)
            response.raise_for_status()

            # Get the response message
            data = response.json()
            message = data["outputs"][0]["outputs"][0]["outputs"]["message"]["message"]
            return message

        except Exception as e:
            return f"Error: {str(e)}"

    def extract_message(data):
        try:
            return data["outputs"][0]["outputs"][0]["outputs"]["message"]["message"]
        except (KeyError, IndexError):
            return None

    # Store the previous answer from ask_agent response
    previous_answer = None

    # the terminal chat
    while True:
        # Get user input
        print("\nAsk the agent anything, such as 'What is 15 * 7?' or 'What is the capital of France?')")
        print("Type 'quit' to exit or 'compare' to see the previous answer")
        user_question = input("Your question: ")

        if user_question.lower() == 'quit':
            break
        elif user_question.lower() == 'compare':
            if previous_answer:
                print(f"\nPrevious answer was: {previous_answer}")
            else:
                print("\nNo previous answer to compare with!")
            continue

        # Get and display the answer
        result = ask_agent(user_question)
        print(f"\nAgent's answer: {result}")
        # Store the answer for comparison
        previous_answer = result
    ```

    </TabItem>
    <TabItem value="JavaScript" label="JavaScript">

    ```js
    const readline = require('readline');

    const rl = readline.createInterface({
        input: process.stdin,
        output: process.stdout
    });

    const url = 'http://LANGFLOW_SERVER_ADDRESS/api/v1/run/FLOW_ID';

    // Store the previous answer from askAgent response
    let previousAnswer = null;

    // the agent flow, with question as input_value
    async function askAgent(question) {
        const payload = {
            "output_type": "chat",
            "input_type": "chat",
            "input_value": question
        };

        const options = {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'x-api-key': 'LANGFLOW_API_KEY'
            },
            body: JSON.stringify(payload)
        };

        try {
            const response = await fetch(url, options);
            const data = await response.json();

            // Extract the message from the nested response
            const message = data.outputs[0].outputs[0].outputs.message.message;
            return message;
        } catch (error) {
            return `Error: ${error.message}`;
        }
    }

    // the terminal chat
    async function startChat() {
        console.log("\nAsk the agent anything, such as 'What is 15 * 7?' or 'What is the capital of France?'");
        console.log("Type 'quit' to exit or 'compare' to see the previous answer");

        const askQuestion = () => {
            rl.question('\nYour question: ', async (userQuestion) => {
                if (userQuestion.toLowerCase() === 'quit') {
                    rl.close();
                    return;
                }

                if (userQuestion.toLowerCase() === 'compare') {
                    if (previousAnswer) {
                        console.log(`\nPrevious answer was: ${previousAnswer}`);
                    } else {
                        console.log("\nNo previous answer to compare with!");
                    }
                    askQuestion();
                    return;
                }

                const result = await askAgent(userQuestion);
                console.log(`\nAgent's answer: ${result}`);
                previousAnswer = result;
                askQuestion();
            });
        };

        askQuestion();
    }

    startChat();
    ```

    </TabItem>
    </Tabs>

2. To view the agent's previous answer, type `compare`. To close the terminal chat, type `exit`.

### Use tweaks to apply temporary overrides to a flow run

You can include tweaks with your requests to temporarily modify flow parameters.
Tweaks are added to the API request, and temporarily change component parameters within your flow.
Tweaks override the flow's components' settings for a single run only.
They don't modify the underlying flow configuration or persist between runs.

Tweaks are added to the `/run` endpoint's `payload`.
To assist with formatting, you can define tweaks in Langflow's **Input Schema** pane before copying the code snippet.

1. To open the **Input Schema** pane, from the **API access** pane, click **Input Schema**.
2. In the **Input Schema** pane, select the parameter you want to modify in your next request.
Enabling parameters in the **Input Schema** pane doesn't permanently change the listed parameters. It only adds them to the sample code snippets.
3. For example, to change the LLM provider from OpenAI to Groq, and include your Groq API key with the request, select the values **Model Providers**, **Model**, and **Groq API Key**.
Langflow updates the `tweaks` object in the code snippets based on your input parameters, and includes default values to guide you.
Use the updated code snippets in your script to run your flow with your overrides.

```json
payload = {
    "output_type": "chat",
    "input_type": "chat",
    "input_value": "hello world!",
    "tweaks": {
        "Agent-ZOknz": {
            "agent_llm": "Groq",
            "api_key": "GROQ_API_KEY",
            "model_name": "llama-3.1-8b-instant"
        }
    }
}
```

## Next steps

* [Trigger flows with the Langflow API](/concepts-publish)
* [Use Langflow as a Model Context Protocol (MCP) server](/mcp-server)
* [Containerize a Langflow application](/develop-application)
* [File management](/concepts-file-management)


================================================================================
filename: Support/contributing-github-issues.mdx
================================================================================

---
title: Get help and request enhancements
slug: /contributing-github-issues
---

The Langflow GitHub repository is an integral part of the [Langflow community](/contributing-community).

In addition to general assistance with Langflow, the repository is the best place to report bugs and request enhancements to ensure that they are tracked by Langflow maintainers.

## GitHub issues

The [Issues page in the Langflow repository](https://github.com/langflow-ai/langflow/issues) is actively updated with bugs and feature requests.

:::tip
The best way to promote a request or bug is to comment on an existing issue.
Highly active issues are more likely to receive attention from contributors.

Before you report a bug or submit a feature request, search for existing similar issues.
Use the [Langflow repository's labels](https://github.com/langflow-ai/langflow/labels) to help filter your search.
:::

## GitHub discussions

If you need help with your code or Langflow in general, you can visit the [Langflow GitHub Discussions page](https://github.com/langflow-ai/langflow/discussions) or reach out through other [Langflow community](/contributing-community) channels.

The Langflow team doesn't provide individual support over email, and the team believes that public discussions help more users by virtue of their discoverability.

## Community guidelines and tips

Because the Issues and Discussion pages are public, the Langflow team asks that you follow these guidelines when submitting questions and issues:

* **Provide as many details as possible**: Simply stating that a feature doesn't work isn't helpful. The Langflow team needs details in order to recreate and find the issue.
* **Explain what exactly went wrong**: Including error messages and descriptions of _how_ your code failed, not just the fact that it failed.
* **Retrace your steps**: Explain what happened before the error, what you expected to happen instead of the error, and any recent changes you made, such as upgrading Langflow or a dependency.
* **Describe your environment**: Include your operating system, Langflow version, Python version, and any other environment-related details that could have contributed to the issue.
* **Include snippets of the code that failed**: Be sure to omit any sensitive values, and only provide parts relevant to the failure, rather than the entire script. Providing code snippets makes it much easier to reproduce errors, troubleshoot, and provide specific advice.
  * If your submission includes long sections of code, logs, or tracebacks, wrap them in [details tags](https://developer.mozilla.org/en/docs/Web/HTML/Element/details) (`<details> PASTE CODE HERE </details>`) to collapse the content and make it easier to read your submission.
* **Omit sensitive information**: Other than the information available on your public GitHub profile, don't include sensitive or personally identifying data, such as security keys, full names, personal identification numbers, addresses, and phone numbers.
* **Be kind**: Although bugs can be frustrating with any software, remember that your messages are read by real people who want to help. While you don't have to be saccharine, there's no need to be rude to get support.
  * Your issues and discussions are attached to your GitHub account, and they can be read by anyone on the internet, including current and potential employers and colleagues.
  * The Langlow repository is a public GitHub repository and, therefore, subject to the [GitHub Code of Conduct](https://docs.github.com/en/site-policy/github-terms/github-community-code-of-conduct).


================================================================================
filename: Support/luna-for-langflow.mdx
================================================================================

---
title: IBM Elite Support for Langflow
slug: /luna-for-langflow
---

IBM Elite Support offers enterprise support for Langflow to help you develop and deploy Langflow applications with confidence.

IBM Elite Support for Langflow is a subscription to the Langflow expertise at IBM.
It's meant for Langflow users who want all the benefits of running their own open-source deployments, as well as the peace of mind that comes with having direct access to the team that has authored the majority of the Langflow code.

IBM Elite Support subscribers can get help with general-purpose and technical questions for their open-source Langflow deployments.
If an issue is encountered, IBM is there to help.

To subscribe or learn more, see [IBM Elite Support for Langflow](https://ibm.com/docs/esfl?topic=elite-support-langflow-specification).


================================================================================
filename: Support/release-notes.mdx
================================================================================

---
title: Langflow release notes
slug: /release-notes
---

import Icon from "@site/src/components/icon";
import McpIcon from '@site/static/logos/mcp-icon.svg';

This page summarizes significant changes to Langflow in each release.
For all changes, see the [Changelog](https://github.com/langflow-ai/langflow/releases/latest).

Due to strict SemVer requirements, Langflow Desktop can have different patch versions than the core Langflow OSS Python package, but the major and minor versions are aligned.

## Prepare to upgrade

:::warning
Whenever possible, the Langflow team recommends installing new Langflow versions in a new virtual environment or VM before upgrading your primary installation.
This allows you to [import flows](/concepts-flows-import#import-a-flow) from your existing installation and test them in the new version without disrupting your existing installation.
In the event of breaking changes or bugs, your existing installation is preserved in a stable state.
If you are upgrading Windows Desktop to 1.6.0, don't auto-upgrade with the in-app **Update** button. Instead, follow the instructions in [Known issue: Don't auto-upgrade Windows Desktop](#windows-desktop-update-issue).
:::

To avoid the impact of potential breaking changes and test new versions, the Langflow team recommends the following upgrade process:

1. Recommended: [Export your projects](/api-projects#export-a-project) to create backups of your flows:

    ```bash
    curl -X GET \
    "$LANGFLOW_SERVER_URL/api/v1/projects/download/$PROJECT_ID" \
      -H "accept: application/json" \
      -H "x-api-key: $LANGFLOW_API_KEY"
    ```

   To export flows from the visual editor, see [Import and export flows](/concepts-flows-import).

2. Install the new version:

   * **Langflow OSS Python package**: Install the new version in a new virtual environment. For instructions, see [Install and run the Langflow OSS Python package](/get-started-installation#install-and-run-the-langflow-oss-python-package).
   * **Langflow Docker image**: Run the new image in a separate container.
   * **Langflow Desktop**: To upgrade in place, open Langflow Desktop, and then click **Upgrade Available** in the Langflow header. If you want to isolate the new version, you must install Langflow Desktop on a separate physical or virtual machine, and then [import your flows](/concepts-flows-import) to the new installation.

3. [Import your flows](/concepts-flows-import) to test them in the new version, [upgrading components](/concepts-components#component-versions) as needed.

    When upgrading components, you can use the **Create backup flow before updating** option if you didn't previously export your flows.

4. If you installed the new version in isolation, upgrade your primary installation after testing the new version.

    If you made changes to your flows in the isolated installation, you might want to export and import those flows back to your upgraded primary installation so you don't have to repeat the component upgrade process.

## 1.6.0

Highlights of this release include the following changes.
For all changes, see the [Changelog](https://github.com/langflow-ai/langflow/releases).

### Known issue, potential security vulnerability: .env file not loaded in versions 1.6.0 through 1.6.3 {#env-file-bug}

Langflow versions 1.6.0 through 1.6.3 have a critical bug where environment variables from `.env` files aren't read.
This affects all deployments using environment variables for configuration, including security settings.

:::warning Potential security vulnerability
If your `.env` file includes `AUTO_LOGIN=false`, upgrading to the impacted versions causes Langflow to fall back to default settings, potentially giving all users superuser access immediately upon upgrade.
Additionally, database credentials, API keys, and other sensitive configurations can't be loaded from `.env` files.

_Don't_ upgrade to any Langflow version from 1.6.0 through 1.6.3 if you use `.env` files for configuration.
Instead, upgrade to 1.6.4, which includes a fix for this bug.
:::

### Known issue: Don't auto-upgrade Windows Desktop {#windows-desktop-update-issue}

:::warning
Windows users of Langflow Desktop should **not** use the in-app update feature to upgrade to Langflow version 1.6.0.
Only Windows Desktop users upgrading to Langflow version 1.6.0 are affected.
:::

The **Update** button in Langflow Desktop will not work for Windows users, and may result in data loss.

Instead, download a fresh installation from the [Langflow website](https://langflow.org) when a new version is available.

Follow the instructions below to minimize the risk of losing flows.

These instructions assume the default path for the Langflow database of `C:\Users\YOUR_USERNAME\AppData\Roaming\com.langflow\data\database.db` or `C:\Users\%YOUR_USERNAME%\AppData\Roaming\com.langflow\data\database.db`. Replace `YOUR_USERNAME` with your Windows username.

1. Ensure you have Administrator privileges.
2. Ensure you have enough disk space for a second installation of Langflow.
3. Close Langflow, and ensure no Langflow process is running in Task Manager.
4. The Langflow 1.6.0 installer automatically performs a database backup in a later step, but an additional manual backup provides additional redundancy and is recommended.
To manually back up your Langflow database file, do the following:
   1. In Windows Explorer, navigate to `C:\Users\YOUR_USERNAME\AppData\Roaming\com.langflow\data\`. Replace `YOUR_USERNAME` with your Windows username.
   2. Copy the `database.db` at this location, and paste it to a safe location.
5. Download the Windows installer from the [Langflow website](https://langflow.org).
6. Run the Windows installer as an Administrator. To run the installer as an Administrator, right-click the executable and select **Run as administrator**.
7. Follow the Windows installer's guided steps.
The Langflow 1.6.0 installer automatically performs a database backup.
These steps install Langflow from scratch, and result in two Langflow installations: the previously installed version, and version 1.6.0.
This is expected behavior.
8. Start version 1.6.0 of Langflow, and confirm your flows behave as expected.
9. **If flows are missing**, restore your flows from your manual backup by doing the following:
    1. Close Langflow.
    2. Navigate to your backup location, and copy the `database.db` file.
    3. Replace the database file in the new installation's `data` directory.
    4. Start Langflow, and confirm your flows behave as expected.
    5. After confirmation, uninstall the previous version of Langflow, and keep version 1.6.0.

### Breaking changes

- Authentication enforced for Langflow API requests by default

    In Langflow version 1.6, `LANGFLOW_AUTO_LOGIN=True` and `LANGFLOW_SKIP_AUTH_AUTO_LOGIN=False` by default.
    This enforces authentication for Langflow API requests while still automatically authenticating all users as superusers in the visual editor.

    **This is a breaking change from 1.5 where both of these environment variables were `true` by default, bypassing all authentication.**

    For temporary backwards compatibility, you can revert to the earlier unauthenticated behavior by setting both variables to `true`.
    However, a future release will set `LANGFLOW_AUTO_LOGIN=False` and remove `LANGFLOW_SKIP_AUTH_AUTO_LOGIN`.
    At that point, Langflow will strictly enforce API key authentication for API requests, and you can manually disable authentication for some features, like the visual editor, by setting `LANGFLOW_AUTO_LOGIN=True`.

    For more information, see the documentation for [`LANGFLOW_AUTO_LOGIN`](/api-keys-and-authentication#langflow-auto-login).

### New features and enhancements

- OpenAI Responses API compatibility

  Langflow now includes an endpoint that is compatible with the [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses) at `POST /api/v1/responses`.
  This allows you to use existing OpenAI client libraries with minimal code changes by replacing the `model` name with your `flow_id`.
  The endpoint supports streaming responses, conversation continuity, tool call results, and global variable passing through headers.
  For more information, see [OpenAI Responses API](/api-openai-responses).

- Advanced document parsing with built-in Docling support

  The **File** component supports advanced parsing with the Docling library.

  To make it easier to use the [**Docling** components](/bundles-docling) and the **File** component's new advanced parsing feature, the Docling dependency is now included with Langflow for all operating systems except macOS Intel (x86_64).

  For more information, see [Advanced parsing](/components-data#advanced-parsing).

- Reorganized component menus and visual editor controls

  - The [workspace](/concepts-overview#workspace) sidebar is divided into separate sections for <Icon name="Search" aria-hidden="true" /> **Search**, <Icon name="Component" aria-hidden="true" /> **Core components**, <McpIcon /> [**MCP servers**](/mcp-server), <Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components), and <Icon name="StickyNote" aria-hidden="true"/> **Add Note**.
  - Lock/unlock controls moved to flow details in [**Projects**](/concepts-flows#projects).
  - Zoom and help controls moved to the lower-right corner of the workspace.
  - Vector store components moved to provider-specific <Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components)
  - **Serper Google Search API** component moved to the **Serper** bundle

- Increased the default maximum file upload size from `100 MB` to `1024 MB`.

- New integrations and bundles:

  - [Traceloop](/integrations-instana-traceloop)
  - [Cohere Rerank](/bundles-cohere)
  - [S3 Bucket Uploader](/bundles-amazon)
  - [Composio single-service components](/bundles-composio)

### Deprecations

- The **Local DB** component is now in legacy status.
Replace this component with the [**Chroma DB** component](/bundles-chroma).

## 1.5.0

Highlights of this release include the following changes.
For all changes, see the [Changelog](https://github.com/langflow-ai/langflow/releases).

### New features and enhancements

- Langflow API requests can require authentication

    To enhance security and ensure proper authentication for automatic login features, Langflow API endpoints now require authentication with a Langflow API key, even when `LANGFLOW_AUTO_LOGIN=True`.
    This change will be enforced in a future release.
    For temporary backwards compatibility, this release adds the `LANGFLOW_SKIP_AUTH_AUTO_LOGIN` environment variable.
    The default value is `true`, which disables API authentication enforcement.
    To enforce API authentication, set `LANGFLOW_SKIP_AUTH_AUTO_LOGIN=False`.
    For more information, see the documentation for [`LANGFLOW_AUTO_LOGIN`](/api-keys-and-authentication#langflow-auto-login).

- Centralized **Language Model** and **Embedding Model** components

    The [**Language Model** component](/components-models) and [**Embedding Model** component](/components-embedding-models) are now core components for your LLM and embeddings flows. They support multiple models and model providers, and allow you to experiment with different models without swapping out single-provider components.
    Find them in the visual editor in the **Models** category.

    The single-provider components moved to the [**Bundles**](/components-bundle-components) section.
    You can use them to replace the **Language Model** and **Embedding Model** core components, or connect them to the **Agent** component with the **Connect other models** provider option.

- MCP server one-click installation

    On your Langflow project's **MCP server** page, click **Auto install** to install your Langflow MCP server to MCP clients with just one click.
    The option to install with a JSON configuration file is available for macOS, Windows, and WSL.
    For more information, see [Use Langflow as an MCP server](/mcp-server).

- MCP server management

    You can now add, remove, and edit your MCP servers in the **MCP Tools** components and through your Langflow **Settings** page.
    For more information, see [Use Langflow as an MCP client](/mcp-client).

- Input schema replaces temporary overrides

    The **Input schema** pane replaces the need to manage tweak values in the **API access** pane. When you enable a parameter in the **Input schema** pane, the parameter is automatically added to your flow's code snippets, providing ready-to-use templates for making requests in your preferred programming language.

- Tools components are redistributed

    All components in the **Tools** category were moved to other component categories, such as **Helpers** and [**Bundles**](/components-bundle-components), or marked as legacy.

    The [**MCP Tools** component](/mcp-client) is now under the **Agents** category.

    Tools that performed the same function were combined into single components that support multiple providers, such as the [**Web Search** component](/components-data#web-search) and the [**News Search** component](/components-data#news-search).

    For more information, see [Tools components](/components-tools).

- Stability improvements

    General stability improvements and bug fixes for enhanced reliability.
    See an issue? [Raise it on GitHub](https://github.com/langflow-ai/langflow/issues).

- New integrations and bundles

   - [**Cleanlab** bundle](/bundles-cleanlab)

## 1.4.2

Highlights of this release include the following changes.
For all changes, see the [Changelog](https://github.com/langflow-ai/langflow/releases).

### New features and enhancements

- Enhanced file and flow management system with improved bulk capabilities.
- Added the **BigQuery** component
- Added the **Twelve Labs** bundle
- Added the **NVIDIA System-Assist** component

### Deprecations

- Deprecated the **Combine Text** component.

## 1.4.1

Highlights of this release include the following changes.
For all changes, see the [Changelog](https://github.com/langflow-ai/langflow/releases).

### New features and enhancements

- Added an enhanced **Breaking Changes** feature to help update components without breaking flows after updating Langflow.

## 1.4.0

Highlights of this release include the following changes.
For all changes, see the [Changelog](https://github.com/langflow-ai/langflow/releases).

### New features and enhancements

- Introduced MCP server functionality to serve Langflow tools to MCP-compatible clients.
- Renamed **Folders** to **Projects** in the visual editor.
- The `/folders` endpoints now redirect to `/projects`.

### Deprecations

- Deprecated the **Gmail**, **Google Drive**, and **Google Search** components.
For alternatives, see the [**Google** bundle](/bundles-google).

## Earlier releases

See the [Changelog](https://github.com/langflow-ai/langflow/releases).


================================================================================
filename: Support/troubleshooting.mdx
================================================================================

---
title: Troubleshoot Langflow
slug: /troubleshoot
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import Icon from "@site/src/components/icon";

This page provides troubleshooting advice for issues you might encounter when using Langflow or contributing to Langflow.

## Missing components

As Langflow development continues, components are often recategorized or deprecated for better alignment or to prepare for new components.

If a component appears to be missing from the <Icon name="Component" aria-hidden="true" /> **Core components** and  menus, try the following:

* <Icon name="Search" aria-hidden="true" /> **Search** for the component.
* Check other component categories and <Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components).
* Check [legacy components](/concepts-components#legacy-components), which are hidden by default.
* Check the [Changelog](https://github.com/langflow-ai/langflow/releases/latest) for component changes in recent releases.
* Make sure the component isn't already present in your flow if it is a single-use component.

If you still cannot locate the component, see [Langflow GitHub Issues and Discussions](/contributing-github-issues).

## No input in the Playground

If there is no message input field in the **Playground**, make sure your flow has a [**Chat Input** component](/components-io#chat-io) that is connected, directly or indirectly, to the **Input** port of a **Language Model** or **Agent** component.

Because the **Playground** is designed for flows that use an LLM in a query-and-response format, such as chatbots and agents, a flow must have **Chat Input**, **Language Model**/**Agent**, and **Chat Output** components to be fully supported by the **Playground** chat interface.

For more information, see [Test flows in the Playground](/concepts-playground).

## Missing key, no key found, or invalid API key

If you get an API key error when running a flow, try the following:

* For all components that require credentials, make sure those components have a valid credential in the component's settings, such as the **API Key** field.
* If you store your credentials in [Langflow global variables](/configuration-global-variables), make sure you selected the correct global variable and that the variable contains a valid credential.
* Make sure the provided credentials are active, have the required permissions, and, if applicable, have sufficient funds in the account to execute the required action. For example, model providers require credits to use their LLMs.

## Langflow installation issues

The following issues can occur when installing Langflow.

### Langflow installation freezes at pip dependency resolution

Installing Langflow OSS with `pip install langflow` slowly fails with this error message:

```text
pip is looking at multiple versions of <<library>> to determine which version is compatible with other requirements. This could take a while.
```

To work around this issue, install Langflow with `uv` instead of `pip`, as explained in [Install and run the Langflow OSS Python package](/get-started-installation#install-and-run-the-langflow-oss-python-package).

### Linux installation fails to build required package

When you try to install Langflow OSS on Linux, installation fails because of outdated or missing packages:

```bash
Resolved 455 packages in 18.92s
  Ã— Failed to build `webrtcvad==2.0.10`
  â”œâ”€â–¶ The build backend returned an error
  â•°â”€â–¶ Call to `setuptools.build_meta:__legacy__.build_wheel` failed (exit status: 1)
```

To resolve this error, install the required build dependencies, and then retry the Langflow installation:

```bash
sudo apt-get update
sudo apt-get install build-essential python3-dev
```

If upgrading your packages doesn't fix the issue, install `gcc` separately, and then retry the Langflow installation:

```bash
sudo apt-get install gcc
```

### Installation failure from `webrtcvad` package

If you experience an error from the `webrtcvad` package, run `uv pip install webrtcvad-wheels` in your virtual environment, and then retry the Langflow installation.

### C++ build tools required for Langflow Desktop on Windows

Microsoft Windows installations of Langflow Desktop require a C++ compiler that may not be present on your system. If you receive a `C++ Build Tools Required!` error, follow the on-screen prompt to install Microsoft C++ Build Tools, or [install Microsoft Visual Studio](https://visualstudio.microsoft.com/downloads/).

## Langflow startup issues

The following issues can occur when attempting to start Langflow.

### No `langflow.__main__` module

When you try to run Langflow with the command `langflow run`, you encounter the following error:

```bash
> No module named 'langflow.__main__'
```

To resolve this issue, try the following:

1. Run `uv run langflow run` instead of `langflow run`.
2. If that doesn't work, reinstall the latest Langflow version with `uv pip install langflow -U`.
3. If that doesn't work, reinstall Langflow and its dependencies with `uv pip install langflow --pre -U --force-reinstall`.

### Langflow runTraceback

When you try to run Langflow using the command `langflow run`, you encounter the following error:

```bash
> langflow runTraceback (most recent call last): File ".../langflow", line 5, in <module>  from langflow.__main__ import mainModuleNotFoundError: No module named 'langflow.__main__'
```

There are two possible reasons for this error:

* **Multiple Langflow installations**: You installed Langflow using `pip install langflow` but you already had a previous version of Langflow installed in your system. In this case, you might be running the wrong executable.

    To solve this issue, run the correct executable by running `python -m langflow run` instead of `langflow run`.

    If that doesn't work, try uninstalling and reinstalling Langflow with `uv pip install langflow --pre -U`.

* **Version conflict during installation**: Some version conflicts might have occurred during the installation process. To resolve this issue, reinstall Langflow and its dependencies by running `python -m pip install langflow --pre -U --force-reinstall`.

### Environment variables not available from terminal

Environment variables set in your terminal aren't automatically available to GUI-based applications like Langflow Desktop when launched through the Finder or the Start Menu.
To set environment variables for Langflow Desktop, see [Set environment variables for Langflow Desktop](/environment-variables#set-environment-variables-for-langflow-desktop).

### Access Langflow Desktop startup logs

If you encounter issues with Langflow Desktop, you might need to [access Langflow Desktop startup logs](/logging#desktop-logs) for debugging.

### User not found or inactive when running multiple flows

When running multiple local Langflow OSS instances on different ports, such as `localhost:7860` and `localhost:7861`, you might see authentication errors in the logs.
For example:

```text
[07/22/25 10:57:07] INFO     2025-07-22 10:57:07 - INFO     - utils - User not found or inactive.
```

To resolve this error, use separate browser instances or browser profiles to access each Langflow instance.

### Package is not installed

In Langflow OSS, you can follow the error message's instructions to install the missing dependency.

To manage dependencies in Langflow Desktop, see [Install custom dependencies in Langflow Desktop](/install-custom-dependencies#langflow-desktop).

## Langflow upgrade issues

The following issues can occur when upgrading your Langflow version.

For information about managing Langflow versions, see [Install Langflow](/get-started-installation).

### Something went wrong running migrations

The following error can occur during Langflow upgrades when the new version can't override `langflow-pre.db` in the Langflow cache folder:

```bash
> Something went wrong running migrations. Please, run 'langflow migration --fix'
```

To resolve this error, clear the cache by deleting the contents of your Langflow cache folder.
The filepath depends on your operating system, installation type, and configuration options.
For more information and default filepaths, see [Memory management options](/memory).

:::danger
Clearing the cache erases your settings.
If you want to retain your settings files, create a backup of those files before clearing the cache folder.
:::

### Langflow Desktop says it is running the latest version, but it is actually behind

If you are running Langflow Desktop version 1.4.2 or earlier, the UI might incorrectly report that you are on the latest version when a newer version is available.

This happens because the automatic update feature in the UI was introduced in version 1.4.2.
Earlier versions can't automatically detect or apply updates.

To resolve this issue, uninstall Langflow Desktop, and then [download and install the latest version of Langflow Desktop](https://langflow.org/desktop).

### 422 error when freezing components after upgrading Pydantic/FastAPI dependencies

If you locally upgrade your Pydantic and FastAPI dependencies, you may encounter a 422 error when trying to freeze components.

This error occurs due to changes in how request bodies are handled in newer versions of these dependencies.

If you're experiencing this issue, update your Langflow installation to version 1.6.5 or later, which includes a fix for this issue.

```bash
uv pip install langflow -U
```

## Langflow uninstall issues

The following issues can occur when uninstalling Langflow.

### Dot directory isn't deleted when uninstalling Langflow Desktop on macOS

On macOS, uninstalling Langflow Desktop deletes the `.app` file but doesn't delete files in `~/.langflow`, which includes files generated during usage like cache and settings.

If you reinstall Langflow Desktop, it starts with the existing data from the previous installation.

To fully remove a Langflow Desktop macOS installation, you must also delete `~/.langflow`:

    ```bash
    rm -rf .langflow
    ```

## Langflow MCP issues {#mcp}

The following issues can occur when using Langflow as an MCP server or client.

### Claude for Desktop doesn't use MCP server tools correctly

If Claude for Desktop doesn't use your server's tools correctly, try explicitly defining the path to your local `uvx` or `npx` executable file in the `claude_desktop_config.json` configuration file:

1. To find your `uvx` path, run `which uvx`.

    To find your `npx` path, run `which npx`.

2. In your `claude_desktop_config.json` file, add the path to your Langflow MCP server configuration, as shown in the following examples.

    <Tabs>
    <TabItem value="uvx" label="uvx" default>

    ```json
    {
      "mcpServers": {
        "PROJECT_NAME": {
          "command": "PATH_TO_UVX",
          "args": [
            "mcp-proxy",
            "http://LANGFLOW_SERVER_ADDRESS/api/v1/mcp/project/PROJECT_ID/sse"
          ]
        }
      }
    }
    ```

    </TabItem>
    <TabItem value="npx" label="npx">

    ```json
    {
      "mcpServers": {
        "PROJECT_NAME": {
          "command": "PATH_TO_NPX",
          "args": [
            "-y",
            "supergateway",
            "--sse",
            "http://LANGFLOW_SERVER_ADDRESS/api/v1/mcp/project/PROJECT_ID/sse"
          ]
        }
      }
    }
    ```

    </TabItem>
    </Tabs>

### MCP browser-based flows don't open a browser on Windows

This is a known issue when using MCP Tools with browser navigation actions, such as Playwright, on Windows: The agent can execute the tool successfully but the browser tab or window doesn't open.

This issue occurs because the MCP server runs from a Python process, which prevents it from opening browser windows in WSL or Windows.

To work around this issue, use the standalone MCP server approach documented in the [Playwright MCP repository](https://github.com/microsoft/playwright-mcp?tab=readme-ov-file#standalone-mcp-server).
After the server is up and running, you can add it as an SSE server in Langflow.
For other browser navigation tools, see the provider's documentation for specific troubleshooting guidance.

### "No tools or prompts connected" on MCP servers in mixed Windows/WSL environments

If you encounter "No tools or prompts connected" errors or connection failures when using Langflow Desktop as an MCP server with clients running in different environments, such as Langflow on a Windows host and an MCP client in WSL, this is due to network isolation between Windows and WSL environments.

WSL cannot directly access Windows localhost services, and Langflow running on a Windows host is not accessible from WSL clients at `localhost:7860`.

To work around this issue, run the server and host in the same operating environment.

Alternatively, configure Langflow Desktop to accept connections from WSL at the default Windows IP address of `10.255.255.254:7860` instead of `localhost`.

## Token length limit errors in Embedding Model components

Token length errors can happen if your chunking strategy doesn't align with your embedding model's tokenization limits.
For more information, see [Tokenization errors due to chunk size](/components-processing#chunk-size).

## Custom components and integrations issues

For troubleshooting advice for a third-party integration, see the information about that integration in the Langflow documentation and the provider's documentation.

If you are building a custom component, see [Error handling and logging for custom Python components](/components-custom-components#error-handling-and-logging).

### Custom components not appearing in the visual editor

If your custom components are not appearing in the Langflow visual editor, try the following troubleshooting steps:

1. Ensure your components follow the [required directory structure](https://docs.langflow.org/components-custom-components#directory-structure-requirements).
    ```
    /your/custom/components/path/       # Base directory set by LANGFLOW_COMPONENTS_PATH
        â””â”€â”€ category_name/              # Required category subfolder that determines menu name
            â”œâ”€â”€ __init__.py             # Required
            â””â”€â”€ custom_component.py     # Component file
    ```

2. Verify each category directory includes an `__init__.py` file.
This is required for Python to recognize the directory as a module.

3. Use the command line argument instead of the environment variable for `LANGFLOW_COMPONENTS_PATH`.
If you're using the `LANGFLOW_COMPONENTS_PATH` environment variable and components aren't loading, try the `--components-path` command line argument instead:

    ```bash
    uv run langflow run --components-path /path/to/your/custom/components
    ```

If you continue to experience issues, please [report them on GitHub](https://github.com/langflow-ai/langflow/issues) with details about your directory structure and component setup.

## See also

- [Langflow GitHub Issues and Discussions](/contributing-github-issues)
- [Langflow telemetry](/contributing-telemetry)


================================================================================
filename: Tutorials/agent.mdx
================================================================================

---
title: Connect applications to agents
slug: /agent-tutorial
---

import Icon from "@site/src/components/icon";

This tutorial shows you how to connect a JavaScript application to a [Langflow agent](/agents).

With an agent, your application can use any connected tools to retrieve more contextual and timely data without changing any application code. The tools are selected by the agent's internal LLM to solve problems and answer questions.

## Prerequisites

* [Install and start Langflow](/get-started-installation)
* Create a [Langflow API key](/api-keys-and-authentication)
* Install the [Langflow JavaScript client](/typescript-client)
* Create an [OpenAI API key](https://platform.openai.com/api-keys)

This tutorial uses an OpenAI LLM. If you want to use a different provider, you need a valid credential for that provider.

## Create an agent flow

The following steps modify the **Simple Agent** template to connect a [**Directory** component](/components-data#directory) and a [**Web Search** component](/components-data#web-search) as tools for an **Agent** component.
The **Directory** component loads all files of a given type from a target directory on your local machine, and the **Web Search** component performs a DuckDuckGo search.
When connected to an **Agent** component as tools, the agent has the option to use these components when handling requests.

1. In Langflow, click **New Flow**, and then select the **Simple Agent** template.
2. Remove the **URL** and **Calculator** tools, and then add **Directory** and **Web Search** components to your flow.
3. In the **Directory** component's **Path** field, enter the directory path and file types that you want to make available to the **Agent** component.

    In this tutorial, the agent needs access to a record of customer purchases, so the directory name is `customer_orders` and the file type is `.csv`. Later in this tutorial, the agent will be prompted to find `email` values in the customer data.

    You can adapt the tutorial to suit your data, or, to follow along with the tutorial, you can download [`customer-orders.csv`](/files/customer_orders.csv) and save it in a `customer_orders` folder on your local machine.

4. In the **Directory** and **Web Search** [components' header menus](/concepts-components#component-menus), enable **Tool Mode** so you can use the components with an agent.

5. Connect the **Directory** and **Web Search** components' **Toolset** ports to the **Agent** component's **Tools** port.
6. In the **Agent** component, enter your OpenAI API key.

    If you want to use a different provider or model, edit the **Model Provider**, **Model Name**, and **API Key** fields accordingly.

7. To test the flow, click <Icon name="Play" aria-hidden="true" /> **Playground**, and then ask the LLM a question, such as `Recommend 3 used items for carol.davis@example.com, based on previous orders.`

    Given the example prompt, the LLM would respond with recommendations and web links for items based on previous orders in `customer_orders.csv`.

    The **Playground** prints the agent's chain of thought as it selects tools to use and interacts with functionality provided by those tools.
    For example, the agent can use the **Directory** component's `as_dataframe` tool to retrieve a [DataFrame](/data-types#dataframe), and the **Web Search** component's `perform_search` tool to find links to related items.

## Add a Prompt Template component to the flow

In this example, the application sends a customer's email address to the Langflow agent. The agent compares the customer's previous orders within the **Directory** component, searches the web for used versions of those items, and returns three results.

1. To include the email address as a value in your flow, add a [**Prompt Template** component](/components-prompts) to your flow between the **Chat Input** and **Agent** components.
2. In the **Prompt Template** component's **Template** field, enter `Recommend 3 used items for {email}, based on previous orders.`
Adding the `{email}` value in curly braces creates a new input in the **Prompt Template** component, and the component connected to the `{email}` port is supplying the value for that variable.
This creates a point for the user's email to enter the flow from your request.
If you aren't using the `customer_orders.csv` example file, modify the input to search for a value in your dataset.

    At this point your flow has six components. The **Chat Input** component is connected to the **Prompt Template** component's **email** input port. Then, the **Prompt Template** component's output is connected to the **Agent** component's **System Message** input port. The **Directory** and **Web Search** components are connected to the **Agent** component's **Tools** port. Finally, the **Agent** component's output is connected to the **Chat Output** component, which returns the final response to the application.

    ![An Agent component connected to Web Search and Directory components as tools](/img/tutorial-agent-with-directory.png)

## Send requests to your flow from a JavaScript application

With your flow operational, connect it to a JavaScript application to use the agent's responses.

1. To construct a JavaScript application to connect to your flow, gather the following information:

    * `LANGFLOW_SERVER_ADDRESS`: Your Langflow server's domain. The default value is `127.0.0.1:7860`. You can get this value from the code snippets on your flow's [**API access** pane](/concepts-publish#api-access).
    * `FLOW_ID`: Your flow's UUID or custom endpoint name. You can get this value from the code snippets on your flow's [**API access** pane](/concepts-publish#api-access).
    * `LANGFLOW_API_KEY`: A valid [Langflow API key](/api-keys-and-authentication).

2. Copy the following script into a JavaScript file, and then replace the placeholders with the information you gathered in the previous step.
If you're using the `customer_orders.csv` example file, you can run this example as-is with the example email address in the code sample.
If not, modify the `const email = "isabella.rodriguez@example.com"` to search for a value in your dataset.

    ```js
    import { LangflowClient } from "@datastax/langflow-client";

    const LANGFLOW_SERVER_ADDRESS = 'LANGFLOW_SERVER_ADDRESS';
    const FLOW_ID = 'FLOW_ID';
    const LANGFLOW_API_KEY = 'LANGFLOW_API_KEY';
    const email = "isabella.rodriguez@example.com";

    async function runAgentFlow(): Promise<void> {
        try {
            // Initialize the Langflow client
            const client = new LangflowClient({
                baseUrl: LANGFLOW_SERVER_ADDRESS,
                apiKey: LANGFLOW_API_KEY
            });

            console.log(`Connecting to Langflow server at: ${LANGFLOW_SERVER_ADDRESS} `);
            console.log(`Flow ID: ${FLOW_ID}`);
            console.log(`Email: ${email}`);

            // Get the flow instance
            const flow = client.flow(FLOW_ID);

            // Run the flow with the email as input
            console.log('\nSending request to agent...');
            const response = await flow.run(email, {
                session_id: email // Use email as session ID for context
            });

            console.log('\n=== Response from Langflow ===');
            console.log('Session ID:', response.sessionId);

            // Extract URLs from the chat message
            const chatMessage = response.chatOutputText();
            console.log('\n=== URLs from Chat Message ===');
            const messageUrls = chatMessage.match(/https?:\/\/[^\s"')\]]+/g) || [];
            const cleanMessageUrls = [...new Set(messageUrls)].map(url => url.trim());
            console.log('URLs from message:');
            cleanMessageUrls.slice(0, 3).forEach(url => console.log(url));

        } catch (error) {
            console.error('Error running flow:', error);

            // Provide error messages
            if (error instanceof Error) {
                if (error.message.includes('fetch')) {
                    console.error('\nMake sure your Langflow server is running and accessible at:', LANGFLOW_SERVER_ADDRESS);
                }
                if (error.message.includes('401') || error.message.includes('403')) {
                    console.error('\nCheck your API key configuration');
                }
                if (error.message.includes('404')) {
                    console.error('\nCheck your Flow ID - make sure it exists and is correct');
                }
            }
        }
    }

    // Run the function
    console.log('Starting Langflow Agent...\n');
    runAgentFlow().catch(console.error);
    ```

3.  Save and run the script to send the request and test the flow.

    Your application receives three URLs for recommended used items based on a customer's previous orders in your local CSV, all without changing any code.

    <details>
    <summary>Result</summary>

    The following is an example response from this tutorial's flow. Due to the nature of LLMs and variations in your inputs, your response might be different.

    ```
    Starting Langflow Agent...

    Connecting to Langflow server at: http://localhost:7860
    Flow ID: local-db-search
    Email: isabella.rodriguez@example.com

    Sending request to agent...

    === Response from Langflow ===
    Session ID: isabella.rodriguez@example.com

    URLs found:
    https://www.facebook.com/marketplace/258164108225783/electronics/
    https://www.facebook.com/marketplace/458332108944152/furniture/
    https://www.facebook.com/marketplace/613732137493719/kitchen-cabinets/
    ```

    </details>

4.  To quickly check traffic to your flow, open the **Playground**.
    New sessions are named after the user's email address.
    Keeping sessions distinct helps the agent maintain context. For more on session IDs, see [Session ID](/session-id).

## Next steps

For more information on building or extending this tutorial, see the following:

* [Model Context Protocol (MCP) servers](/mcp-server)
* [Langflow deployment overview](/deployment-overview)


================================================================================
filename: Tutorials/chat-with-files.mdx
================================================================================

---
title: Create a chatbot that can ingest files
slug: /chat-with-files
---

import Icon from "@site/src/components/icon";

This tutorial shows you how to build a chatbot that can read and answer questions about files you upload, such as meeting notes or job applications.

For example, you could upload a contract and ask, â€œWhat are the termination clauses in this agreement?â€ Or upload a resume and ask, â€œDoes this candidate have experience with marketing analytics?â€

The main focus of this tutorial is to show you how to provide files as input to a Langflow flow, so your chatbot can use the content of those files in its responses.

## Prerequisites

* [Install and start Langflow](/get-started-installation)
* Create a [Langflow API key](/api-keys-and-authentication)
* Create an [OpenAI API key](https://platform.openai.com/api-keys)

This tutorial uses an OpenAI LLM. If you want to use a different provider, you need a valid credential for that provider.

## Create a flow that accepts file input

To ingest files, your flow must have a **File** component attached to a component that receives input, such as a **Prompt Template** or **Agent** component.

The following steps modify the **Basic Prompting** template to accept file input:

1. In Langflow, click **New Flow**, and then select the **Basic Prompting** template.
2. In the **Language Model** component, enter your OpenAI API key.

    If you want to use a different provider or model, edit the **Model Provider**, **Model Name**, and **API Key** fields accordingly.

3. To verify that your API key is valid, click <Icon name="Play" aria-hidden="true" /> **Playground**, and then ask the LLM a question.
The LLM should respond according to the specifications in the **Prompt Template** component's **Template** field.

4. Exit the **Playground**, and then modify the **Prompt Template** component to accept file input in addition to chat input.
To do this, edit the **Template** field, and then replace the default prompt with the following text:

    ```text
    ChatInput:
    {chat-input}
    File:
    {file}
    ```

    :::tip
    You can use any string to name your template variables.
    These strings become the names of the fields (input ports) on the **Prompt Template** component.

    For this tutorial, the variables are named after the components that connect to them: **chat-input** for the **Chat Input** component and **file** for the **File** component.
    :::

5. Add a [**File** component](/components-data#file) to the flow, and then connect the **Raw Content** output port to the **Prompt Template** component's **file** input port.
To connect ports, click and drag from one port to the other.

    You can add files directly to the **File** component to pre-load input before running the flow, or you can load files at runtime. The next section of this tutorial covers runtime file uploads.

    At this point your flow has five components. The **Chat Input** and **File** components are connected to the **Prompt Template** component's input ports. Then, the **Prompt Template** component's output port is connected to the **Language Model** component's input port. Finally, the **Language Model** component's output port is connected to the **Chat Output** component, which returns the final response to the user.

    ![File loader chat flow](/img/tutorial-chat-file-loader.png)

## Send requests to your flow from a Python application

This section of the tutorial demonstrates how you can send file input to a flow from an application.

To do this, your application must send a `POST /run` request to your Langflow server with the file you want to upload and a text prompt.
The result includes the outcome of the flow run and the LLM's response.

This example uses a local Langflow instance, and it asks the LLM to evaluate a sample resume.
If you don't have a resume on hand, you can download [fake-resume.txt](/files/fake-resume.txt).

:::tip
For help with constructing file upload requests in Python, JavaScript, and curl, see the [Langflow File Upload Utility](https://langflow-file-upload-examples.onrender.com).
:::

1. To construct the request, gather the following information:

    * `LANGFLOW_SERVER_ADDRESS`: Your Langflow server's domain. The default value is `127.0.0.1:7860`. You can get this value from the code snippets on your flow's [**API access** pane](/concepts-publish#api-access).
    * `FLOW_ID`: Your flow's UUID or custom endpoint name. You can get this value from the code snippets on your flow's [**API access** pane](/concepts-publish#api-access).
    * `FILE_COMPONENT_ID`: The UUID of the **File** component in your flow, such as `File-KZP68`. To find the component ID, open your flow in Langflow, click the **File** component, and then click **Controls**. The component ID is at the top of the **Controls** pane.
    * `CHAT_INPUT`: The message you want to send to the Chat Input of your flow, such as `Evaluate this resume for a job opening in my Marketing department.`
    * `FILE_NAME` and `FILE_PATH`: The name and path to the local file that you want to send to your flow.
    * `LANGFLOW_API_KEY`: A valid [Langflow API key](/api-keys-and-authentication).

2. Copy the following script into a Python file, and then replace the placeholders with the information you gathered in the previous step:

    ```python
    # Python example using requests
    import requests
    import json

    # 1. Set the upload URL
    url = "http://LANGFLOW_SERVER_ADDRESS/api/v2/files/"

    # 2. Prepare the file and payload
    payload = {}
    files = [
      ('file', ('FILE_PATH', open('FILE_NAME', 'rb'), 'application/octet-stream'))
    ]
    headers = {
      'Accept': 'application/json',
      'x-api-key': 'LANGFLOW_API_KEY'
    }

    # 3. Upload the file to Langflow
    response = requests.request("POST", url, headers=headers, data=payload, files=files)
    print(response.text)

    # 4. Get the uploaded file path from the response
    uploaded_data = response.json()
    uploaded_path = uploaded_data.get('path')

    # 5. Call the Langflow run endpoint with the uploaded file path
    run_url = "http://LANGFLOW_SERVER_ADDRESS/api/v1/run/FLOW_ID"
    run_payload = {
        "input_value": "CHAT_INPUT",
        "output_type": "chat",
        "input_type": "chat",
        "tweaks": {
            "FILE_COMPONENT_ID": {
                "path": uploaded_path
            }
        }
    }
    run_headers = {
        'Content-Type': 'application/json',
        'Accept': 'application/json',
        'x-api-key': 'LANGFLOW_API_KEY'
    }
    run_response = requests.post(run_url, headers=run_headers, data=json.dumps(run_payload))
    langflow_data = run_response.json()
    # Output only the message
    message = None
    try:
        message = langflow_data['outputs'][0]['outputs'][0]['results']['message']['data']['text']
    except (KeyError, IndexError, TypeError):
        pass
    print(message)

    ```

    This script contains two requests.

    The first request uploads a file, such as `fake-resume.txt`, to your Langflow server at the `/v2/files` endpoint. This request returns a file path that can be referenced in subsequent Langflow requests, such as `02791d46-812f-4988-ab1c-7c430214f8d5/fake-resume.txt`

    The second request sends a chat message to the Langflow flow at the `/v1/run/` endpoint.
    The `tweaks` parameter includes the path to the uploaded file as the variable `uploaded_path`, and sends this file directly to the **File** component.

3. Save and run the script to send the requests and test the flow.

    The initial output contains the JSON response object from the file upload endpoint, including the internal path where Langflow stores the file.
    Then, the LLM retrieves the file and evaluates its content, in this case the suitability of the resume for a job position.

    <details>
    <summary>Result</summary>

    The following is an example response from this tutorial's flow. Due to the nature of LLMs and variations in your inputs, your response might be different.

    ```
    {"id":"793ba3d8-5e7a-4499-8b89-d9a7b6325fee","name":"fake-resume (1)","path":"02791d46-812f-4988-ab1c-7c430214f8d5/fake-resume.txt","size":1779,"provider":null}
    The resume for Emily J. Wilson presents a strong candidate for a position in your Marketing department. Here are some key points to consider:

    ### Strengths:
    1. **Experience**: With over 8 years in marketing, Emily has held progressively responsible positions, culminating in her current role as Marketing Director. This indicates a solid foundation in the field.

    2. **Quantifiable Achievements**: The resume highlights specific accomplishments, such as a 25% increase in brand recognition and a 30% sales increase after launching new product lines. These metrics demonstrate her ability to drive results.

    3. **Diverse Skill Set**: Emily's skills encompass various aspects of marketing, including strategy development, team management, social media marketing, event planning, and data analysis. This versatility can be beneficial in a dynamic marketing environment.

    4. **Educational Background**: Her MBA and a Bachelor's degree in Marketing provide a strong academic foundation, which is often valued in marketing roles.

    5. **Certifications**: The Certified Marketing Professional (CMP) and Google Analytics Certification indicate a commitment to professional development and staying current with industry standards.

    ### Areas for Improvement:
    1. **Specificity in Skills**: While the skills listed are relevant, providing examples of how she has applied these skills in her previous roles could strengthen her resume further.

    2. **References**: While stating that references are available upon request is standard, including a couple of testimonials or notable endorsements could enhance credibility.

    3. **Formatting**: Ensure that the resume is visually appealing and easy to read. Clear headings and bullet points help in quickly identifying key information.

    ### Conclusion:
    Overall, Emily J. Wilson's resume reflects a well-rounded marketing professional with a proven track record of success. If her experience aligns with the specific needs of your Marketing department, she could be a valuable addition to your team. Consider inviting her for an interview to further assess her fit for the role.
    ```

    </details>

## Next steps

To continue building on this tutorial, try these next steps.

### Process multiple files loaded at runtime

To process multiple files in a single flow run, add a separate **File** component for each file you want to ingest. Then, modify your script to upload each file, retrieve each returned file path, and then pass a unique file path to each **File** component ID.

For example, you can modify `tweaks` to accept multiple **File** components.
The following code is just an example; it isn't working code:

```python
## set multiple file paths
file_paths = {
    FILE_COMPONENT_1: uploaded_path_1,
    FILE_COMPONENT_2: uploaded_path_2
}

def chat_with_flow(input_message, file_paths):
    """Compare the contents of these two files."""
    run_url = f"{LANGFLOW_SERVER_ADDRESS}/api/v1/run/{FLOW_ID}"
    # Prepare tweaks with both file paths
    tweaks = {}
    for component_id, file_path in file_paths.items():
        tweaks[component_id] = {"path": file_path}
```

You can also use a [**Directory** component](/components-data#directory) to load all files in a directory or pass an archive file to the **File** component.

### Upload external files at runtime

To upload files from another machine that isn't your local environment, your Langflow server must first be accessible over the internet. Then, authenticated users can upload files to your public Langflow server's `/v2/files/` endpoint, as shown in the tutorial. For more information, see [Langflow deployment overview](/deployment-overview).

### Preload files outside the chat session

You can use the **File** component to load files anywhere in a flow, not just in a chat session.

In the visual editor, you can preload files to the **File** component by selecting them from your local machine or [Langflow file management](/concepts-file-management).

For example, you can preload an instructions file for a prompt template, or you can preload a vector store with documents that you want to query in a Retrieval Augmented Generation (RAG) flow.

For more information about the **File** component and other data loading components, see [Data components](/components-data).


================================================================================
filename: Tutorials/chat-with-rag.mdx
================================================================================

---
title: Create a vector RAG chatbot
slug: /chat-with-rag
---

import Icon from "@site/src/components/icon";
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

This tutorial demonstrates how you can use Langflow to create a chatbot application that uses Retrieval Augmented Generation (RAG) to embed your data as vectors in a vector database, and then chat with the data.

## Prerequisites

* [Install and start Langflow](/get-started-installation)
* Create a [Langflow API key](/api-keys-and-authentication)
* Create an [OpenAI API key](https://platform.openai.com/api-keys)
* Install the [Langflow JavaScript client](/typescript-client)
* Be familiar with vector search concepts and applications, such as vector databases and RAG

## Create a vector RAG flow

1. In Langflow, click **New Flow**, and then select the **Vector Store RAG** template.

    <details>
    <summary>About the Vector Store RAG template</summary>

    This template has two flows.

    The **Load Data Flow** populates a vector store with data from a file.
    This data is used to respond to queries submitted to the **Retriever Flow**.

    Specifically, the **Load Data Flow** ingests data from a local file, splits the data into chunks, loads and indexes the data in your vector database, and then computes embeddings for the chunks. The embeddings are also stored with the loaded data. This flow only needs to run when you need to load data into your vector database.

    The **Retriever Flow** receives chat input, generates an embedding for the input, and then uses several components to reconstruct chunks into text and generate a response by comparing the new embedding to the stored embeddings to find similar data.

    </details>

2. Add your **OpenAI** API key to both **OpenAI Embeddings** components.

3. Optional: Replace both **Astra DB** vector store components with a **Chroma DB** or another vector store component of your choice.
This tutorial uses Chroma DB.

    The **Load Data Flow** should have **File**, **Split Text**, **Embedding Model**, vector store (such as **Chroma DB**), and **Chat Output** components:

    ![File loader chat flow](/img/tutorial-chatbot-embed-files.png)

    The **Retriever Flow** should have **Chat Input**, **Embedding Model**, vector store, **Parser**, **Prompt**, **Language Model**, and **Chat Output** components:

    ![Chat with RAG flow](/img/tutorial-chatbot-chat-flow.png)

    The flows are ready to use.
    Continue the tutorial to learn how to use the loading flow to load data into your vector store, and then call the chat flow in a chatbot application.

## Load data and generate embeddings

To load data and generate embeddings, you can use the visual editor or the `/v2/files` endpoint.

The visual editor option is simpler, but it is only recommended for scenarios where the user who created the flow is the same user who loads data into the database.

In situations where many users load data or you need to load data programmatically, use the Langflow API option.

<Tabs>
<TabItem value="visual" label="Visual editor" default>

1. In your RAG chatbot flow, click the **File** component, and then click **File**.
2. Select the local file you want to upload, and then click **Open**.
    The file is loaded to your Langflow server.
3. To load the data into your vector database, click the vector store component, and then click <Icon name="Play" aria-hidden="true" /> **Run component** to run the selected component and all prior dependent components.

</TabItem>
<TabItem value="API" label="Langflow API">

To load data programmatically, use the `/v2/files/` and `/v1/run/$FLOW_ID` endpoints. The first endpoint loads a file to your Langflow server, and then returns an uploaded file path. The second endpoint runs the **Load Data Flow**, referencing the uploaded file path, to chunk, embed, and load the data into the vector store.

The following script demonstrates this process.
For help with creating this script, use the [Langflow File Upload Utility](https://langflow-file-upload-examples.onrender.com/).

```js
// Node 18+ example using global fetch, FormData, and Blob
import fs from 'fs/promises';

// 1. Prepare the form data with the file to upload
const fileBuffer = await fs.readFile('FILE_NAME');
const data = new FormData();
data.append('file', new Blob([fileBuffer]), 'FILE_NAME');
const headers = { 'x-api-key': 'LANGFLOW_API_KEY' };

// 2. Upload the file to Langflow
const uploadRes = await fetch('LANGFLOW_SERVER_ADDRESS/api/v2/files/', {
  method: 'POST',
  headers,
  body: data
});
const uploadData = await uploadRes.json();
const uploadedPath = uploadData.path;

// 3. Call the Langflow run endpoint with the uploaded file path
const payload = {
  input_value: "Analyze this file",
  output_type: "chat",
  input_type: "text",
  tweaks: {
    'FILE_COMPONENT_NAME': {
      path: uploadedPath
    }
  }
};
const runRes = await fetch('LANGFLOW_SERVER_ADDRESS/api/v1/run/FLOW_ID', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json', 'x-api-key': 'LANGFLOW_API_KEY' },
  body: JSON.stringify(payload)
});
const langflowData = await runRes.json();
// Output only the message
console.log(langflowData.outputs?.[0]?.outputs?.[0]?.results?.message?.data?.text);
```

</TabItem>
</Tabs>

When the flow runs, the flow ingests the selected file, chunks the data, loads the data into the vector store database, and then generates embeddings for the chunks, which are also stored in the vector store.

Your database now contains data with vector embeddings that an LLM can use as context to respond to queries, as demonstrated in the next section of the tutorial.

## Chat with your flow from a JavaScript application

To chat with the data in your vector database, create a chatbot application that runs the **Retriever Flow** programmatically.

This tutorial uses JavaScript for demonstration purposes.

1. To construct the chatbot, gather the following information:

    * `LANGFLOW_SERVER_ADDRESS`: Your Langflow server's domain. The default value is `127.0.0.1:7860`. You can get this value from the code snippets on your flow's [**API access** pane](/concepts-publish#api-access).
    * `FLOW_ID`: Your flow's UUID or custom endpoint name. You can get this value from the code snippets on your flow's [**API access** pane](/concepts-publish#api-access).
    * `LANGFLOW_API_KEY`: A valid [Langflow API key](/api-keys-and-authentication).

2. Copy the following script into a JavaScript file, and then replace the placeholders with the information you gathered in the previous step:

    ```js
    const readline = require('readline');
    const { LangflowClient } = require('@datastax/langflow-client');

    const API_KEY = 'LANGFLOW_API_KEY';
    const SERVER = 'LANGFLOW_SERVER_ADDRESS';
    const FLOW_ID = 'FLOW_ID';

    const rl = readline.createInterface({ input: process.stdin, output: process.stdout });

    // Initialize the Langflow client
    const client = new LangflowClient({
        baseUrl: SERVER,
        apiKey: API_KEY
    });

    async function sendMessage(message) {
        try {
            const response = await client.flow(FLOW_ID).run(message, {
                session_id: 'user_1'
            });

            // Use the convenience method to get the chat output text
            return response.chatOutputText() || 'No response';
        } catch (error) {
            return `Error: ${error.message}`;
        }
    }

    function chat() {
        console.log('ðŸ¤– Langflow RAG Chatbot (type "quit" to exit)\n');

        const ask = () => {
            rl.question('ðŸ‘¤ You: ', async (input) => {
                if (['quit', 'exit', 'bye'].includes(input.trim().toLowerCase())) {
                    console.log('ðŸ‘‹ Goodbye!');
                    rl.close();
                    return;
                }

                const response = await sendMessage(input.trim());
                console.log(`ðŸ¤– Assistant: ${response}\n`);
                ask();
            });
        };

        ask();
    }

    chat();
    ```

    The script creates a Node application that chats with the content in your vector database, using the `chat` input and output types to communicate with your flow.
    Chat maintains ongoing conversation context across multiple messages. If you used `text` type inputs and outputs, each request is a standalone text string.

    :::tip
    The [Langflow TypeScript client](/typescript-client) has a `chatOutputText()` convenience method that simplifies working with Langflow's complex JSON response structure.
    Instead of manually navigating through multiple levels of nested objects with `data.outputs[0].outputs[0].results.message.data.text`, the client automatically extracts the message text and handles potentially undefined values gracefully.
    :::

3. Save and run the script to send the requests and test the flow.

    <details>
    <summary>Result</summary>

    The following is an example response from this tutorial's flow. Due to the nature of LLMs and variations in your inputs, your response might be different.

    ```
    ðŸ‘¤ You: Do you have any documents about engines?
    ðŸ¤– Assistant: Yes, the provided text contains several warnings and guidelines related to engine installation, maintenance, and selection. It emphasizes the importance of using the correct engine for specific applications, ensuring all components are in good condition, and following safety precautions to prevent fire or explosion. If you need more specific information or details, please let me know!

    ðŸ‘¤ You: It should be about a Briggs and Stratton engine.
    ðŸ¤– Assistant: The text provides important safety and installation guidelines for Briggs & Stratton engines. It emphasizes that these engines should not be used on 3-wheel All-Terrain Vehicles (ATVs), motor bikes, aircraft products, or vehicles intended for competitive events, as such uses are not approved by Briggs & Stratton.

    If you have any specific questions about Briggs & Stratton engines or need further information, feel free to ask!
    ```

    </details>

## Next steps

For more information on building or extending this tutorial, see the following:

* [Model Context Protocol (MCP) servers](/mcp-server)
* [Langflow deployment overview](/deployment-overview)


================================================================================
filename: Tutorials/mcp-tutorial.mdx
================================================================================

---
title: Connect to MCP servers from your application
slug: /mcp-tutorial
---

import Icon from "@site/src/components/icon";
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import McpIcon from '@site/static/logos/mcp-icon.svg';

This tutorial shows you how to connect MCP servers to your applications using Langflow's [**MCP Tools** component](/mcp-client).

The [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) helps agents integrate with LLMs through _MCP clients_ and _MCP servers_.
Specifically, MCP servers host tools that agents (MCP clients) use to complete specialized tasks.
MCP servers are connected to MCP clients like Cursor.
Then, you interact with the client, and the client uses tools from the connected servers as needed to complete your requests.

You can run Langflow as an MCP client and an MCP server:

* [Use Langflow as an MCP client](/mcp-client): When run as an MCP client, an **Agent** component in a Langflow flow can use connected components as tools to handle requests.
You can use existing components as tools, and you can connect any MCP server to your flow to make that server's tools available to the agent.

* [Use Langflow as an MCP server](/mcp-server): When run as an MCP server, your flows become tools that can be used by an MCP client, which could be an external client or another Langflow flow.

In this tutorial, you will use the Langflow **MCP Tools** component to connect multiple MCP servers to your flow, and then you'll use a Python application to run your flow and chat with the agent programmatically.

## Prerequisites

* [Install and start Langflow](/get-started-installation)
* Create a [Langflow API key](/api-keys-and-authentication)
* Create an [OpenAI API key](https://platform.openai.com/api-keys)

This tutorial uses an OpenAI LLM. If you want to use a different provider, you need a valid credential for that provider.

## Create an agent flow

1. In Langflow, click **New Flow**, and then select the **Simple Agent** template.

2. In the **Agent** component, enter your OpenAI API key.

    If you want to use a different provider or model, edit the **Model Provider**, **Model Name**, and **API Key** fields accordingly.

3. To test the flow, click <Icon name="Play" aria-hidden="true" /> **Playground**, and then ask the LLM `Is it safe to go hiking in the Adirondacks today?`

    This query demonstrates how an LLM, by itself, might not have access to information or functions designed to address specialized queries. In this example, the default OpenAI model provides a vague response, although the agent does know the current date by using its internal `get_current_date` function.

    ```text
    Today is July 11, 2025.
    To determine if it's safe to go hiking in the Adirondacks today, you should check the current weather conditions, trail advisories, and any local alerts (such as bear activity or flooding).
    Would you like a detailed weather forecast or information on trail conditions for the Adirondacks today?
    ```

    To improve the response, you can connect MCP servers to your flow that provide specialized tools for the agent to use when generating responses. In the next part of this tutorial, you'll connect an MCP server that provides the agent with real-time weather information so that it can generate a more specific response.

## Add an MCP Tools component

There are many MCP servers available online that offer different tools for different tasks.
To use an MCP server with an MCP client, you have to make the server available to the client.
With all MCP clients, there are several ways to do this:

* Install the server locally.
* Use `uvx` or `npx` to fetch and run a server package.
* Call a server running remotely, like those available on Smithery.

This tutorial demonstrates how to install a weather server locally with `uv pip install`, and how to use `npx` to run the geolocation server package.
Your particular MCP server's requirements may vary.

In Langflow, you use the **MCP Tools** component to connect a specific MCP server to a flow.
You need one **MCP Tools** component for each MCP server that you want your flow to use.

1. For this tutorial, install a [weather MCP server](https://github.com/isdaniel/mcp_weather_server) on your local machine with uv and Python:

    ```shell
    uv pip install mcp_weather_server
    ```

    Make sure you install the server in the same Python environment where Langflow is running:

    * Langflow in a virtual environment: Activate the environment before installing the server.
    * Langflow Docker image: Install the server inside the Docker container.
    * Langflow Desktop or system-wide Langflow OSS: Install the server globally or in the same user environment where you run Langflow.

2. In your **Simple Agent** flow, remove the **URL** and **Calculator** tools, and then add an [**MCP Tools**](/mcp-client) component.

3. Click the **MCP Tools** component, and then click <Icon name="Plus" aria-hidden="true"/> **Add MCP Server**.

    :::tip
    To manage all of your MCP server connections, click <McpIcon /> **MCP servers** in the visual editor, or click your profile icon, select **Settings**, and then click **MCP Servers**.
    :::

4. In the **Add MCP Server** pane, provide the server startup command and arguments to connect the weather MCP server to your flow. For this tutorial, use either the **JSON** or **STDIO** option.

    Langflow runs the command to launch the server when the agent determines that it needs to use a tool provided by that server.

    Notice that both configurations provide the same information but in different formats.
    This means that if your MCP server repository only provides a JSON file for the server, you can still use those values with the STDIO option.

    <Tabs>
    <TabItem value="JSON" label="JSON" default>

    To provide the MCP server configuration as a JSON object, select **JSON**, and then paste the server configuration into the **JSON** field:

    ```json
    {
      "mcpServers": {
        "weather": {
          "command": "python",
          "args": [
            "-m",
            "mcp_weather_server"
          ],
          "disabled": false,
          "autoApprove": []
        }
      }
    }
    ```

    </TabItem>
    <TabItem value="STDIO" label="STDIO">

    To provide the MCP server configuration in a GUI format, select **STDIO**, and then enter the MCP server configuration values into the given fields:

    - **Name**: `weather`
    - **Command**: `python`
    - **Arguments**:
      - `-m`
      - `mcp_weather_server`

    </TabItem>
    </Tabs>

5. Click **Add Server**, and then wait for the **Actions** list to populate. This means that the MCP server successfully connected.

    With this weather server, the **MCP Tools** component also adds an optional **City** field.
    For this tutorial, don't enter anything in this field.
    Instead, you will add a geolocation MCP server in the next step, which the agent will use to detect your location.

6. Click the **MCP Tools** component, enable **Tool Mode** in the [component's header menu](/concepts-components#component-menus), and then connect the component's **Toolset** port to the **Agent** component's **Tools** port.

    At this point your flow has four connected components:

    * The **Chat Input** component is connected to the **Agent** component's **Input** port. This allows to flow to be triggered by an incoming prompt from a user or application.
    * The **MCP Tools** component with the weather MCP server is connected to the **Agent** component's **Tools** port. The agent may not use this server for every request; the agent only uses this connection if it decides the server can help respond to the prompt.
    * The **Agent** component's **Output** port is connected to the **Chat Output** component, which returns the final response to the user or application.

    ![An Agent component connected to an MCP weather server](/img/tutorial-mcp-weather.png)

7. To test the weather MCP server, click <Icon name="Play" aria-hidden="true" /> **Playground**, and then ask the LLM `Is it safe to go hiking in the Adirondacks today?`

    The  **Playground** shows you the agent's logic as it analyzes the request and select tools to use.

    Ideally, the agent's response will be more specific than the previous response because of the additional context provided by the weather MCP server.
    For example:

    ```text
    The current weather in Lake Placid, a central location in the Adirondacks,
    is foggy with a temperature of 17.2Â°C (about 63Â°F).
    If you plan to go hiking today, be cautious as fog can reduce visibility
    on trails and make navigation more difficult.
    ```

    This is a better response, but what makes this MCP server more valuable than just calling a weather API?

    First, MCP servers are often customized for specific tasks, such as highly specialized actions or chained tools for complex, multi-step problem solving.
    Typically, you would have to write a custom script for a specific task, possibly including multiple API calls in a single script, and then you would have to either execute this script outside the context of the agent or provide it to your agent in some way.

    Instead, the MCP ensures that all MCP servers are added to agents in the same way, without having to know each server's specific endpoint structures or write custom integrations.
    The MCP is a standardized way to integrate many diverse tools into agentic applications.
    You don't have to learn a new API or write custom code every time you want to use a new MCP server.

    Additionally, you can attach many MCP servers to one agent, depending on the problems you want your application to solve.
    The more servers you add, the more specialized context the agent can use in its responses.
    In this tutorial, adding the weather MCP server already improved the quality of the LLM's response.
    In the next section of the tutorial, you will add a `ip_geolocation` MCP server so the agent can detect the user's location if they don't specify a location in their prompt.

## Add a geolocation server

The [Toolkit MCP server](https://github.com/cyanheads/toolkit-mcp-server) includes multiple MCP tools for network monitoring, including IP geolocation. It isn't extremely precise, but it doesn't require an API key.

Note that this tool returns the IP geolocation of your Langflow server, so if your server is deployed remotely, consider alternative approaches for getting user-specific location data, such as browser geolocation APIs.

This MCP server can be started with one [npx](https://docs.npmjs.com/cli/v8/commands/npx) command, which downloads and runs the [Toolkit MCP server Node registry package](https://www.npmjs.com/package/@cyanheads/toolkit-mcp-server) without installing the package locally.

To add the Toolkip MCP server to your flow, do the following:

1. Add another **MCP Tools** component to your flow, click the component, and then click <Icon name="Plus" aria-hidden="true"/> **Add MCP Server**.

2. Select **STDIO**.

3. For **Name**, enter `ip_geolocation`.

    :::tip
    The tool name and description help the agent select tools.
    If your agent is struggling to select tools, make sure the names and descriptions are clear and human-readable.
    :::

4. For **Command**, enter `npx @cyanheads/toolkit-mcp-server`.

5. Click **Add Server**, and then wait for the **Actions** list to populate. This means that the MCP server successfully connected.

6. Click the **MCP Tools** component, enable **Tool Mode** in the [component's header menu](/concepts-components#component-menus), and then connect the component's **Toolset** port to the **Agent** component's **Tools** port.

    Your flow now has an additional **MCP Tools** component for a total of five components.

    ![An Agent component connected to MCP weather and geolocation servers](/img/tutorial-mcp-geolocation.png)

## Create a Python application that connects to Langflow

At this point, you can open the **Playground** and ask about the weather in your current location to test the IP geolocation tool.
However, geolocation tools are most useful in applications where you or your users want to ask about the weather from different places around the world.

In the last part of this tutorial, you'll learn how to use the Langflow API to run a flow in a script.
This could be part of a larger application, such as a mobile app where users want to know if the weather is good for a particular sport.

When you use the Langflow API to run a flow, you can change some aspects of the flow without changing the code.
For example, you can add more MCP servers to your flow in Langflow, and then use the same script to run the flow.
You can use the same input or a new input that prompts the agent to use other tools.

1. For this tutorial's Python script, gather the following information:

    * `LANGFLOW_SERVER_ADDRESS`: Your Langflow server's domain. The default value is `127.0.0.1:7860`. You can get this value from the code snippets on your flow's [**API access** pane](/concepts-publish#api-access).
    * `FLOW_ID`: Your flow's UUID or custom endpoint name. You can get this value from the code snippets on your flow's [**API access** pane](/concepts-publish#api-access).
    * `LANGFLOW_API_KEY`: A valid [Langflow API key](/api-keys-and-authentication).

2. Copy the following script into a Python file, and then replace the placeholders with the information you gathered in the previous step:

    ```python
    import requests
    import os

    url = "LANGFLOW_SERVER_ADDRESS/api/v1/run/FLOW_ID"  # The complete API endpoint URL for this flow

    # Request payload configuration
    payload = {
        "output_type": "chat",
        "input_type": "chat",
        "input_value": "What's the weather like where I am right now?"
    }

    # Request headers
    headers = {
        "Content-Type": "application/json",
        "x-api-key": "LANGFLOW_API_KEY"
    }

    try:
        # Send API request
        response = requests.request("POST", url, json=payload, headers=headers)
        response.raise_for_status()  # Raise exception for bad status codes

        # Parse and print only the message text
        data = response.json()
        message = data["outputs"][0]["outputs"][0]["results"]["message"]["text"]
        print(message)

    except requests.exceptions.RequestException as e:
        print(f"Error making API request: {e}")
    except ValueError as e:
        print(f"Error parsing response: {e}")
    except (KeyError, IndexError) as e:
        print(f"Error extracting message from response: {e}")
    ```

    Notice that this script uses a different prompt than the previous **Playground** examples.
    In this script, the `input_value` asks about the weather in the user's current location without providing any hints about the user's location, such as a particular city.

    Additionally, this script includes parsing code to extract the LLM's reply from the entire Langflow API response.
    You will want to use similar extraction in your own applications because the Langflow API response includes metadata and other information that isn't relevant to the reply passed to the user.

3.  Save and run the script to send the request and test the flow.

    The agent uses the `ip_geolocation` tool to detect the requester's location, and then it uses the `weather` tool to retrieve weather information for that location.
    For example:

    ```text
    The weather in Waynesboro, Pennsylvania, is currently overcast with a temperature of 23.0Â°C (about 73.4Â°F).
    If you need more details or have any other questions, feel free to ask!
    ```

    Remember, the `ip_geolocation` tool used in this tutorial uses your Langflow server's location, which can be different from your actual location.

## Next steps

To continue building on the concepts introduced in this tutorial, see the following:

* [Use Langflow as an MCP client](/mcp-client)
* [Use Langflow Agents](/agents)
* [Use Langflow as an MCP server](/mcp-server)
* [Langflow deployment overview](/deployment-overview)


================================================================================
filename: _partial-agents-work.mdx
================================================================================

<details>
<summary>How do agents work?</summary>

Agents extend Large Language Models (LLMs) by integrating _tools_, which are functions that provide additional context and enable autonomous task execution.
These integrations make agents more specialized and powerful than standalone LLMs.

Whereas an LLM might generate acceptable, inert responses to general queries and tasks, an agent can leverage the integrated context and tools to provide more relevant responses and even take action.
For example, you might create an agent that can access your company's documentation, repositories, and other resources to help your team with tasks that require knowledge of your specific products, customers, and code.

Agents use LLMs as a reasoning engine to process input, determine which actions to take to address the query, and then generate a response.
The response could be a typical text-based LLM response, or it could involve an action, like editing a file, running a script, or calling an external API.

In an agentic context, tools are functions that the agent can run to perform tasks or access external resources.
A function is wrapped as a `Tool` object with a common interface that the agent understands.
Agents become aware of tools through tool registration, which is when the agent is provided a list of available tools typically at agent initialization.
The `Tool` object's description tells the agent what the tool can do so that it can decide whether the tool is appropriate for a given request.

</details>


================================================================================
filename: _partial-conditional-params.mdx
================================================================================

Some parameters are conditional, and they are only available after you set other parameters or select specific options for other parameters.
Conditional parameters may not be visible on the **Controls** pane until you set the required dependencies.


================================================================================
filename: _partial-dev-mode-windows.mdx
================================================================================

If you are running Langflow Desktop on Windows, you must [enable Developer Mode](https://learn.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development#activate-developer-mode) to use the Docling components.
The location of this setting depends on your Windows OS version.
Find **For developers** in your Windows Settings, or search for "Developer" in the Windows search bar, and then enable **Developer mode**.
You might need to restart your computer or Langflow to apply the change.


================================================================================
filename: _partial-escape-curly-braces.mdx
================================================================================

If your template includes literal text and variables, you can use double curly braces to escape literal curly braces in the template and prevent interpretation of that text as a variable.
For example: `This is a template with {{literal text in curly braces}} and a {variable}`.


================================================================================
filename: _partial-hidden-params.mdx
================================================================================

import Icon from "@site/src/components/icon";

Some parameters are hidden by default in the visual editor.
You can modify all parameters through the <Icon name="SlidersHorizontal" aria-hidden="true"/> **Controls** in the [component's header menu](/concepts-components#component-menus).


================================================================================
filename: _partial-legacy.mdx
================================================================================

import Icon from "@site/src/components/icon";

Legacy components are longer supported and can be removed in a future release.
You can continue to use them in existing flows, but it is recommended that you replace them with supported components as soon as possible.
Suggested replacements are included in the **Legacy** banner on components in your flows.
They are also given in release notes and Langflow documentation whenever possible.

If you aren't sure how to replace a legacy component, <Icon name="Search" aria-hidden="true" /> **Search** for components by provider, service, or component name.
The component may have been deprecated in favor of a completely new component, a similar component, or a new version of the same component in a different category.

If there is no obvious replacement, consider whether another component can be adapted to your use case.
For example, many <Icon name="Component" aria-hidden="true" /> **Core components** provide generic functionality that can support multiple providers and use cases, such as the [**API Request** component](/components-data#api-request).

If neither of these options are viable, you could use the legacy component's code to create your own custom component, or [start a discussion](/contributing-github-issues) about the legacy component.

To discourage use of legacy components in new flows, these components are hidden by default.
In the visual editor, you can click <Icon name="SlidersHorizontal" aria-hidden="true" /> **Component settings** to toggle the **Legacy** filter.


================================================================================
filename: _partial-mcp-node-tip.mdx
================================================================================

:::tip
`uvx` is included with `uv` in the Langflow package.

To use `npx` server commands, you must first install an LTS release of [Node.js](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm). If you run Langflow in Docker, install Node.js inside the container image and rebuild so that `npx`-based MCP servers are available at runtime. For more information, see [Package management](/develop-application#package-management).

For an example of an `npx` MCP server in Langflow, see [Connect an Astra DB MCP server to Langflow](/mcp-component-astra).
:::



================================================================================
filename: _partial-podman-alt.mdx
================================================================================

:::tip
Podman can be used instead of Docker for all commands shown here. For more information, see the [Podman documentation](https://podman.io/docs).
:::


================================================================================
filename: _partial-vector-rag-blurb.mdx
================================================================================

import Icon from "@site/src/components/icon";

By design, vector data is essential for LLM applications, such as chatbots and agents.

While you can use an LLM alone for generic chat interactions and common tasks, you can take your application to the next level with context sensitivity (such as RAG) and custom datasets (such as internal business data).
This often requires integrating vector databases and vector searches that provide the additional context and define meaningful queries.

Langflow includes vector store components that can read and write vector data, including embedding storage, similarity search, Graph RAG traversals, and dedicated search instances like OpenSearch.
Because of their interdependent functionality, it is common to use vector store, language model, and embedding model components in the same flow or in a series of dependent flows.

To find available vector store components, browse <Icon name="Blocks" aria-hidden="true" /> [**Bundles**](/components-bundle-components) or <Icon name="Search" aria-hidden="true" /> **Search** for your preferred vector database provider.


================================================================================
filename: _partial-vector-rag-flow.mdx
================================================================================

import Icon from "@site/src/components/icon";

:::tip
For a tutorial that uses vector data in a flow, see [Create a vector RAG chatbot](/chat-with-rag).
:::

The following example demonstrates how to use vector store components in flows alongside related components like embedding model and language model components.
These steps walk through important configuration details, functionality, and best practices for using these components effectively.
This is only one example; it isn't a prescriptive guide to all possible use cases or configurations.

1. Create a flow with the **Vector Store RAG** template.

    This template has two subflows.
    The **Load Data** subflow loads embeddings and content into a vector database, and the **Retriever** subflow runs a vector search to retrieve relevant context based on a user's query.

2. Configure the database connection for both [**Astra DB** components](/bundles-datastax#astra-db), or replace them with another pair of vector store components of your choice.
Make sure the components connect to the same vector store, and that the component in the **Retriever** subflow is able to run a similarity search.

    The parameters you set in each vector store component depend on the component's role in your flow.
    In this example, the **Load Data** subflow _writes_ to the vector store, whereas the **Retriever** subflow _reads_ from the vector store.
    Therefore, search-related parameters are only relevant to the **Vector Search** component in the **Retriever** subflow.

    For information about specific parameters, see the documentation for your chosen vector store component.

3. To configure the embedding model, do one of the following:

    * **Use an OpenAI model**: In both **OpenAI Embeddings** components, enter your OpenAI API key.
    You can use the default model or select a different OpenAI embedding model.

    * **Use another provider**: Replace the **OpenAI Embeddings** components with another pair of [embedding model components](/components-embedding-models) of your choice, and then configure the parameters and credentials accordingly.

    * **Use Astra DB vectorize**: If you are using an Astra DB vector store that has a vectorize integration, you can remove both **OpenAI Embeddings** components.
    If you do this, the vectorize integration automatically generates embeddings from the **Ingest Data** (in the **Load Data** subflow) and **Search Query** (in the **Retriever** subflow).


    :::tip
    If your vector store already contains embeddings, make sure your embedding model components use the same model as your previous embeddings.
    Mixing embedding models in the same vector store can produce inaccurate search results.
    :::

4. Recommended: In the [**Split Text** component](/components-processing#split-text), optimize the chunking settings for your embedding model.
For example, if your embedding model has a token limit of 512, then the **Chunk Size** parameter must not exceed that limit.

    Additionally, because the **Retriever** subflow passes the chat input directly to the vector store component for vector search, make sure that your chat input string doesn't exceed your embedding model's limits.
    For this example, you can enter a query that is within the limits; however, in a production environment, you might need to implement additional checks or preprocessing steps to ensure compliance.
    For example, use additional components to prepare the chat input before running the vector search, or enforce chat input limits in your application code.

5. In the **Language Model** component, enter your OpenAI API key, or select a different provider and model to use for the chat portion of the flow.

6. Run the **Load Data** subflow to populate your vector store.
In the **File** component, select one or more files, and then click <Icon name="Play" aria-hidden="true" /> **Run component** on the vector store component in the **Load Data** subflow.

    The **Load Data** subflow loads files from your local machine, chunks them, generates embeddings for the chunks, and then stores the chunks and their embeddings in the vector database.

    ![Embedding data into a vector store](/img/vector-store-document-ingestion.png)

    The **Load Data** subflow is separate from the **Retriever** subflow because you probably won't run it every time you use the chat.
    You can run the **Load Data** subflow as needed to preload or update the data in your vector store.
    Then, your chat interactions only use the components that are necessary for chat.

    If your vector store already contains data that you want to use for vector search, then you don't need to run the **Load Data** subflow.

7. Open the **Playground** and start chatting to run the **Retriever** subflow.

    The **Retriever** subflow generates an embedding from chat input, runs a vector search to retrieve similar content from your vector store, parses the search results into supplemental context for the LLM, and then uses the LLM to generate a natural language response to your query.
    The LLM uses the vector search results along with its internal training data and tools, such as basic web search and datetime information, to produce the response.

    ![Retrieval from a vector store](/img/vector-store-retrieval.png)

    To avoid passing the entire block of raw search results to the LLM, the **Parser** component extracts `text` strings from the search results `Data` object, and then passes them to the **Prompt Template** component in `Message` format.
    From there, the strings and other template content are compiled into natural language instructions for the LLM.

    You can use other components for this transformation, such as the **Data Operations** component, depending on how you want to use the search results.

    To view the raw search results, click <Icon name="TextSearch" aria-hidden="true" /> **Inspect output** on the vector store component after running the **Retriever** subflow.


================================================================================
filename: _partial-vector-search-results.mdx
================================================================================

If you use a vector store component to query your vector database, it produces search results that you can pass to downstream components in your flow as a list of [`Data`](/data-types#data) objects or a tabular [`DataFrame`](/data-types#dataframe).
If both types are supported, you can set the format near the vector store component's output port in the visual editor.


================================================================================
filename: _partial-vector-store-instance.mdx
================================================================================

Because Langflow is based on LangChain, vector store components use an instance of [LangChain vector store](https://docs.langchain.com/oss/python/integrations/vectorstores) to drive the underlying read and write functions.
These instances are provider-specific and configured according to the component's parameters, such as the connection string, index name, and schema.

In component code, this is often instantiated as `vector_store`, but some vector store components use a different name, such as the provider name.

Some LangChain classes don't expose all possible options as component parameters.
Depending on the provider, these options might use default values or allow modification through environment variables, if they are supported in Langflow.
For information about specific options, see the LangChain API reference and vector store provider's documentation.
